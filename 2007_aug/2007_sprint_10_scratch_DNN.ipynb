{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kouta/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kouta/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kouta/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kouta/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kouta/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kouta/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/kouta/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kouta/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kouta/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kouta/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kouta/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kouta/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# インポート\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.sparse as sp\n",
    "import statsmodels.api as sm\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ディープニューラルネットワークスクラッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前回は3層のニューラルネットワークを作成しましたが、今回はこれを任意の層数に拡張しやすいものに書き換えていきます。その上で、活性化関数や初期値、最適化手法について発展的なものを扱えるようにしていきます。\n",
    "\n",
    "\n",
    "このようなスクラッチを行うことで、今後各種フレームワークを利用していくにあたり、内部の動きが想像できることを目指します。\n",
    "\n",
    "\n",
    "名前は新しくScratchDeepNeuralNetrowkClassifierクラスとしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 層などのクラス化\n",
    "クラスにまとめて行くことで、構成を変更しやすい実装にしていきます。\n",
    "\n",
    "\n",
    "手を加える箇所\n",
    "\n",
    "\n",
    "層の数\n",
    "層の種類（今後畳み込み層など他のタイプの層が登場する）\n",
    "活性化関数の種類\n",
    "重みやバイアスの初期化方法\n",
    "最適化手法\n",
    "\n",
    "そのために、全結合層、各種活性化関数、重みやバイアスの初期化、最適化手法それぞれのクラスを作成します。\n",
    "\n",
    "\n",
    "実装方法は自由ですが、簡単な例を紹介します。サンプルコード1のように全結合層と活性化関数のインスタンスを作成し、サンプルコード2,3のようにして使用します。それぞれのクラスについてはこのあと解説します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 《サンプルコード1》\n",
    "\n",
    "\n",
    "ScratchDeepNeuralNetrowkClassifierのfitメソッド内\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# self.sigma : ガウス分布の標準偏差\n",
    "# self.lr : 学習率\n",
    "# self.n_nodes1 : 1層目のノード数\n",
    "# self.n_nodes2 : 2層目のノード数\n",
    "# self.n_output : 出力層のノード数\n",
    "optimizer = SGD(self.lr)\n",
    "self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "self.activation1 = Tanh()\n",
    "self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "self.activation2 = Tanh()\n",
    "self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "self.activation3 = Softmax()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 《サンプルコード2》\n",
    "\n",
    "\n",
    "イテレーションごとのフォワード\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "A1 = self.FC1.forward(X)\n",
    "Z1 = self.activation1.forward(A1)\n",
    "A2 = self.FC2.forward(Z1)\n",
    "Z2 = self.activation2.forward(A2)\n",
    "A3 = self.FC3.forward(Z2)\n",
    "Z3 = self.activation3.forward(A3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 《サンプルコード3》\n",
    "\n",
    "\n",
    "イテレーションごとのバックワード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "dA3 = self.activation3.backward(Z3, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "dZ2 = self.FC3.backward(dA3)\n",
    "dA2 = self.activation2.backward(dZ2)\n",
    "dZ1 = self.FC2.backward(dA2)\n",
    "dA1 = self.activation1.backward(dZ1)\n",
    "dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】全結合層のクラス化\n",
    "全結合層のクラス化を行なってください。\n",
    "\n",
    "\n",
    "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
    "\n",
    "\n",
    "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
    "\n",
    "\n",
    "また、引数として自身のインスタンスselfを渡すこともできます。これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、全て全結合層が持つインスタンス変数にすることができます。\n",
    "\n",
    "\n",
    "初期化方法と最適化手法のクラスについては後述します。\n",
    "\n",
    "### 《雛形》"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, lr, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer(lr)\n",
    "        self.initializer = initializer()\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.W = self.initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = self.initializer.B(n_nodes2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.X = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        pass\n",
    "        # 更新\n",
    "        batch_size = dA.shape[0]\n",
    "        self.dW = np.dot(self.X.T, dA) / batch_size\n",
    "        self.dB = np.sum(dA, axis=0, keepdims=True)[0]\n",
    "        dZ = np.dot(dA, self.W.T) \n",
    "        self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = FC(lr=0.01, n_nodes1=784, n_nodes2=400, initializer=HeInitializer, optimizer=AdaGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19165627, -0.00174032, -0.35767651, ..., -0.12533766,\n",
       "         0.02016278, -0.48636624],\n",
       "       [ 0.61738534,  0.21311891,  0.24371706, ...,  0.32981892,\n",
       "        -0.14100436, -0.46085382],\n",
       "       [ 0.72354571,  0.13904864,  0.08141931, ..., -0.07824193,\n",
       "         0.09635788, -0.66528307],\n",
       "       ...,\n",
       "       [ 0.45502782, -0.02487044, -0.00229433, ...,  0.41732181,\n",
       "         0.3791583 , -0.37315155],\n",
       "       [ 0.07655439, -0.00428627,  0.30158949, ..., -0.22494649,\n",
       "        -0.56595782, -0.39354185],\n",
       "       [ 0.19900252, -0.17529759, -0.14717502, ..., -0.43370845,\n",
       "        -0.06073787, -0.34110054]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.forward(X_train[100:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(xs, ys):\n",
    "    plt.figure()\n",
    "    plt.plot(xs, ys, marker='.')\n",
    "    plt.grid()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):# インスタンス変数の初期化\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:# hがなければ、辞書型を代入する。\n",
    "            self.h = {}\n",
    "            for key, val in params.items():# itemsで、keyとvalueをゲットする。\n",
    "                self.h[key] = np.zeros_like(val) # それぞれkeyに対応した値を代入する。\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]#h <- h + ∂L/∂W ⊙ ∂L/∂W に対応するところ\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)#  W <- W - η *(1/√h)* ∂L/∂W に対応するところ。\n",
    "            # 最後の1e-7は、self.h[key]の中に０があった場合に、０で除算してしまうことを防ぐためのもの。\n",
    "            # ディープラーニングのフレームワークではこの小さな値もパラメータとして設定できるようになっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】初期化方法のクラス化\n",
    "初期化を行うコードをクラス化してください。\n",
    "\n",
    "\n",
    "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
    "\n",
    "\n",
    "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。\n",
    "\n",
    "\n",
    "### 《雛形》"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = sigma * np.random.randn( n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】最適化手法のクラス化\n",
    "最適化手法のクラス化を行なってください。\n",
    "\n",
    "\n",
    "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
    "\n",
    "\n",
    "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。\n",
    "\n",
    "\n",
    "### 《雛形》"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        layer.B -= self.lr * layer.dB\n",
    "        layer.W -= self.lr * layer.dW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】活性化関数のクラス化\n",
    "活性化関数のクラス化を行なってください。\n",
    "\n",
    "\n",
    "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.y = 1 / (1 + np.exp(-np.clip(A, -250, 250))) \n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dz = dout * (1 - self.y) * self.y\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.y = np.tanh(A) \n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dz = dout * (1 - self.y**2)\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, loss_func=\"cel\"):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "\n",
    "    \n",
    "    def forward(self, A):\n",
    "        maxA = np.max(A, axis=1).reshape(-1,1)\n",
    "        frac1 = np.exp(A-maxA)\n",
    "        frac2 = np.sum(frac1, axis=1,  keepdims=True)\n",
    "        self.y = frac1 / frac2\n",
    "        return self.y\n",
    "    \n",
    "    \n",
    "    def backward(self, z, t):\n",
    "        self.loss = self._cross_entropy_loss(z, t)\n",
    "        y = z - t\n",
    "        #print(y[0])\n",
    "        #print(y.shape)\n",
    "        return y\n",
    "    \n",
    "        \n",
    "    def _cross_entropy_loss(self, z, t):\n",
    "        if t.ndim == 1:\n",
    "            z = z.reshape(1, z.size)\n",
    "            t = t.reshape(1, t.size)\n",
    "\n",
    "        batch_size = z.shape[0]\n",
    "        loss = -np.sum(t * np.log(z + 1e-9)) / batch_size\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cel():\n",
    "    def cross_entropy_loss(self, y, t):\n",
    "        if y.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "\n",
    "        batch_size = t.shape[0]\n",
    "        loss = -np.sum(y * np.log(t + 1e-9)) / batch_size\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 発展的要素\n",
    "活性化関数や重みの初期値、最適化手法に関してこれまで見てきた以外のものを実装していきます。\n",
    "\n",
    "\n",
    "# 【問題5】ReLUクラスの作成\n",
    "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
    "\n",
    "\n",
    "ReLUは以下の数式です。\n",
    "\n",
    "$$\n",
    "f(x)=ReLU(x)={xif x>0,0if x≦0.\n",
    "$$\n",
    "\n",
    "$x$ : ある特徴量。スカラー\n",
    "\n",
    "\n",
    "実装上はnp.maximumを使い配列に対してまとめて計算が可能です。\n",
    "\n",
    "\n",
    "[numpy.maximum — NumPy v1.15 Manual](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.maximum.html)\n",
    "\n",
    "\n",
    "一方、バックプロパゲーションのための $x$ に関する $f(x)$ の微分は以下のようになります。\n",
    "\n",
    "$$\n",
    "∂f(x)∂x={1if x>0,0if x≦0.\n",
    "$$\n",
    "\n",
    "数学的には微分可能ではないですが、 $x=0$ のとき $0$ とすることで対応しています。\n",
    "\n",
    "\n",
    "フォワード時の $x$ の正負により、勾配を逆伝播するかどうかが決まるということになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        y = x.copy()\n",
    "        y[self.mask] = 0\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】重みの初期値\n",
    "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。\n",
    "\n",
    "\n",
    "XavierInitializerクラスと、HeInitializerクラスを作成してください。\n",
    "\n",
    "\n",
    "### Xavierの初期値\n",
    "Xavierの初期値における標準偏差 $\\sigma$ は次の式で求められます。\n",
    "\n",
    "$$\n",
    "\\sigma=1\\sqrt{n}\n",
    "$$\n",
    "\n",
    "$n$ : 前の層のノード数\n",
    "\n",
    "\n",
    "《論文》\n",
    "\n",
    "\n",
    "[Glorot, X., & Bengio, Y. (n.d.). Understanding the difficulty of training deep feedforward neural networks.](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "\n",
    "\n",
    "### Heの初期値\n",
    "Heの初期値における標準偏差 $\\sigma$ は次の式で求められます。\n",
    "\n",
    "$$\n",
    "\\sigma=\\sqrt{\\frac{1}{2n}}\n",
    "$$\n",
    "\n",
    "$n$ : 前の層のノード数\n",
    "\n",
    "\n",
    "《論文》\n",
    "\n",
    "\n",
    "[He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.](https://arxiv.org/pdf/1502.01852.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.sigma = 1/np.sqrt(n_nodes1)\n",
    "        W = sigma * np.random.randn( n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        sigma = np.sqrt(1/n_nodes1)\n",
    "        W = sigma * np.random.randn( n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】最適化手法\n",
    "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。\n",
    "\n",
    "\n",
    "まず、これまで使ってきたSGDを確認します。\n",
    "\n",
    "\n",
    "$$\n",
    "W^′_i=W_i - \\alpha E(\\partial L \\partial W_i)\n",
    "$$\n",
    "$$\n",
    "B^′_i=B_i - \\alpha E(\\partial L \\partial B_i)\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率（層ごとに変えることも可能だが、基本的には全て同じとする）\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_i}$ : $W_i$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B_i}$ : $B_i$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$E()$ : ミニバッチ方向にベクトルの平均を計算\n",
    "\n",
    "\n",
    "続いて、AdaGradです。バイアスの数式は省略しますが、重みと同様のことをします。\n",
    "\n",
    "\n",
    "更新された分だけその重みに対する学習率を徐々に下げていきます。イテレーションごとの勾配の二乗和 $H$ を保存しておき、その分だけ学習率を小さくします。\n",
    "\n",
    "\n",
    "学習率は重み一つひとつに対して異なることになります。\n",
    "\n",
    "\n",
    "$$\n",
    "H^′_i=H_i + E(\\frac{\\partial L}{ \\partial W_i}) * E(\\frac{\\partial L} {\\partial Wi})\n",
    "$$\n",
    "$$\n",
    "W^′_i=W_i - \\alpha \\frac{1}{\\sqrt{H^′_i}}E(\\frac{\\partial L}{\\partial W_i})\n",
    "$$\n",
    "\n",
    "$H_i$ : i層目に関して、前のイテレーションまでの勾配の二乗和（初期値は0）\n",
    "\n",
    "\n",
    "$H_i^{\\prime}$ : 更新した $H_i$\n",
    "\n",
    "\n",
    "《論文》\n",
    "\n",
    "\n",
    "[Duchi JDUCHI, J., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan. Journal of Machine Learning Research (Vol. 12).](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.hw = None\n",
    "        self.hb = None\n",
    "        \n",
    "    def update(self, layer):\n",
    "        if self.hw is None and self.hb is None :\n",
    "            self.hw = 0\n",
    "            self.hb = 0\n",
    "            \n",
    "        self.hw += np.sum(layer.dW ** 2)\n",
    "        self.hb += np.sum(layer.dB ** 2)\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(self.hw) + 1e-7)\n",
    "        layer.B -= self.lr * layer.dB / (np.sqrt(self.hb) + 1e-7)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題8】クラスの完成\n",
    "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【データ作成】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape) # (60000, 28, 28)\n",
    "print(X_test.shape) # (10000, 28, 28)\n",
    "print(X_train[0].dtype) # uint8\n",
    "#print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0\n",
    "print(X_train[0].dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mini_batch = GetMiniBatch(X_train, y_train, batch_size=20)\n",
    "#print(len(get_mini_batch)) # 2400\n",
    "#print(get_mini_batch[5]) # 5番目のミニバッチが取得できる\n",
    "for mini_X_train, mini_y_train in get_mini_batch:\n",
    "    # このfor文内でミニバッチが使える\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【解答】自作関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    DNNを行うクラス\n",
    "    paramater\n",
    "    ---------------------------------\n",
    "    self.lr : 学習率 \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr, batch_size=20, verbose=True):\n",
    "        \"\"\"\n",
    "        initializer = \"XavierInitializer\", \"HeInitializer\"\n",
    "        optimizer = \"SGD\", \"AdaGrad\"\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def _one_hot(self, y):\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        return y_one_hot\n",
    "    \n",
    "    def fit(self, X, y, epoch=3, n_nodes_1=400, n_nodes_2=200, activation=Sigmoid, Initializer=HeInitializer, optimizer=AdaGrad):\n",
    "        \"\"\"\n",
    "        activation : \"Sigmoid\", \"Tanh\"\n",
    "        \"\"\"\n",
    "        self.loss_dict = {}\n",
    "        self.n_nodes_1 = n_nodes_1\n",
    "        self.n_nodes_2 = n_nodes_2\n",
    "        \n",
    "        if y.ndim == 1:\n",
    "            y = self._one_hot(y)\n",
    "        \n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_output = y.shape[1]\n",
    "        \n",
    "        self.FC1 = FC(self.lr, self.n_features, self.n_nodes_1, Initializer, optimizer)\n",
    "        self.activation1 = ReLU()\n",
    "        self.FC2 = FC(self.lr, self.n_nodes_1, self.n_nodes_2,  Initializer, optimizer)\n",
    "        self.activation2 = ReLU()\n",
    "        self.FC3 = FC(self.lr, self.n_nodes_2, self.n_output,  Initializer, optimizer)\n",
    "        self.activation3 = Softmax()\n",
    "        \n",
    "        cnt = 1\n",
    "        for i in range(epoch):\n",
    "            # ミニバッチインスタンス\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=20)\n",
    "            # ミニバッチ開始、バッチサイズ20\n",
    "            #print(self.FC2.W)\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                A1 = self.FC1.forward(mini_X_train)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                \n",
    "                \"\"\"\n",
    "                Loss Curvを描くための処理\n",
    "                \"\"\"\n",
    "                self.loss_dict[cnt] = self.activation3._cross_entropy_loss(Z3, mini_y_train)\n",
    "                #print(self.loss_dict)\n",
    "                \n",
    "\n",
    "                dA3 = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "                \n",
    "                #print(\"batch_{} : loss : {}\".format(cnt, self.loss_dict[cnt]))\n",
    "                \n",
    "                cnt += 1\n",
    "            \n",
    "            \n",
    "            #epoch_pred = self.predict(X)\n",
    "            #epoch_acc = ((np.sum(y == epoch_pred)).astype(np.float) / X.shape[0])\n",
    "            \n",
    "            print(\"-------------------------------------------\")\n",
    "            #print(\"epoch_{} // acc : {}\".format(i, epoch_acc))\n",
    "            print(\"epoch_{} // loss : {}\".format(i, self.loss_dict[cnt - get_mini_batch._stop]))\n",
    "            #print(self.loss_dict)\n",
    "\n",
    "        #return self.loss_dict\n",
    "            \n",
    "    def predict(self,X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        y = self.activation3.forward(A3)\n",
    "        \n",
    "        return np.argmax(y, axis=1)\n",
    "    \n",
    "#    def plot_loss(self,loss_dict):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = ScratchDeepNeuralNetrowkClassifier(0.1, batch_size=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "epoch_0 // loss : 2.2475931400692533\n",
      "-------------------------------------------\n",
      "epoch_1 // loss : 0.36399550129671826\n",
      "-------------------------------------------\n",
      "epoch_2 // loss : 0.3089587876179551\n",
      "-------------------------------------------\n",
      "epoch_3 // loss : 0.2801598378404234\n",
      "-------------------------------------------\n",
      "epoch_4 // loss : 0.2604301485523818\n",
      "-------------------------------------------\n",
      "epoch_5 // loss : 0.24556356363081516\n",
      "-------------------------------------------\n",
      "epoch_6 // loss : 0.23344655355799535\n",
      "-------------------------------------------\n",
      "epoch_7 // loss : 0.22336406169539308\n",
      "-------------------------------------------\n",
      "epoch_8 // loss : 0.21496132041479815\n",
      "-------------------------------------------\n",
      "epoch_9 // loss : 0.20770544628169496\n",
      "-------------------------------------------\n",
      "epoch_10 // loss : 0.2013793896767043\n",
      "-------------------------------------------\n",
      "epoch_11 // loss : 0.19591486280706646\n",
      "-------------------------------------------\n",
      "epoch_12 // loss : 0.19096429860143124\n",
      "-------------------------------------------\n",
      "epoch_13 // loss : 0.18645772722328896\n",
      "-------------------------------------------\n",
      "epoch_14 // loss : 0.18227191671976\n",
      "-------------------------------------------\n",
      "epoch_15 // loss : 0.17842991122646548\n",
      "-------------------------------------------\n",
      "epoch_16 // loss : 0.17485117501013167\n",
      "-------------------------------------------\n",
      "epoch_17 // loss : 0.17163071761556084\n",
      "-------------------------------------------\n",
      "epoch_18 // loss : 0.1686373908822029\n",
      "-------------------------------------------\n",
      "epoch_19 // loss : 0.1659358614272089\n",
      "-------------------------------------------\n",
      "epoch_20 // loss : 0.16341029930861997\n",
      "-------------------------------------------\n",
      "epoch_21 // loss : 0.16111622577139867\n",
      "-------------------------------------------\n",
      "epoch_22 // loss : 0.15891390781239628\n",
      "-------------------------------------------\n",
      "epoch_23 // loss : 0.15685494250380436\n",
      "-------------------------------------------\n",
      "epoch_24 // loss : 0.15493545524746208\n",
      "-------------------------------------------\n",
      "epoch_25 // loss : 0.15319040497987896\n",
      "-------------------------------------------\n",
      "epoch_26 // loss : 0.15153321006411063\n",
      "-------------------------------------------\n",
      "epoch_27 // loss : 0.14996552641164929\n",
      "-------------------------------------------\n",
      "epoch_28 // loss : 0.1484013566520716\n",
      "-------------------------------------------\n",
      "epoch_29 // loss : 0.14688260214801757\n",
      "-------------------------------------------\n",
      "epoch_30 // loss : 0.14549784068893906\n",
      "-------------------------------------------\n",
      "epoch_31 // loss : 0.14414683498989384\n",
      "-------------------------------------------\n",
      "epoch_32 // loss : 0.14293685963088637\n",
      "-------------------------------------------\n",
      "epoch_33 // loss : 0.1417950577641119\n",
      "-------------------------------------------\n",
      "epoch_34 // loss : 0.14064281492692401\n",
      "-------------------------------------------\n",
      "epoch_35 // loss : 0.13952509728470258\n",
      "-------------------------------------------\n",
      "epoch_36 // loss : 0.13842632977434538\n",
      "-------------------------------------------\n",
      "epoch_37 // loss : 0.13735919092128496\n",
      "-------------------------------------------\n",
      "epoch_38 // loss : 0.13633849891408859\n",
      "-------------------------------------------\n",
      "epoch_39 // loss : 0.13536484969783447\n",
      "-------------------------------------------\n",
      "epoch_40 // loss : 0.13444035148568081\n",
      "-------------------------------------------\n",
      "epoch_41 // loss : 0.13348320074665715\n",
      "-------------------------------------------\n",
      "epoch_42 // loss : 0.13257696078713344\n",
      "-------------------------------------------\n",
      "epoch_43 // loss : 0.131710681717744\n",
      "-------------------------------------------\n",
      "epoch_44 // loss : 0.1308944862877182\n",
      "-------------------------------------------\n",
      "epoch_45 // loss : 0.1300873980748356\n",
      "-------------------------------------------\n",
      "epoch_46 // loss : 0.12930981420116558\n",
      "-------------------------------------------\n",
      "epoch_47 // loss : 0.1286016237970909\n",
      "-------------------------------------------\n",
      "epoch_48 // loss : 0.1279180906155622\n",
      "-------------------------------------------\n",
      "epoch_49 // loss : 0.12723708691143537\n",
      "-------------------------------------------\n",
      "epoch_50 // loss : 0.12655617450337098\n",
      "-------------------------------------------\n",
      "epoch_51 // loss : 0.1259077609063815\n",
      "-------------------------------------------\n",
      "epoch_52 // loss : 0.12525677421700826\n",
      "-------------------------------------------\n",
      "epoch_53 // loss : 0.12462597435808073\n",
      "-------------------------------------------\n",
      "epoch_54 // loss : 0.1240129671654631\n",
      "-------------------------------------------\n",
      "epoch_55 // loss : 0.12338655254922457\n",
      "-------------------------------------------\n",
      "epoch_56 // loss : 0.12277660866294626\n",
      "-------------------------------------------\n",
      "epoch_57 // loss : 0.12218855962200217\n",
      "-------------------------------------------\n",
      "epoch_58 // loss : 0.12155982008406603\n",
      "-------------------------------------------\n",
      "epoch_59 // loss : 0.12089580794981399\n",
      "-------------------------------------------\n",
      "epoch_60 // loss : 0.12022754631901456\n",
      "-------------------------------------------\n",
      "epoch_61 // loss : 0.11959628140667544\n",
      "-------------------------------------------\n",
      "epoch_62 // loss : 0.11899272843231927\n",
      "-------------------------------------------\n",
      "epoch_63 // loss : 0.1184066825992945\n",
      "-------------------------------------------\n",
      "epoch_64 // loss : 0.11781751620225549\n",
      "-------------------------------------------\n",
      "epoch_65 // loss : 0.11722827671449927\n",
      "-------------------------------------------\n",
      "epoch_66 // loss : 0.11664389057320305\n",
      "-------------------------------------------\n",
      "epoch_67 // loss : 0.11607696068651101\n",
      "-------------------------------------------\n",
      "epoch_68 // loss : 0.11550766097380265\n",
      "-------------------------------------------\n",
      "epoch_69 // loss : 0.11496237359939236\n",
      "-------------------------------------------\n",
      "epoch_70 // loss : 0.1144309920224745\n",
      "-------------------------------------------\n",
      "epoch_71 // loss : 0.11387992261511068\n",
      "-------------------------------------------\n",
      "epoch_72 // loss : 0.11336468428684379\n",
      "-------------------------------------------\n",
      "epoch_73 // loss : 0.11283562522263786\n",
      "-------------------------------------------\n",
      "epoch_74 // loss : 0.1123168262413714\n",
      "-------------------------------------------\n",
      "epoch_75 // loss : 0.11181853696108249\n",
      "-------------------------------------------\n",
      "epoch_76 // loss : 0.11131725161685184\n",
      "-------------------------------------------\n",
      "epoch_77 // loss : 0.11082618888364952\n",
      "-------------------------------------------\n",
      "epoch_78 // loss : 0.11035829715627683\n",
      "-------------------------------------------\n",
      "epoch_79 // loss : 0.10987953357894047\n",
      "-------------------------------------------\n",
      "epoch_80 // loss : 0.10939202899758951\n",
      "-------------------------------------------\n",
      "epoch_81 // loss : 0.10890409683244377\n",
      "-------------------------------------------\n",
      "epoch_82 // loss : 0.10844875940944387\n",
      "-------------------------------------------\n",
      "epoch_83 // loss : 0.10799932239406815\n",
      "-------------------------------------------\n",
      "epoch_84 // loss : 0.10756066341604904\n",
      "-------------------------------------------\n",
      "epoch_85 // loss : 0.1071268342254978\n",
      "-------------------------------------------\n",
      "epoch_86 // loss : 0.10671532635760309\n",
      "-------------------------------------------\n",
      "epoch_87 // loss : 0.10629957633716516\n",
      "-------------------------------------------\n",
      "epoch_88 // loss : 0.10586109225686031\n",
      "-------------------------------------------\n",
      "epoch_89 // loss : 0.1054235315974094\n",
      "-------------------------------------------\n",
      "epoch_90 // loss : 0.10499627508624772\n",
      "-------------------------------------------\n",
      "epoch_91 // loss : 0.1045792673771524\n",
      "-------------------------------------------\n",
      "epoch_92 // loss : 0.10417580513249702\n",
      "-------------------------------------------\n",
      "epoch_93 // loss : 0.10379532809304721\n",
      "-------------------------------------------\n",
      "epoch_94 // loss : 0.10340207809830607\n",
      "-------------------------------------------\n",
      "epoch_95 // loss : 0.10300910555005904\n",
      "-------------------------------------------\n",
      "epoch_96 // loss : 0.1026369988558696\n",
      "-------------------------------------------\n",
      "epoch_97 // loss : 0.10228497781426546\n",
      "-------------------------------------------\n",
      "epoch_98 // loss : 0.10193301305315776\n",
      "-------------------------------------------\n",
      "epoch_99 // loss : 0.10157780767620864\n",
      "24min 56s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "tmp.fit(X_train, y_train,epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = tmp.predict(X_test)\n",
    "pred[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9462"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc2 = ((np.sum(y_test == pred)).astype(np.float) / X_test.shape[0])\n",
    "acc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題9】学習と推定\n",
    "層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【解答】tanh版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    DNNを行うクラス\n",
    "    paramater\n",
    "    ---------------------------------\n",
    "    self.lr : 学習率 \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr, batch_size=20, verbose=True):\n",
    "        \"\"\"\n",
    "        initializer = \"XavierInitializer\", \"HeInitializer\"\n",
    "        optimizer = \"SGD\", \"AdaGrad\"\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def _one_hot(self, y):\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        return y_one_hot\n",
    "    \n",
    "    def fit(self, X, y, epoch=10, n_nodes_1=400, n_nodes_2=200, activation=Tanh, Initializer=HeInitializer, optimizer=AdaGrad):\n",
    "        \"\"\"\n",
    "        activation : \"Sigmoid\", \"Tanh\"\n",
    "        \"\"\"\n",
    "        self.loss_dict = {}\n",
    "        self.n_nodes_1 = n_nodes_1\n",
    "        self.n_nodes_2 = n_nodes_2\n",
    "        \n",
    "        if y.ndim == 1:\n",
    "            y = self._one_hot(y)\n",
    "        \n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_output = y.shape[1]\n",
    "        \n",
    "        self.FC1 = FC(self.lr, self.n_features, self.n_nodes_1, Initializer, optimizer)\n",
    "        self.activation1 = Tanh()\n",
    "        self.FC2 = FC(self.lr, self.n_nodes_1, self.n_nodes_2,  Initializer, optimizer)\n",
    "        self.activation2 = Tanh()\n",
    "        self.FC3 = FC(self.lr, self.n_nodes_2, self.n_output,  Initializer, optimizer)\n",
    "        self.activation3 = Softmax()\n",
    "        \n",
    "        cnt = 1\n",
    "        for i in range(epoch):\n",
    "            # ミニバッチインスタンス\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=20)\n",
    "            # ミニバッチ開始、バッチサイズ20\n",
    "            #print(self.FC2.W)\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                A1 = self.FC1.forward(mini_X_train)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                \n",
    "                \"\"\"\n",
    "                Loss Curvを描くための処理\n",
    "                \"\"\"\n",
    "                self.loss_dict[cnt] = self.activation3._cross_entropy_loss(Z3, mini_y_train)\n",
    "                #print(self.loss_dict)\n",
    "                \n",
    "\n",
    "                dA3 = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "                \n",
    "                #print(\"batch_{} : loss : {}\".format(cnt, self.loss_dict[cnt]))\n",
    "                \n",
    "                cnt += 1\n",
    "            \n",
    "            \n",
    "            #epoch_pred = self.predict(X)\n",
    "            #epoch_acc = ((np.sum(y == epoch_pred)).astype(np.float) / X.shape[0])\n",
    "            \n",
    "            print(\"-------------------------------------------\")\n",
    "            #print(\"epoch_{} // acc : {}\".format(i, epoch_acc))\n",
    "            print(\"epoch_{} // loss : {}\".format(i, self.loss_dict[cnt - get_mini_batch._stop]))\n",
    "            #print(self.loss_dict)\n",
    "\n",
    "        #return self.loss_dict\n",
    "            \n",
    "    def predict(self,X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        y = self.activation3.forward(A3)\n",
    "        \n",
    "        return np.argmax(y, axis=1)\n",
    "    \n",
    "#    def plot_loss(self,loss_dict):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "q9_test = ScratchDeepNeuralNetrowkClassifier(0.1, batch_size=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "epoch_0 // loss : 2.359404200715454\n",
      "-------------------------------------------\n",
      "epoch_1 // loss : 0.29868154381402645\n",
      "-------------------------------------------\n",
      "epoch_2 // loss : 0.2384823113033116\n",
      "-------------------------------------------\n",
      "epoch_3 // loss : 0.20888777443203552\n",
      "-------------------------------------------\n",
      "epoch_4 // loss : 0.19058787408956226\n",
      "-------------------------------------------\n",
      "epoch_5 // loss : 0.1779238551686208\n",
      "-------------------------------------------\n",
      "epoch_6 // loss : 0.16851501016059967\n",
      "-------------------------------------------\n",
      "epoch_7 // loss : 0.16117211646934443\n",
      "-------------------------------------------\n",
      "epoch_8 // loss : 0.1552322768736678\n",
      "-------------------------------------------\n",
      "epoch_9 // loss : 0.15029533438333284\n",
      "-------------------------------------------\n",
      "epoch_10 // loss : 0.14610440054635682\n",
      "-------------------------------------------\n",
      "epoch_11 // loss : 0.14248625535309378\n",
      "-------------------------------------------\n",
      "epoch_12 // loss : 0.13931932587560794\n",
      "-------------------------------------------\n",
      "epoch_13 // loss : 0.1365154165698343\n",
      "-------------------------------------------\n",
      "epoch_14 // loss : 0.1340087502551278\n",
      "-------------------------------------------\n",
      "epoch_15 // loss : 0.13174910898720602\n",
      "-------------------------------------------\n",
      "epoch_16 // loss : 0.12969738131767367\n",
      "-------------------------------------------\n",
      "epoch_17 // loss : 0.12782257867059238\n",
      "-------------------------------------------\n",
      "epoch_18 // loss : 0.12609978001338198\n",
      "-------------------------------------------\n",
      "epoch_19 // loss : 0.12450868113176297\n",
      "-------------------------------------------\n",
      "epoch_20 // loss : 0.12303254840262838\n",
      "-------------------------------------------\n",
      "epoch_21 // loss : 0.12165744973893547\n",
      "-------------------------------------------\n",
      "epoch_22 // loss : 0.12037167956883062\n",
      "-------------------------------------------\n",
      "epoch_23 // loss : 0.11916532228585641\n",
      "-------------------------------------------\n",
      "epoch_24 // loss : 0.11802991624652229\n",
      "-------------------------------------------\n",
      "epoch_25 // loss : 0.11695819193237464\n",
      "-------------------------------------------\n",
      "epoch_26 // loss : 0.11594386560104239\n",
      "-------------------------------------------\n",
      "epoch_27 // loss : 0.11498147499539339\n",
      "-------------------------------------------\n",
      "epoch_28 // loss : 0.11406624731069756\n",
      "-------------------------------------------\n",
      "epoch_29 // loss : 0.11319399217323874\n",
      "-------------------------------------------\n",
      "epoch_30 // loss : 0.11236101420607944\n",
      "-------------------------------------------\n",
      "epoch_31 // loss : 0.11156404107567237\n",
      "-------------------------------------------\n",
      "epoch_32 // loss : 0.11080016387822819\n",
      "-------------------------------------------\n",
      "epoch_33 // loss : 0.1100667874397799\n",
      "-------------------------------------------\n",
      "epoch_34 // loss : 0.10936158863929515\n",
      "-------------------------------------------\n",
      "epoch_35 // loss : 0.10868248126912579\n",
      "-------------------------------------------\n",
      "epoch_36 // loss : 0.10802758625616252\n",
      "-------------------------------------------\n",
      "epoch_37 // loss : 0.10739520630508229\n",
      "-------------------------------------------\n",
      "epoch_38 // loss : 0.10678380420982733\n",
      "-------------------------------------------\n",
      "epoch_39 // loss : 0.10619198422396217\n",
      "-------------------------------------------\n",
      "epoch_40 // loss : 0.10561847599442349\n",
      "-------------------------------------------\n",
      "epoch_41 // loss : 0.10506212065346351\n",
      "-------------------------------------------\n",
      "epoch_42 // loss : 0.10452185873566718\n",
      "-------------------------------------------\n",
      "epoch_43 // loss : 0.10399671964479332\n",
      "-------------------------------------------\n",
      "epoch_44 // loss : 0.10348581244192959\n",
      "-------------------------------------------\n",
      "epoch_45 // loss : 0.10298831776437636\n",
      "-------------------------------------------\n",
      "epoch_46 // loss : 0.10250348071564361\n",
      "-------------------------------------------\n",
      "epoch_47 // loss : 0.10203060459232156\n",
      "-------------------------------------------\n",
      "epoch_48 // loss : 0.10156904533449203\n",
      "-------------------------------------------\n",
      "epoch_49 // loss : 0.1011182066036415\n",
      "-------------------------------------------\n",
      "epoch_50 // loss : 0.10067753540640534\n",
      "-------------------------------------------\n",
      "epoch_51 // loss : 0.10024651819445858\n",
      "-------------------------------------------\n",
      "epoch_52 // loss : 0.09982467738088836\n",
      "-------------------------------------------\n",
      "epoch_53 // loss : 0.09941156822182898\n",
      "-------------------------------------------\n",
      "epoch_54 // loss : 0.09900677601922776\n",
      "-------------------------------------------\n",
      "epoch_55 // loss : 0.09860991360664162\n",
      "-------------------------------------------\n",
      "epoch_56 // loss : 0.0982206190850494\n",
      "-------------------------------------------\n",
      "epoch_57 // loss : 0.09783855378001868\n",
      "-------------------------------------------\n",
      "epoch_58 // loss : 0.09746340039527143\n",
      "-------------------------------------------\n",
      "epoch_59 // loss : 0.09709486134086595\n",
      "-------------------------------------------\n",
      "epoch_60 // loss : 0.09673265721694269\n",
      "-------------------------------------------\n",
      "epoch_61 // loss : 0.09637652543632816\n",
      "-------------------------------------------\n",
      "epoch_62 // loss : 0.09602621897130853\n",
      "-------------------------------------------\n",
      "epoch_63 // loss : 0.09568150521165249\n",
      "-------------------------------------------\n",
      "epoch_64 // loss : 0.09534216492246939\n",
      "-------------------------------------------\n",
      "epoch_65 // loss : 0.09500799129181471\n",
      "-------------------------------------------\n",
      "epoch_66 // loss : 0.09467878905910622\n",
      "-------------------------------------------\n",
      "epoch_67 // loss : 0.09435437371642172\n",
      "-------------------------------------------\n",
      "epoch_68 // loss : 0.09403457077561914\n",
      "-------------------------------------------\n",
      "epoch_69 // loss : 0.09371921509500683\n",
      "-------------------------------------------\n",
      "epoch_70 // loss : 0.09340815025995006\n",
      "-------------------------------------------\n",
      "epoch_71 // loss : 0.09310122801240839\n",
      "-------------------------------------------\n",
      "epoch_72 // loss : 0.09279830772492641\n",
      "-------------------------------------------\n",
      "epoch_73 // loss : 0.09249925591504446\n",
      "-------------------------------------------\n",
      "epoch_74 // loss : 0.09220394579654019\n",
      "-------------------------------------------\n",
      "epoch_75 // loss : 0.09191225686423558\n",
      "-------------------------------------------\n",
      "epoch_76 // loss : 0.09162407450946748\n",
      "-------------------------------------------\n",
      "epoch_77 // loss : 0.09133928966357185\n",
      "-------------------------------------------\n",
      "epoch_78 // loss : 0.09105779846701599\n",
      "-------------------------------------------\n",
      "epoch_79 // loss : 0.09077950196201826\n",
      "-------------------------------------------\n",
      "epoch_80 // loss : 0.09050430580671963\n",
      "-------------------------------------------\n",
      "epoch_81 // loss : 0.09023212000913586\n",
      "-------------------------------------------\n",
      "epoch_82 // loss : 0.08996285867929224\n",
      "-------------------------------------------\n",
      "epoch_83 // loss : 0.08969643979809028\n",
      "-------------------------------------------\n",
      "epoch_84 // loss : 0.08943278500158214\n",
      "-------------------------------------------\n",
      "epoch_85 // loss : 0.08917181937944649\n",
      "-------------------------------------------\n",
      "epoch_86 // loss : 0.08891347128657473\n",
      "-------------------------------------------\n",
      "epoch_87 // loss : 0.08865767216676415\n",
      "-------------------------------------------\n",
      "epoch_88 // loss : 0.08840435638760046\n",
      "-------------------------------------------\n",
      "epoch_89 // loss : 0.08815346108570239\n",
      "-------------------------------------------\n",
      "epoch_90 // loss : 0.08790492602155564\n",
      "-------------------------------------------\n",
      "epoch_91 // loss : 0.08765869344324048\n",
      "-------------------------------------------\n",
      "epoch_92 // loss : 0.08741470795841126\n",
      "-------------------------------------------\n",
      "epoch_93 // loss : 0.08717291641393625\n",
      "-------------------------------------------\n",
      "epoch_94 // loss : 0.08693326778266082\n",
      "-------------------------------------------\n",
      "epoch_95 // loss : 0.08669571305679355\n",
      "-------------------------------------------\n",
      "epoch_96 // loss : 0.08646020514746011\n",
      "-------------------------------------------\n",
      "epoch_97 // loss : 0.08622669879000733\n",
      "-------------------------------------------\n",
      "epoch_98 // loss : 0.08599515045466266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "epoch_99 // loss : 0.08576551826219944\n",
      "17min 11s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "q9_test.fit(X_train, y_train,epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_list = []\n",
    "for i in range(1,24000,2400):\n",
    "    plot_list.append(q9_test.loss_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(1,11), plot_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_pred = q9_test.predict(X_test)\n",
    "tanh_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy: 0.9366\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Accuracy: {}\".format(accuracy_score(y_test, tanh_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シグモイド関数よりも若干下がった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
