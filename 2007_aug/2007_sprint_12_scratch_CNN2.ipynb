{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.sparse as sp\n",
    "import statsmodels.api as sm\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import OrderedDict\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2次元の畳み込みニューラルネットワークスクラッチ\n",
    "\n",
    "2次元に対応した畳み込みニューラルネットワーク（CNN）のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "プーリング層なども作成することで、CNNの基本形を完成させます。クラスの名前はScratch2dCNNClassifierとしてください。\n",
    "\n",
    "\n",
    "データセットの用意\n",
    "引き続きMNISTデータセットを使用します。2次元畳み込み層へは、28×28の状態で入力します。\n",
    "\n",
    "\n",
    "今回は白黒画像ですからチャンネルは1つしかありませんが、チャンネル方向の軸は用意しておく必要があります。\n",
    "\n",
    "\n",
    "(n_samples, n_channels, height, width)のNCHWまたは(n_samples, height, width, n_channels)のNHWCどちらかの形にしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】2次元畳み込み層の作成\n",
    "1次元畳み込み層のクラスConv1dを発展させ、2次元畳み込み層のクラスConv2dを作成してください。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "\n",
    "$$\n",
    "a_{i,j,m} = \\sum_{k=0}^{K-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1}x_{(i+s),(j+t),k}w_{s,t,k,m}+b_{m}\n",
    "$$\n",
    "\n",
    "$a_{i,j,m}$ : 出力される配列のi行j列、mチャンネルの値\n",
    "\n",
    "$i$ : 配列の行方向のインデックス\n",
    "\n",
    "$j$ : 配列の列方向のインデックス\n",
    "\n",
    "$m$ : 出力チャンネルのインデックス\n",
    "\n",
    "$K$ : 入力チャンネル数\n",
    "\n",
    "$F_{h}, F_{w}$ : 高さ方向（h）と幅方向（w）のフィルタのサイズ\n",
    "\n",
    "$x_{(i+s),(j+t),k}$: 入力の配列の(i+s)行(j+t)列、kチャンネルの値\n",
    "\n",
    "$w_{s,t,k,m}$: 重みの配列のs行t列目。kチャンネルの入力に対して、mチャンネルへ出力する重み\n",
    "\n",
    "$b_m$ : mチャンネルへの出力のバイアス項\n",
    "\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "\n",
    "次に更新式です。1次元畳み込み層や全結合層と同じ形です。\n",
    "\n",
    "$$\n",
    "w_{s,t,k,m}^{\\prime} = w_{s,t,k,m} - \\alpha \\frac{\\partial L}{\\partial w_{s,t,k,m}} \\\\ b_{m}^{\\prime} = b_{m} - \\alpha \\frac{\\partial L}{\\partial b_{m}}\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_{s,t,k,m}}$ : $w_{s,t,k,m}$に関する損失 $L$ の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b_{m}}$ : $b_{m}$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "勾配  $\\frac{\\partial L}{\\partial w_{s,t,k,m}}$ や $\\frac{\\partial L}{\\partial b_{m}}$ を求めるためのバックプロパゲーションの数式が以下である。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{s,t,k,m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1} \\frac{\\partial L}{\\partial a_{i,j,m}}x_{(i+s)(j+t),k}\\\\ \\frac{\\partial L}{\\partial b_{m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1}\\frac{\\partial L}{\\partial a_{i,j,m}}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$ : 勾配の配列のi行j列、mチャンネルの値\n",
    "\n",
    "$N_{out,h},N_{out,w}$ : 高さ方向（h）と幅方向（w）の出力のサイズ\n",
    "\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{i,j,k}} = \\sum_{m=0}^{M-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1} \\frac{\\partial L}{\\partial a_{(i-s),(j-t),m}}w_{s,t,k,m}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_{i,j,k}}$ : 前の層に流す誤差の配列のi列j行、kチャンネルの値\n",
    "\n",
    "$M$ : 出力チャンネル数\n",
    "\n",
    "\n",
    "ただし、 $i-s<0$ または $i-s>N_{out,h}-1$ または $j-t<0$ または $j-t>N_{out,w}-1$ のとき $\\frac{\\partial L}{\\partial a_{(i-s),(j-t),m}} =0$ です。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNISTはデータサイズが大きいので、いったん[Optical Recognition of Handwritten Digits Data Set](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits)のコピーである、[sklearn.datasets.load_digits](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)を使って、正しく動いているか検証してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【テストデータ作成】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 8, 8)\n",
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 1, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "X,y = load_digits(n_class = 10, return_X_y = True, as_frame = False)\n",
    "X = X.reshape((-1,8,8))\n",
    "print(X.shape)\n",
    "for i in range(2):\n",
    "    print(y[i])\n",
    "    plt.imshow(X[i],\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "X = X[:, np.newaxis, :, :]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "標準化/正規化ももちろん必須なので、してあげましょう。\n",
    "\n",
    "モノクロ画像なので、チャンネル側がありませんが、reshapeして、チャンネル方向に拡張してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 正規化\n",
    "X = (X - np.min(X)) / (np.max(X) - np.min(X))\n",
    "print(np.max(X)) # 1.0\n",
    "print(np.min(X)) # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797,)\n",
      "(1797, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "# one-hot-encoding\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "print(y.shape) # (60000,)\n",
    "print(y_one_hot.shape) # (60000, 10)\n",
    "print(y_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1347, 1, 8, 8)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【解答】conv関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    \n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    \n",
    "    def _calc_out_shape(self, Nin, fh, fw):\n",
    "        #NCHWの場合\n",
    "        h, w = Nin.shape[2],Nin.shape[3]\n",
    "\n",
    "        out_h = int((h + 2*self.pad - fh) / self.stride +1)\n",
    "        out_w = int((w + 2*self.pad - fw) / self.stride + 1)\n",
    "\n",
    "        return out_h, out_w\n",
    "\n",
    "    def forward(self, X):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W =X.shape\n",
    "        out_h, out_w = self._calc_out_shape(X, FH, FW)\n",
    "        \n",
    "        col = im2col(X, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T # フィルターの展開\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        print(\"after np.dot shape : {}\".format(out.shape))\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        print(\"out shape : {}\".format(out.shape))\n",
    "        \n",
    "        self.X = X\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1,FN)\n",
    "        \n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        \n",
    "        self.dW = self.dW.transpose(1,0).reshape(FN, C, FH, FW)\n",
    "        \n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        \n",
    "        dX = col2im(dcol, self.X.shape, FH, FW, self.stride, self.pad)\n",
    "        \n",
    "        return dX\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(32,1,3,3)\n",
    "#print(W.shape)\n",
    "#col_W = W.reshape(32, -1).T # フィルターの展開\n",
    "#col_W.shape\n",
    "b = np.zeros(32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_shape before transpose :(1347, 1, 3, 3, 6, 6)\n",
      "after np.dot shape : (48492, 32)\n",
      "out shape : (1347, 32, 6, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1347, 32, 6, 6)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Conv2d(W,b)\n",
    "out = test.forward(X_train)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1347, 1, 8, 8)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back = test.backward(out)\n",
    "back.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shapeの遷移は、\n",
    "\n",
    "* 入力時 : (1347,1,8,8)\n",
    "* forward propagation出力時 : (1347,32,6,6)\n",
    "* back propagation出力時 : (1347,1,8,8)\n",
    "\n",
    "となったので、数値は見ていないがshapeの動きは正しいと思われる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】2次元畳み込み後の出力サイズ\n",
    "畳み込みを行うと特徴マップのサイズが変化します。どのように変化するかは以下の数式から求められます。この計算を行う関数を作成してください。\n",
    "\n",
    "$$\n",
    "N_{h,out} = \\frac{N_{h,in}+2P_{h}-F_{h}}{S_{h}} + 1\\\\ N_{w,out} = \\frac{N_{w,in}+2P_{w}-F_{w}}{S_{w}} + 1\n",
    "$$\n",
    "\n",
    "$N_{out}$ : 出力のサイズ（特徴量の数）\n",
    "\n",
    "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
    "\n",
    "$P$ : ある方向へのパディングの数\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "$S$ : ストライドのサイズ\n",
    "\n",
    "$h$ が高さ方向、 $w$ が幅方向である"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【解答】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_out_shape(Nin, F=(3,3), P=(0, 0), S=(1,1)):\n",
    "    #NCHWの場合\n",
    "    h, w = Nin.shape[2],Nin.shape[3]\n",
    "    #print(h,w)\n",
    "    fh, fw = F[0], F[1]\n",
    "    ph, pw = P[0], P[1]\n",
    "    sh, sw = S[0], S[1]\n",
    "    \n",
    "    h_out = int((h + 2*ph - fh) / sh +1)\n",
    "    w_out = int((w + 2*pw - fw) / sw + 1)\n",
    "    \n",
    "    return h_out, w_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_out_shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】最大プーリング層の作成\n",
    "最大プーリング層のクラスMaxPool2Dを作成してください。プーリング層は数式で表さない方が分かりやすい部分もありますが、数式で表すとフォワードプロパゲーションは以下のようになります。\n",
    "\n",
    "$$\n",
    "a_{i,j,k} = \\max_{(p,q)\\in P_{i,j}}x_{p,q,k}\n",
    "$$\n",
    "\n",
    "$P_{i,j}$ : i行j列への出力する場合の入力配列のインデックスの集合。 $S_{h}×S_{w}$ の範囲内の行（p）と列（q）\n",
    "\n",
    "$S_{h}, S_{w}$  : 高さ方向（h）と幅方向（w）のストライドのサイズ\n",
    "\n",
    "$(p,q)\\in P_{i,j}$ : $P_{i,j}$ に含まれる行（p）と列（q）のインデックス\n",
    "\n",
    "$a_{i,j,m}$: 出力される配列のi行j列、kチャンネルの値\n",
    "\n",
    "$x_{p,q,k}$ : 入力の配列のp行q列、kチャンネルの値\n",
    "\n",
    "\n",
    "ある範囲の中でチャンネル方向の軸は残したまま最大値を計算することになります。\n",
    "\n",
    "\n",
    "バックプロパゲーションのためには、フォワードプロパゲーションのときの最大値のインデックス \n",
    "$(p,q)$ を保持しておく必要があります。フォワード時に最大値を持っていた箇所にそのままの誤差を流し、そこ以外には0を入れるためです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【解答】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.X = None\n",
    "        self.arg_max = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        N,C,H,W = X.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        # 展開(1)\n",
    "        col = im2col(X, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        #print(\"col:{}\".format(col.shape))\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        #print(\"im2col out & reshape:{}\\n\".format(col))\n",
    "        \n",
    "        arg_max = np.argmax(col, axis=1) # colの各配列のなかで一番大きい値のインデックスを持っている配列\n",
    "        #print(arg_max[0])\n",
    "        #最大値(2)\n",
    "        out = np.max(col, axis=1)\n",
    "        \n",
    "        #整形(3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        #print(\"out:\\n{}\\n\".format(out))\n",
    "\n",
    "        self.X = X\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1) # doutを順番入れ替え　(0,1,2,3) (0,3,1,2) (0,2,3,1) 元に戻ってる \n",
    "        print(\"transpose:{}\".format(dout.shape))\n",
    "        #print(\"dout transpose:{}\".format(dout))\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w # pool_sizeを計算\n",
    "        print(\"pool_size:{}\".format(pool_size))\n",
    "    \n",
    "        dmax = np.zeros((dout.size, pool_size)) # dmax = (doutのサイズ,プールサイズ)\n",
    "        #print(\"dmax_size:{}\".format(dmax.shape))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten() # dmaxの0~arg_max.size行の最大値インデックス列に、doutをflattenしたものを追加していく\n",
    "        print(\"dmax_size(flatten):{}\".format(dmax.shape))\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) # \n",
    "        print(\"dmax_size(reshape):{}\".format(dmax.shape))\n",
    "        #print(\"dmax:{}\".format(dmax))\n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1) # dcolはdmaxをreshapeして作る\n",
    "        print(\"dcol_size:{}\".format(dcol.shape))\n",
    "        #print(\"dcol:{}\".format(dcol))\n",
    "        dX = col2im(dcol, self.X.shape, self.pool_h, self.pool_w, self.stride, self.pad) # dcolをcol2imに通して出力\n",
    "        print(\"dX_size:{}\".format(dX.shape))\n",
    "        print(dX)\n",
    "        \n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[6, 0, 1, 2],\n",
       "         [1, 7, 2, 1],\n",
       "         [5, 6, 4, 2],\n",
       "         [6, 5, 2, 6]]]])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3 = np.random.randint(0,9,(1,1,4,4))\n",
    "test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = Pooling(2,2,stride=2, pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_shape before transpose :(1, 1, 2, 2, 2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1, 2, 2)"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_test = tmp.forward(test3)\n",
    "pool_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transpose:(1, 2, 2, 1)\n",
      "pool_size:4\n",
      "dmax_size(flatten):(4, 4)\n",
      "dmax_size(reshape):(1, 2, 2, 1, 4)\n",
      "dcol_size:(4, 4)\n",
      "dX_size:(1, 1, 4, 4)\n",
      "[[[[0. 0. 0. 2.]\n",
      "   [0. 7. 0. 0.]\n",
      "   [0. 6. 0. 0.]\n",
      "   [0. 0. 0. 6.]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1, 4, 4)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling_back = tmp.backward(pool_test)\n",
    "pooling_back.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】（アドバンス課題）平均プーリングの作成\n",
    "平均プーリング層のクラスAveragePool2Dを作成してください。\n",
    "\n",
    "\n",
    "範囲内の最大値ではなく、平均値を出力とするプーリング層です。\n",
    "\n",
    "\n",
    "画像認識関係では最大プーリング層が一般的で、平均プーリングはあまり使われません。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【解答】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Average_Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, X):\n",
    "        N,C,H,W = X.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        # 展開(1)\n",
    "        col = im2col(X, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        #print(col.shape)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        print(col[0])\n",
    "        #最大値(2)\n",
    "        out = np.mean(col, axis=1)\n",
    "        print(out[0])\n",
    "        \n",
    "        #整形(3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_shape before transpose :(1347, 32, 3, 3, 4, 4)\n",
      "[0.66466979 0.79953291 0.596617   0.45401189 0.95124222 0.98708122\n",
      " 0.31068093 1.20061788 0.87264838]\n",
      "0.7596780237365981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.75967802, 1.05466894, 1.19378094, 0.94502212],\n",
       "       [0.8407299 , 0.97617051, 1.16713542, 0.96694031],\n",
       "       [0.73403034, 0.86693414, 1.16479646, 1.06228871],\n",
       "       [0.58931271, 0.95872787, 1.34424983, 1.30689418]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = Average_Pooling(3,3)\n",
    "mean_pool = temp.forward(out)\n",
    "mean_pool[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】平滑化\n",
    "平滑化するためのFlattenクラスを作成してください。\n",
    "\n",
    "\n",
    "フォワードのときはチャンネル、高さ、幅の3次元を1次元にreshapeします。その値は記録しておき、バックワードのときに再びreshapeによって形を戻します。\n",
    "\n",
    "\n",
    "この平滑化のクラスを挟むことで出力前の全結合層に適した配列を作ることができます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【解答】平滑化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.shape = None\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        self.shape = Z.shape\n",
    "        return Z.reshape(self.shape[0], np.prod(self.shape[1:]))\n",
    "    \n",
    "    def backward(self, dX):\n",
    "        return dX.reshape(self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平滑化前:(1347, 32, 6, 6)\n",
      "平滑化関数出力:(1347, 1152)\n",
      "元に戻す:(1347, 32, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"平滑化前:{}\".format(out.shape))\n",
    "flat = Flatten()\n",
    "dx = flat.forward(out)\n",
    "print(\"平滑化関数出力:{}\".format(dx.shape))\n",
    "z = flat.backward(dx)\n",
    "print(\"元に戻す:{}\".format(z.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちゃんとshape変更できている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 時間計測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちょっと気になったので、np.ravelとnp.prodのどっちが早いのか測ってみたところ、prodの方が早かった。\n",
    "\n",
    "なので平滑化はprodで実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.3 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "# 簡略化のため1回分しか計測していないが、10000回やって平均とってもprodのが早かった。\n",
    "np.prod(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.59 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit  -r 1 -n 1\n",
    "np.ravel(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】学習と推定\n",
    "作成したConv2dを使用してMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "\n",
    "精度は低くともまずは動くことを目指してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNISTデータの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n",
      "(10000, 1, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train[:, np.newaxis, :, :]\n",
    "X_test = X_test[:, np.newaxis, :, :]\n",
    "\n",
    "print(X_train.shape) # (60000, 28, 28)\n",
    "print(X_test.shape) # (10000, 28, 28)\n",
    "print(X_train[0].dtype) # uint8\n",
    "#print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "print(y_train.shape) # (100000,)\n",
    "print(y_one_hot.shape) # (100000, 2)\n",
    "print(y_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 1, 28, 28)"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 10)"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    #print(input_data)\n",
    "    #print(\"im2col input shape :{}\".format(input_data.shape))\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "        \n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "        \n",
    "    #print(\"before transpose shape : {}\".format(col.shape))\n",
    "    #print(col)\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    #print(\"after im2col shape : {}\".format(col.shape))\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 入力データの形状（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)   # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "def softmax_loss(X, t):\n",
    "    y = softmax(X)\n",
    "    return cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU関数クラス\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.mask = (X <= 0)\n",
    "        out = X.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dX = dout\n",
    "        \n",
    "        return dX            \n",
    "\n",
    "\n",
    "# シグモイド関数クラス\n",
    "\n",
    "class Simoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.out = 1 / (1+np.exp(-X))\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = doutt * (1.0 - self.out) * self.out\n",
    "        return dX\n",
    "    \n",
    "    \n",
    "# 全結合層クラス\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.X = None\n",
    "        self.original_X_shape = None\n",
    "        # 重み・バイアスパラメータの微分\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # テンソル対応\n",
    "        self.original_X_shape = X.shape\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        self.X = X\n",
    "        \n",
    "        out = np.dot(self.X, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dout)\n",
    "        self.db =np.sum(dout, axis=0)\n",
    "        \n",
    "        dX = dX.reshape(*self.original_X_shape) # 入力データの形状に戻す(テンソル対応)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    \n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ\n",
    "        \n",
    "    def forward(self, X, t):\n",
    "        \"\"\"\n",
    "        X : 入力データ\n",
    "        t : 教師データ\n",
    "        \"\"\"\n",
    "        self.t = t\n",
    "        self.y = softmax(X)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\n",
    "            dX = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dX = self.y.copy()\n",
    "            dX[np.arrange(batch_size), self.t] -= 1\n",
    "            dX = dX / batch_size\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "\n",
    "    \n",
    "# 畳み込み層クラス\n",
    "\n",
    "class Conv2d:\n",
    "    \n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    \n",
    "    def _calc_out_shape(self, Nin, fh, fw):\n",
    "        #NCHWの場合\n",
    "        h, w = Nin.shape[2],Nin.shape[3]\n",
    "\n",
    "        out_h = int((h + 2*self.pad - fh) / self.stride +1)\n",
    "        out_w = int((w + 2*self.pad - fw) / self.stride + 1)\n",
    "\n",
    "        return out_h, out_w\n",
    "\n",
    "    def forward(self, X):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W =X.shape\n",
    "        out_h, out_w = self._calc_out_shape(X, FH, FW)\n",
    "        \n",
    "        col = im2col(X, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T # フィルターの展開\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        #print(\"after np.dot shape : {}\".format(out.shape))\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        #print(\"out shape : {}\".format(out.shape))\n",
    "        \n",
    "        self.X = X\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1,FN)\n",
    "        \n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        \n",
    "        self.dW = self.dW.transpose(1,0).reshape(FN, C, FH, FW)\n",
    "        \n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        \n",
    "        dX = col2im(dcol, self.X.shape, FH, FW, self.stride, self.pad)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    \n",
    "# プーリングクラス\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.X = None\n",
    "        self.arg_max = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        N,C,H,W = X.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        # 展開(1)\n",
    "        col = im2col(X, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        #print(\"col:{}\".format(col.shape))\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        #print(\"im2col out & reshape:{}\\n\".format(col))\n",
    "        \n",
    "        arg_max = np.argmax(col, axis=1) # colの各配列のなかで一番大きい値のインデックスを持っている配列\n",
    "        #print(arg_max[0])\n",
    "        #最大値(2)\n",
    "        out = np.max(col, axis=1)\n",
    "        \n",
    "        #整形(3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        #print(\"out:\\n{}\\n\".format(out))\n",
    "\n",
    "        self.X = X\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1) # doutを順番入れ替え　(0,1,2,3) (0,3,1,2) (0,2,3,1) 元に戻ってる \n",
    "        #print(\"transpose:{}\".format(dout.shape))\n",
    "        #print(\"dout transpose:{}\".format(dout))\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w # pool_sizeを計算\n",
    "        #print(\"pool_size:{}\".format(pool_size))\n",
    "    \n",
    "        dmax = np.zeros((dout.size, pool_size)) # dmax = (doutのサイズ,プールサイズ)\n",
    "        #print(\"dmax_size:{}\".format(dmax.shape))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten() # dmaxの0~arg_max.size行の最大値インデックス列に、doutをflattenしたものを追加していく\n",
    "        #print(\"dmax_size(flatten):{}\".format(dmax.shape))\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) # \n",
    "        #print(\"dmax_size(reshape):{}\".format(dmax.shape))\n",
    "        #print(\"dmax:{}\".format(dmax))\n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1) # dcolはdmaxをreshapeして作る\n",
    "        #print(\"dcol_size:{}\".format(dcol.shape))\n",
    "        #print(\"dcol:{}\".format(dcol))\n",
    "        dX = col2im(dcol, self.X.shape, self.pool_h, self.pool_w, self.stride, self.pad) # dcolをcol2imに通して出力\n",
    "        #print(\"dX_size:{}\".format(dX.shape))\n",
    "        #print(dX)\n",
    "        \n",
    "        return dX\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_param={'filter_num':32, 'filter_size':3, 'pad':0, 'stride':1}\n",
    "\n",
    "class SimpleConvnet:\n",
    "    \"\"\"単純なConvNet\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), conv_param=conv_param, hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param[\"filter_num\"]\n",
    "        filter_size = conv_param[\"filter_size\"]\n",
    "        filter_pad = conv_param[\"pad\"]\n",
    "        filter_stride = conv_param[\"stride\"]\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        \n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        # レイヤの作成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers[\"Conv1\"] = Conv2d(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        self.layers[\"ReLU1\"] = ReLU()\n",
    "        self.layers[\"Pool1\"] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers[\"Affine1\"] = Affine(self.params[\"W2\"], self.params[\"b2\"])\n",
    "        self.layers[\"ReLU2\"] = ReLU()\n",
    "        self.layers[\"Affine2\"] = Affine(self.params[\"W3\"], self.params[\"b3\"])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        print(\"レイヤの初期化完了\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        for layer in self.layers.values():\n",
    "            X = layer.forward(X)\n",
    "        \n",
    "        print(\"predict完了\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def loss(self, X, t):\n",
    "        \"\"\"\n",
    "        損失関数を求める\n",
    "        引数のXは入力データ、tは正解ラベル\n",
    "        例：loss(X_train, y_train)\n",
    "        \"\"\"\n",
    "        y = self.predict(X)\n",
    "        print(\"loss完了\")\n",
    "        \n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, X, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "            \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(X.shape[0] / batch_size)):\n",
    "            tX = X[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tX)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == y)\n",
    "        \n",
    "        print(\"accuracy完了\")\n",
    "        \n",
    "            \n",
    "        return acc / X.shape[0]\n",
    "\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizerクラスまとめ\n",
    "\n",
    "class SGD:\n",
    "\n",
    "    \"\"\"確率的勾配降下法（Stochastic Gradient Descent）\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] \n",
    "\n",
    "\n",
    "class Momentum:\n",
    "\n",
    "    \"\"\"Momentum SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]\n",
    "\n",
    "\n",
    "class Nesterov:\n",
    "\n",
    "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.v[key] *= self.momentum\n",
    "            self.v[key] -= self.lr * grads[key]\n",
    "            params[key] += self.momentum * self.momentum * self.v[key]\n",
    "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "\n",
    "    \"\"\"RMSprop\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"ニューラルネットの訓練を行うクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=5, mini_batch_size=100,\n",
    "                 optimizer='AdaGrad', optimizer_param={'lr':0.01}, \n",
    "                 evaluate_sample_num_per_epoch=None, verbose=False):\n",
    "        self.network = network\n",
    "        self.verbose = verbose\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
    "\n",
    "        # optimizer\n",
    "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
    "                                'adagrad':AdaGrad, 'rmsprop':RMSprop, 'adam':Adam}\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "        \n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
    "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "        \n",
    "        grads = self.network.gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.network.params, grads)\n",
    "        \n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "        if self.verbose: print(\"train loss:\" + str(loss))\n",
    "        \n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
    "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
    "            if not self.evaluate_sample_num_per_epoch is None:\n",
    "                t = self.evaluate_sample_num_per_epoch\n",
    "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
    "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
    "                \n",
    "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
    "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "\n",
    "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "レイヤの初期化完了\n",
      "train loss:1.762868366960376\n",
      "=== epoch:1, train acc:1.0, test acc:1.0 ===\n",
      "train loss:1.5933201131125774\n",
      "train loss:1.2186131339890114\n",
      "train loss:1.4055079009052418\n",
      "train loss:1.2899185945670038\n",
      "train loss:0.9419109445011642\n",
      "train loss:0.8701744106249886\n",
      "train loss:0.6756982727728529\n",
      "train loss:0.6616575196307791\n",
      "train loss:0.7304833546122513\n",
      "train loss:0.6064031270388559\n",
      "train loss:0.5101162456422008\n",
      "train loss:0.3929089280020618\n",
      "train loss:0.5160390102328095\n",
      "train loss:0.4660951978866461\n",
      "train loss:0.5376439072735715\n",
      "train loss:0.6770273440683148\n",
      "train loss:0.6424332186548385\n",
      "train loss:0.37102353665017956\n",
      "train loss:0.4564794306928324\n",
      "train loss:0.4426406704747609\n",
      "train loss:0.5347797034250217\n",
      "train loss:0.36968822700734705\n",
      "train loss:0.31795562734416516\n",
      "train loss:0.29857844787189425\n",
      "train loss:0.4215425858031072\n",
      "train loss:0.4147275672088738\n",
      "train loss:0.40425648376160744\n",
      "train loss:0.2290886531389255\n",
      "train loss:0.28346601621196055\n",
      "train loss:0.25624956486647027\n",
      "train loss:0.26323870894208284\n",
      "train loss:0.33577285066759477\n",
      "train loss:0.2349418209062925\n",
      "train loss:0.34003910931358666\n",
      "train loss:0.3093801993772788\n",
      "train loss:0.30591356283789506\n",
      "train loss:0.2776701362526164\n",
      "train loss:0.23644379497433832\n",
      "train loss:0.3378604015410531\n",
      "train loss:0.25992745244222926\n",
      "train loss:0.18807535785445315\n",
      "train loss:0.29627418504475034\n",
      "train loss:0.21216219025771813\n",
      "train loss:0.30133174473213353\n",
      "train loss:0.18698032219694188\n",
      "train loss:0.3863778720904804\n",
      "train loss:0.1850167477052007\n",
      "train loss:0.226832804772341\n",
      "train loss:0.28560665638619315\n",
      "train loss:0.2151277209080586\n",
      "train loss:0.16357209560763725\n",
      "train loss:0.20347513285265048\n",
      "train loss:0.16909008075125662\n",
      "train loss:0.20865093720961136\n",
      "train loss:0.36893791279557037\n",
      "train loss:0.24789374734467284\n",
      "train loss:0.24594872393423922\n",
      "train loss:0.1263789240553238\n",
      "train loss:0.16109461093141694\n",
      "train loss:0.2865948836898018\n",
      "train loss:0.16663813218129522\n",
      "train loss:0.15085584172365848\n",
      "train loss:0.2590884382703813\n",
      "train loss:0.2758669661394773\n",
      "train loss:0.23057853221746316\n",
      "train loss:0.20673520161662778\n",
      "train loss:0.1623617313831181\n",
      "train loss:0.19703129596183286\n",
      "train loss:0.25679553124863436\n",
      "train loss:0.19891603123897578\n",
      "train loss:0.11154852080286963\n",
      "train loss:0.2331568162768792\n",
      "train loss:0.17925640223119008\n",
      "train loss:0.22003650047819806\n",
      "train loss:0.18418652163615165\n",
      "train loss:0.1717127440403163\n",
      "train loss:0.1322779929527784\n",
      "train loss:0.2414681933804818\n",
      "train loss:0.23401785405482867\n",
      "train loss:0.1906914209647837\n",
      "train loss:0.16119115519232274\n",
      "train loss:0.14713472075125422\n",
      "train loss:0.25948545445180465\n",
      "train loss:0.15581979168471596\n",
      "train loss:0.21597442384351986\n",
      "train loss:0.19776323034463306\n",
      "train loss:0.20755853783717382\n",
      "train loss:0.20785212416847415\n",
      "train loss:0.12294889892456831\n",
      "train loss:0.15084541699052803\n",
      "train loss:0.23776198390058295\n",
      "train loss:0.13624986234094516\n",
      "train loss:0.29133638966884534\n",
      "train loss:0.09842553412893194\n",
      "train loss:0.19591042124404356\n",
      "train loss:0.1804712845549265\n",
      "train loss:0.33940567220102236\n",
      "train loss:0.18505696743450828\n",
      "train loss:0.15792215026498094\n",
      "train loss:0.14458599149898965\n",
      "train loss:0.15939321735603498\n",
      "train loss:0.12105045346951432\n",
      "train loss:0.17747510873083427\n",
      "train loss:0.18164282680664295\n",
      "train loss:0.13702901707513318\n",
      "train loss:0.1724157575203728\n",
      "train loss:0.18330071940639578\n",
      "train loss:0.20776436894885209\n",
      "train loss:0.17551297338959546\n",
      "train loss:0.1545653018876277\n",
      "train loss:0.23739865929926027\n",
      "train loss:0.10821429560450499\n",
      "train loss:0.15960401341070973\n",
      "train loss:0.16506543213020652\n",
      "train loss:0.15416834712816738\n",
      "train loss:0.21399849751745895\n",
      "train loss:0.3104729379955235\n",
      "train loss:0.2857266627210058\n",
      "train loss:0.10884942724395946\n",
      "train loss:0.07010189584783581\n",
      "train loss:0.07421343054833845\n",
      "train loss:0.1642720837186577\n",
      "train loss:0.11745292580937418\n",
      "train loss:0.13529561934032194\n",
      "train loss:0.1637460824753583\n",
      "train loss:0.1066645084457543\n",
      "train loss:0.14722302021695344\n",
      "train loss:0.08448052861958749\n",
      "train loss:0.11479261382517227\n",
      "train loss:0.13483243394931566\n",
      "train loss:0.10933041648722516\n",
      "train loss:0.15442892722965568\n",
      "train loss:0.1306087429672325\n",
      "train loss:0.15921284739009267\n",
      "train loss:0.1011169877480451\n",
      "train loss:0.1538439116513893\n",
      "train loss:0.19518995659927274\n",
      "train loss:0.16739348659192133\n",
      "train loss:0.1572983645643431\n",
      "train loss:0.12279989473967508\n",
      "train loss:0.06229956704652185\n",
      "train loss:0.29749712090235375\n",
      "train loss:0.12650029774375937\n",
      "train loss:0.12413666117249188\n",
      "train loss:0.12525148908754985\n",
      "train loss:0.2936918383584295\n",
      "train loss:0.1565778618894021\n",
      "train loss:0.14664215834850972\n",
      "train loss:0.12387595718145261\n",
      "train loss:0.14964967026159828\n",
      "train loss:0.11252585751008605\n",
      "train loss:0.09906176198294483\n",
      "train loss:0.11330159517952668\n",
      "train loss:0.08028580107663558\n",
      "train loss:0.13945178401622038\n",
      "train loss:0.11419089787030742\n",
      "train loss:0.1688834906788815\n",
      "train loss:0.058311085719186126\n",
      "train loss:0.07243096987355656\n",
      "train loss:0.19406167762164778\n",
      "train loss:0.16940691939029107\n",
      "train loss:0.14146771075800937\n",
      "train loss:0.10399332293280247\n",
      "train loss:0.10017976461223219\n",
      "train loss:0.11932192040239045\n",
      "train loss:0.07609443167848025\n",
      "train loss:0.11118866475052268\n",
      "train loss:0.1634165880076611\n",
      "train loss:0.07951622841851154\n",
      "train loss:0.1074802740884698\n",
      "train loss:0.09756147072265535\n",
      "train loss:0.1525627064426883\n",
      "train loss:0.16911531591604936\n",
      "train loss:0.0998488300418313\n",
      "train loss:0.09986599893596697\n",
      "train loss:0.2460753373333085\n",
      "train loss:0.1123715301114097\n",
      "train loss:0.07299129744188543\n",
      "train loss:0.12361691504333716\n",
      "train loss:0.1019319455144815\n",
      "train loss:0.17061449643379617\n",
      "train loss:0.15760370368763296\n",
      "train loss:0.12290123994519637\n",
      "train loss:0.21844860684023382\n",
      "train loss:0.18225862685443164\n",
      "train loss:0.12458297900109777\n",
      "train loss:0.10109727842845107\n",
      "train loss:0.06088324296546587\n",
      "train loss:0.12256266251962643\n",
      "train loss:0.12570143908056022\n",
      "train loss:0.07573309624727721\n",
      "train loss:0.15583515617961932\n",
      "train loss:0.0961675943655293\n",
      "train loss:0.179115429270795\n",
      "train loss:0.08326461290823406\n",
      "train loss:0.09325026702964997\n",
      "train loss:0.1461320487385852\n",
      "train loss:0.09077210044368231\n",
      "train loss:0.1417004634147998\n",
      "train loss:0.14193975950093918\n",
      "train loss:0.10103170580638725\n",
      "train loss:0.10126774879032763\n",
      "train loss:0.10997515660529604\n",
      "train loss:0.13598848247245346\n",
      "train loss:0.07969588850399688\n",
      "train loss:0.09492346900476255\n",
      "train loss:0.16716864577160595\n",
      "train loss:0.15222172158766326\n",
      "train loss:0.11087452602049806\n",
      "train loss:0.18715928887225824\n",
      "train loss:0.06919031761558367\n",
      "train loss:0.11455364713447319\n",
      "train loss:0.09134980278625865\n",
      "train loss:0.1300681747867224\n",
      "train loss:0.06635137998646645\n",
      "train loss:0.08422038805657146\n",
      "train loss:0.10861708164853379\n",
      "train loss:0.1343043909239742\n",
      "train loss:0.21753085873470104\n",
      "train loss:0.17122156108184605\n",
      "train loss:0.10280091973890122\n",
      "train loss:0.1279077938558425\n",
      "train loss:0.12992439900135797\n",
      "train loss:0.12302714617523522\n",
      "train loss:0.11101238035871991\n",
      "train loss:0.14345202360028575\n",
      "train loss:0.13472779699402232\n",
      "train loss:0.18037733081083807\n",
      "train loss:0.04559811698407459\n",
      "train loss:0.15753232737342257\n",
      "train loss:0.07598390752268283\n",
      "train loss:0.10868893692913799\n",
      "train loss:0.18515700452663986\n",
      "train loss:0.07429992353381279\n",
      "train loss:0.15963441961645353\n",
      "train loss:0.10472547466109867\n",
      "train loss:0.11129692477479276\n",
      "train loss:0.058145250656137425\n",
      "train loss:0.12006571933416103\n",
      "train loss:0.14433332914803984\n",
      "train loss:0.11861108745224717\n",
      "train loss:0.08352861059253086\n",
      "train loss:0.18498999709690675\n",
      "train loss:0.15776074517824312\n",
      "train loss:0.10087606323319602\n",
      "train loss:0.11511585949597962\n",
      "train loss:0.08600365833927164\n",
      "train loss:0.13471308944011962\n",
      "train loss:0.1089673037448876\n",
      "train loss:0.14686846772330514\n",
      "train loss:0.18860119772509912\n",
      "train loss:0.0956493504001954\n",
      "train loss:0.1500568389308423\n",
      "train loss:0.18736311791188506\n",
      "train loss:0.1238270755963006\n",
      "train loss:0.12501923078539504\n",
      "train loss:0.13141585807711842\n",
      "train loss:0.0857387322158119\n",
      "train loss:0.12234147478335652\n",
      "train loss:0.1961172456858998\n",
      "train loss:0.11204611462363148\n",
      "train loss:0.08551495910264675\n",
      "train loss:0.13850246296581903\n",
      "train loss:0.10552748186716057\n",
      "train loss:0.0751399066114497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1303592169573185\n",
      "train loss:0.08434251383580635\n",
      "train loss:0.15014088936837103\n",
      "train loss:0.06957763895670137\n",
      "train loss:0.186395382690232\n",
      "train loss:0.08073110842232813\n",
      "train loss:0.16446878535982778\n",
      "train loss:0.1017959465888735\n",
      "train loss:0.06590517422702431\n",
      "train loss:0.11053755258021593\n",
      "train loss:0.11964996969869611\n",
      "train loss:0.09765737993661446\n",
      "train loss:0.13533055369032415\n",
      "train loss:0.11904923442446558\n",
      "train loss:0.08106426918523862\n",
      "train loss:0.1278549525163004\n",
      "train loss:0.08552533602254696\n",
      "train loss:0.16791938707173693\n",
      "train loss:0.1714043221349077\n",
      "train loss:0.10153332194529512\n",
      "train loss:0.09889110552441346\n",
      "train loss:0.22543608334369714\n",
      "train loss:0.10224310428898276\n",
      "train loss:0.08932070975043802\n",
      "train loss:0.05861252277347126\n",
      "train loss:0.07931461639548934\n",
      "train loss:0.044538021421600994\n",
      "train loss:0.15409942956684297\n",
      "train loss:0.10138708387022335\n",
      "train loss:0.14902156718794987\n",
      "train loss:0.10775454231850354\n",
      "train loss:0.11822556679562247\n",
      "train loss:0.12338056042632507\n",
      "train loss:0.1213578018064154\n",
      "train loss:0.0449928131907047\n",
      "train loss:0.21588729360195594\n",
      "train loss:0.08795335301528658\n",
      "train loss:0.08980700032226194\n",
      "train loss:0.08909150799302332\n",
      "train loss:0.05150547044561732\n",
      "train loss:0.11386627673065744\n",
      "train loss:0.16961689653743608\n",
      "train loss:0.11157920046459525\n",
      "train loss:0.06076346932719944\n",
      "train loss:0.09519057698950542\n",
      "train loss:0.1107240428230148\n",
      "train loss:0.07131317494561254\n",
      "train loss:0.09866673459707372\n",
      "train loss:0.10393199109927002\n",
      "train loss:0.07764449102346065\n",
      "train loss:0.0738147423873594\n",
      "train loss:0.06389164115063493\n",
      "train loss:0.13733897028005468\n",
      "train loss:0.18772815900710502\n",
      "train loss:0.11306589215890707\n",
      "train loss:0.11526720330701254\n",
      "train loss:0.10261719052578744\n",
      "train loss:0.09662439433643061\n",
      "train loss:0.07189203278488292\n",
      "train loss:0.05350411597329651\n",
      "train loss:0.10493685132386797\n",
      "train loss:0.05058573023249067\n",
      "train loss:0.13279271653931196\n",
      "train loss:0.20237733231598734\n",
      "train loss:0.22545876631867062\n",
      "train loss:0.06078135450836778\n",
      "train loss:0.07701446347745398\n",
      "train loss:0.06746061702412577\n",
      "train loss:0.1355710380545642\n",
      "train loss:0.08683176219519692\n",
      "train loss:0.1412917829023777\n",
      "train loss:0.034913810677834\n",
      "train loss:0.058776736831030345\n",
      "train loss:0.20477940943700917\n",
      "train loss:0.20231345103220102\n",
      "train loss:0.11543979562879686\n",
      "train loss:0.08777506830745285\n",
      "train loss:0.09944244944343783\n",
      "train loss:0.10885532258719985\n",
      "train loss:0.19281916537945162\n",
      "train loss:0.035237178052718365\n",
      "train loss:0.13480096217837287\n",
      "train loss:0.12821588859868754\n",
      "train loss:0.06391305025990762\n",
      "train loss:0.09037373815807558\n",
      "train loss:0.12182595244296378\n",
      "train loss:0.12001114360962291\n",
      "train loss:0.04040027809388978\n",
      "train loss:0.08166583279283923\n",
      "train loss:0.0781981994957228\n",
      "train loss:0.06100563550309386\n",
      "train loss:0.1308102917556289\n",
      "train loss:0.07644574735957498\n",
      "train loss:0.13466101584902987\n",
      "train loss:0.10656651397252441\n",
      "train loss:0.12015524631981993\n",
      "train loss:0.027337990307126345\n",
      "train loss:0.03580597196630506\n",
      "train loss:0.11422359669930339\n",
      "train loss:0.055748110074838035\n",
      "train loss:0.12041419891812419\n",
      "train loss:0.09507128152675964\n",
      "train loss:0.11066597339869812\n",
      "train loss:0.036570569791934\n",
      "train loss:0.19733380633733794\n",
      "train loss:0.09103042171196861\n",
      "train loss:0.1157228322091355\n",
      "train loss:0.03737079809913166\n",
      "train loss:0.055245748792869635\n",
      "train loss:0.12724663475469733\n",
      "train loss:0.11399710617835689\n",
      "train loss:0.18146238328756994\n",
      "train loss:0.12707964903585106\n",
      "train loss:0.0844901826756549\n",
      "train loss:0.12587192156572816\n",
      "train loss:0.07036872526871064\n",
      "train loss:0.10171014552780398\n",
      "train loss:0.0775357351981972\n",
      "train loss:0.12110050967997056\n",
      "train loss:0.07449317080578789\n",
      "train loss:0.10160068998123087\n",
      "train loss:0.08668976074332438\n",
      "train loss:0.07566893235857779\n",
      "train loss:0.2037342527056805\n",
      "train loss:0.14496899867336582\n",
      "train loss:0.041723096305093306\n",
      "train loss:0.05139977568984966\n",
      "train loss:0.10401276817761823\n",
      "train loss:0.05777365059893567\n",
      "train loss:0.07054165336574737\n",
      "train loss:0.03157883165940442\n",
      "train loss:0.09077414603710293\n",
      "train loss:0.0917588820612128\n",
      "train loss:0.2986874246241142\n",
      "train loss:0.06077676558177014\n",
      "train loss:0.07725143888580073\n",
      "train loss:0.0319145725920419\n",
      "train loss:0.07141334771729108\n",
      "train loss:0.1377128060764894\n",
      "train loss:0.10234800947020636\n",
      "train loss:0.05032175346692202\n",
      "train loss:0.09395306009487055\n",
      "train loss:0.04421781089481053\n",
      "train loss:0.10809187741223879\n",
      "train loss:0.18308618191237627\n",
      "train loss:0.13481919809525267\n",
      "train loss:0.1940217425458941\n",
      "train loss:0.13017703856223592\n",
      "train loss:0.17632188019600797\n",
      "train loss:0.11691056348380904\n",
      "train loss:0.12482037946030358\n",
      "train loss:0.0925345610302923\n",
      "train loss:0.09538139071919094\n",
      "train loss:0.10201463151063413\n",
      "train loss:0.06568766545418407\n",
      "train loss:0.11205450294901223\n",
      "train loss:0.0773050852509368\n",
      "train loss:0.11893263047844396\n",
      "train loss:0.05879026584997775\n",
      "train loss:0.03783328127778653\n",
      "train loss:0.04508866659622122\n",
      "train loss:0.04956438832733195\n",
      "train loss:0.1673186656658071\n",
      "train loss:0.09987321564790783\n",
      "train loss:0.07159690164431855\n",
      "train loss:0.11055650732849046\n",
      "train loss:0.05335535651915408\n",
      "train loss:0.03901648585125753\n",
      "train loss:0.16762234012020408\n",
      "train loss:0.058339221213563476\n",
      "train loss:0.10024494031185485\n",
      "train loss:0.040392324443831985\n",
      "train loss:0.13431778741238826\n",
      "train loss:0.08692143696639786\n",
      "train loss:0.06479076877516608\n",
      "train loss:0.08242178391054807\n",
      "train loss:0.04032374837900062\n",
      "train loss:0.09927466758299781\n",
      "train loss:0.1397526659287352\n",
      "train loss:0.06922372364361105\n",
      "train loss:0.08300267565565113\n",
      "train loss:0.0744865463399263\n",
      "train loss:0.07181209938142985\n",
      "train loss:0.21002012216597396\n",
      "train loss:0.06579950560610472\n",
      "=== epoch:2, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.1044281802249669\n",
      "train loss:0.08272618105662838\n",
      "train loss:0.05417804496543582\n",
      "train loss:0.15252401732731286\n",
      "train loss:0.11519288518455002\n",
      "train loss:0.0791174843429026\n",
      "train loss:0.1409779170563924\n",
      "train loss:0.15120281480413328\n",
      "train loss:0.13825503889082924\n",
      "train loss:0.1272741499052754\n",
      "train loss:0.06608865185064518\n",
      "train loss:0.10055572653837638\n",
      "train loss:0.05727264987319287\n",
      "train loss:0.11719366621478133\n",
      "train loss:0.03384436725139389\n",
      "train loss:0.06412397323062319\n",
      "train loss:0.10140024079703534\n",
      "train loss:0.16570798419329308\n",
      "train loss:0.09638057410315129\n",
      "train loss:0.0627268797005764\n",
      "train loss:0.107841119262738\n",
      "train loss:0.10979456374803037\n",
      "train loss:0.04385720209660651\n",
      "train loss:0.06332443306049647\n",
      "train loss:0.04078425855322107\n",
      "train loss:0.14351112797443813\n",
      "train loss:0.08682946767009467\n",
      "train loss:0.1123784818669381\n",
      "train loss:0.07281479382544126\n",
      "train loss:0.04899445577807869\n",
      "train loss:0.10929428100980296\n",
      "train loss:0.11498929004788483\n",
      "train loss:0.0934387953452071\n",
      "train loss:0.071733701064169\n",
      "train loss:0.11552799367624106\n",
      "train loss:0.10568852770009904\n",
      "train loss:0.1035194742208793\n",
      "train loss:0.05864968427081061\n",
      "train loss:0.08073798066430254\n",
      "train loss:0.09257053119472564\n",
      "train loss:0.09508468432851601\n",
      "train loss:0.07995371035231474\n",
      "train loss:0.06975469510480951\n",
      "train loss:0.12107260994951079\n",
      "train loss:0.07851525211271743\n",
      "train loss:0.14352381774150338\n",
      "train loss:0.08698385243806628\n",
      "train loss:0.08340509142022096\n",
      "train loss:0.06949635234531888\n",
      "train loss:0.10928626332627629\n",
      "train loss:0.06780104162111127\n",
      "train loss:0.09950007393914666\n",
      "train loss:0.06140911908744798\n",
      "train loss:0.11288036309975437\n",
      "train loss:0.07817849357248192\n",
      "train loss:0.14001928323413715\n",
      "train loss:0.07042002774087595\n",
      "train loss:0.09225532067583561\n",
      "train loss:0.05322166508604433\n",
      "train loss:0.0661873773851039\n",
      "train loss:0.05244976872379688\n",
      "train loss:0.05268795268892337\n",
      "train loss:0.1458217456142875\n",
      "train loss:0.055549407931926614\n",
      "train loss:0.14724713229776637\n",
      "train loss:0.07798050285996506\n",
      "train loss:0.054247548976212964\n",
      "train loss:0.05530594327912474\n",
      "train loss:0.07735143004852685\n",
      "train loss:0.04492455345617327\n",
      "train loss:0.05808948956405844\n",
      "train loss:0.11463196749026917\n",
      "train loss:0.047044819691613914\n",
      "train loss:0.1585669856217993\n",
      "train loss:0.050548165495183206\n",
      "train loss:0.06724860723681714\n",
      "train loss:0.07574790422281634\n",
      "train loss:0.14273753507163384\n",
      "train loss:0.04389044105526392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.08738123249443613\n",
      "train loss:0.07313225908137765\n",
      "train loss:0.13263024636346663\n",
      "train loss:0.0561921949657589\n",
      "train loss:0.04617353100649347\n",
      "train loss:0.06372634694401975\n",
      "train loss:0.04636868256874831\n",
      "train loss:0.06239454065082992\n",
      "train loss:0.06152111082929757\n",
      "train loss:0.08043517456323858\n",
      "train loss:0.11623716960911842\n",
      "train loss:0.06313208884676312\n",
      "train loss:0.050662098563806664\n",
      "train loss:0.07805144540175117\n",
      "train loss:0.07146741596666935\n",
      "train loss:0.07645297585277996\n",
      "train loss:0.12193550415014891\n",
      "train loss:0.08360963105574619\n",
      "train loss:0.038419887713927035\n",
      "train loss:0.04486447514568168\n",
      "train loss:0.07219495900559317\n",
      "train loss:0.056524784600016885\n",
      "train loss:0.05517809738028998\n",
      "train loss:0.033024492326882475\n",
      "train loss:0.12103829530704009\n",
      "train loss:0.12518671530389658\n",
      "train loss:0.09913526979961565\n",
      "train loss:0.08465410382672013\n",
      "train loss:0.119690247949805\n",
      "train loss:0.09966685672657953\n",
      "train loss:0.07224443956412531\n",
      "train loss:0.08088518321381458\n",
      "train loss:0.05627423959498514\n",
      "train loss:0.07149844071698962\n",
      "train loss:0.10310270927841386\n",
      "train loss:0.05839093774432486\n",
      "train loss:0.0879547330800219\n",
      "train loss:0.08350526986495002\n",
      "train loss:0.05716272849259346\n",
      "train loss:0.10444247358852976\n",
      "train loss:0.06317592657616528\n",
      "train loss:0.03523675957452903\n",
      "train loss:0.057684144056574134\n",
      "train loss:0.06163023142464131\n",
      "train loss:0.13645519876508186\n",
      "train loss:0.07415633575497732\n",
      "train loss:0.05326859495721184\n",
      "train loss:0.03377398122934121\n",
      "train loss:0.08993985882749202\n",
      "train loss:0.07925810061753283\n",
      "train loss:0.08658948246990247\n",
      "train loss:0.1265380262563573\n",
      "train loss:0.07039550922439157\n",
      "train loss:0.024241448131592395\n",
      "train loss:0.06672389758759646\n",
      "train loss:0.10244179171864128\n",
      "train loss:0.058621112315362565\n",
      "train loss:0.05905699280553543\n",
      "train loss:0.07959022216705751\n",
      "train loss:0.0631056807616512\n",
      "train loss:0.06052368042331544\n",
      "train loss:0.07586745128798816\n",
      "train loss:0.14904725705834065\n",
      "train loss:0.16508116014695481\n",
      "train loss:0.03544250296156107\n",
      "train loss:0.12088944213863735\n",
      "train loss:0.04151142858891538\n",
      "train loss:0.06979124271238116\n",
      "train loss:0.1347136156738225\n",
      "train loss:0.048066548243784146\n",
      "train loss:0.09169353930152077\n",
      "train loss:0.04109954251061182\n",
      "train loss:0.06709979398288811\n",
      "train loss:0.09243509196841748\n",
      "train loss:0.03936780200238503\n",
      "train loss:0.07012471438142737\n",
      "train loss:0.11630798263379624\n",
      "train loss:0.042916072062762795\n",
      "train loss:0.12219886943107014\n",
      "train loss:0.044068163844053966\n",
      "train loss:0.08229556867268999\n",
      "train loss:0.07642639921638093\n",
      "train loss:0.06551549658089242\n",
      "train loss:0.1405177137001969\n",
      "train loss:0.04808248852572926\n",
      "train loss:0.1003388635850299\n",
      "train loss:0.1264179671144588\n",
      "train loss:0.03948932577300203\n",
      "train loss:0.04536696566926247\n",
      "train loss:0.07106098066116834\n",
      "train loss:0.044841767924603086\n",
      "train loss:0.03813803193516914\n",
      "train loss:0.11772993645358251\n",
      "train loss:0.07807268541961171\n",
      "train loss:0.04509540697986452\n",
      "train loss:0.07984480834030507\n",
      "train loss:0.055068758763518154\n",
      "train loss:0.12825664420019078\n",
      "train loss:0.04370605376431791\n",
      "train loss:0.09596931455790507\n",
      "train loss:0.09884986893923381\n",
      "train loss:0.06297630981266863\n",
      "train loss:0.10685586333390332\n",
      "train loss:0.10618701170087302\n",
      "train loss:0.10275914640363881\n",
      "train loss:0.12879478980570663\n",
      "train loss:0.06892886616719955\n",
      "train loss:0.04940557365587749\n",
      "train loss:0.05771098219194072\n",
      "train loss:0.12518241867838617\n",
      "train loss:0.05958242255088466\n",
      "train loss:0.06304225083077951\n",
      "train loss:0.09655488629172317\n",
      "train loss:0.06347444573083041\n",
      "train loss:0.052672499608200765\n",
      "train loss:0.14945209871201942\n",
      "train loss:0.12674302633747106\n",
      "train loss:0.10948352544678977\n",
      "train loss:0.05840409927124354\n",
      "train loss:0.05714679635110939\n",
      "train loss:0.06796447838168143\n",
      "train loss:0.132739393568185\n",
      "train loss:0.06646014770216867\n",
      "train loss:0.06596780764484433\n",
      "train loss:0.04090745591420718\n",
      "train loss:0.0444310967464914\n",
      "train loss:0.07037041434260416\n",
      "train loss:0.05438089928846524\n",
      "train loss:0.14322284298461832\n",
      "train loss:0.09083632210968151\n",
      "train loss:0.0953303614529404\n",
      "train loss:0.14576609315794045\n",
      "train loss:0.11256745736556294\n",
      "train loss:0.04240102624650759\n",
      "train loss:0.07135462503674303\n",
      "train loss:0.0515961607370975\n",
      "train loss:0.04114788293491206\n",
      "train loss:0.0982763646256212\n",
      "train loss:0.03683821151194679\n",
      "train loss:0.08767163071323561\n",
      "train loss:0.04698064891739491\n",
      "train loss:0.0560088186000759\n",
      "train loss:0.10890333988667265\n",
      "train loss:0.03939333496413416\n",
      "train loss:0.04344409130282441\n",
      "train loss:0.06771742114640288\n",
      "train loss:0.06573385874874678\n",
      "train loss:0.16366947365180046\n",
      "train loss:0.05837253931843271\n",
      "train loss:0.06701022447571099\n",
      "train loss:0.06536303541154019\n",
      "train loss:0.03117550374041018\n",
      "train loss:0.06346709435959054\n",
      "train loss:0.08351595469382865\n",
      "train loss:0.04710846882536497\n",
      "train loss:0.06105426264444302\n",
      "train loss:0.10887521213335609\n",
      "train loss:0.04561001414079523\n",
      "train loss:0.07451955481192979\n",
      "train loss:0.11073659888028495\n",
      "train loss:0.11082877792091655\n",
      "train loss:0.03385054795556689\n",
      "train loss:0.0775782643011829\n",
      "train loss:0.05228795360001545\n",
      "train loss:0.05509097970340963\n",
      "train loss:0.06525978210363313\n",
      "train loss:0.055567963571685135\n",
      "train loss:0.03070177743450363\n",
      "train loss:0.03819795202036285\n",
      "train loss:0.06436923014370734\n",
      "train loss:0.027078474409634415\n",
      "train loss:0.079948036606984\n",
      "train loss:0.07251889655391011\n",
      "train loss:0.06681461373650034\n",
      "train loss:0.05338755765765454\n",
      "train loss:0.04623355748251758\n",
      "train loss:0.0460441075650779\n",
      "train loss:0.047213143183472826\n",
      "train loss:0.10051467655249531\n",
      "train loss:0.08890840000180975\n",
      "train loss:0.10294665274607644\n",
      "train loss:0.038186843747823794\n",
      "train loss:0.08891392512145382\n",
      "train loss:0.04626482185184166\n",
      "train loss:0.038494741152939535\n",
      "train loss:0.10389859594122161\n",
      "train loss:0.06711123971712794\n",
      "train loss:0.06376017888584118\n",
      "train loss:0.1406709718653214\n",
      "train loss:0.05686264541948761\n",
      "train loss:0.09714349167414\n",
      "train loss:0.049653761449824314\n",
      "train loss:0.13928930455464722\n",
      "train loss:0.053691628047867895\n",
      "train loss:0.05258998109820185\n",
      "train loss:0.07656449401854183\n",
      "train loss:0.05764607551303712\n",
      "train loss:0.09013193313619927\n",
      "train loss:0.12762869821766026\n",
      "train loss:0.14375890838485547\n",
      "train loss:0.0648830755636776\n",
      "train loss:0.0286026792654867\n",
      "train loss:0.0497459467159909\n",
      "train loss:0.09478196384543995\n",
      "train loss:0.19030828296483854\n",
      "train loss:0.059255168237722894\n",
      "train loss:0.06651830763794901\n",
      "train loss:0.08050224469708285\n",
      "train loss:0.0835654309736723\n",
      "train loss:0.08469605614787236\n",
      "train loss:0.08236911743018024\n",
      "train loss:0.059736967092833125\n",
      "train loss:0.046303904384275535\n",
      "train loss:0.08352591343250391\n",
      "train loss:0.12412262723349564\n",
      "train loss:0.044875695940171434\n",
      "train loss:0.11268201226987655\n",
      "train loss:0.06394213441785432\n",
      "train loss:0.1103100519948364\n",
      "train loss:0.07585503203913067\n",
      "train loss:0.09754414914398757\n",
      "train loss:0.015534166842365651\n",
      "train loss:0.0600323518624577\n",
      "train loss:0.0884617087203815\n",
      "train loss:0.05289860043303901\n",
      "train loss:0.07358184004873529\n",
      "train loss:0.08752289990518515\n",
      "train loss:0.06663811506562317\n",
      "train loss:0.07783411994943776\n",
      "train loss:0.03972260993623149\n",
      "train loss:0.13586254018684785\n",
      "train loss:0.12221354854117741\n",
      "train loss:0.09229901346954687\n",
      "train loss:0.0600713737891783\n",
      "train loss:0.05303110883062858\n",
      "train loss:0.04752652756858291\n",
      "train loss:0.05515399997376758\n",
      "train loss:0.03925900414782639\n",
      "train loss:0.056657887297779326\n",
      "train loss:0.04082232373569838\n",
      "train loss:0.07991564722841642\n",
      "train loss:0.03404570419674542\n",
      "train loss:0.06637779948079482\n",
      "train loss:0.1082890771427917\n",
      "train loss:0.02365001381984678\n",
      "train loss:0.05966034625213854\n",
      "train loss:0.039274446422919766\n",
      "train loss:0.0932222764026624\n",
      "train loss:0.05413078162526434\n",
      "train loss:0.04323205510059677\n",
      "train loss:0.0519865018009708\n",
      "train loss:0.06703986196526142\n",
      "train loss:0.082948712858284\n",
      "train loss:0.021963272454430215\n",
      "train loss:0.03814249809915638\n",
      "train loss:0.09405477541472478\n",
      "train loss:0.15169690479693299\n",
      "train loss:0.06634218834935351\n",
      "train loss:0.060672510066806476\n",
      "train loss:0.02310793306218828\n",
      "train loss:0.048295761565001157\n",
      "train loss:0.05524619780100774\n",
      "train loss:0.04026189566885328\n",
      "train loss:0.03924940743665337\n",
      "train loss:0.07112286167491332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.04509600207849826\n",
      "train loss:0.07451404963777425\n",
      "train loss:0.04555770342461969\n",
      "train loss:0.045068740300977156\n",
      "train loss:0.038176211656900085\n",
      "train loss:0.029176721876802193\n",
      "train loss:0.07740427374080344\n",
      "train loss:0.017810776904421555\n",
      "train loss:0.06865662595271582\n",
      "train loss:0.01753711340772859\n",
      "train loss:0.16850657078655423\n",
      "train loss:0.1117408575620633\n",
      "train loss:0.09021414676972019\n",
      "train loss:0.10515834113326157\n",
      "train loss:0.11831790484034829\n",
      "train loss:0.053901951670369284\n",
      "train loss:0.0487998917828509\n",
      "train loss:0.04424561111960002\n",
      "train loss:0.030149151262361163\n",
      "train loss:0.03567060345651281\n",
      "train loss:0.11384124025406399\n",
      "train loss:0.07051172809049637\n",
      "train loss:0.06623976444323591\n",
      "train loss:0.07395675809029353\n",
      "train loss:0.09067817244103273\n",
      "train loss:0.05321937039580036\n",
      "train loss:0.04199599088607784\n",
      "train loss:0.04080927416270172\n",
      "train loss:0.05173721119262544\n",
      "train loss:0.06733260101075535\n",
      "train loss:0.052031711741352175\n",
      "train loss:0.040651076559142824\n",
      "train loss:0.026179482677750335\n",
      "train loss:0.09937733483661892\n",
      "train loss:0.06172465241003646\n",
      "train loss:0.04191488994300813\n",
      "train loss:0.05719476737132094\n",
      "train loss:0.038964850827736874\n",
      "train loss:0.0867400254326634\n",
      "train loss:0.11709991354074116\n",
      "train loss:0.14083200550219138\n",
      "train loss:0.028409570047031315\n",
      "train loss:0.02522606815842008\n",
      "train loss:0.03460872172062166\n",
      "train loss:0.11731154465244996\n",
      "train loss:0.02245214339072577\n",
      "train loss:0.033217670872469345\n",
      "train loss:0.08691383039815871\n",
      "train loss:0.09943716361195559\n",
      "train loss:0.050621071540179564\n",
      "train loss:0.1610847219037051\n",
      "train loss:0.01998538265734366\n",
      "train loss:0.04660341918354833\n",
      "train loss:0.20498120336875625\n",
      "train loss:0.08006869123752965\n",
      "train loss:0.03047897032336067\n",
      "train loss:0.06076612418667094\n",
      "train loss:0.19268752302596187\n",
      "train loss:0.10938239912855006\n",
      "train loss:0.044146531137240784\n",
      "train loss:0.09658566240716336\n",
      "train loss:0.0768609524073718\n",
      "train loss:0.06353160214516605\n",
      "train loss:0.11123111127556412\n",
      "train loss:0.16420964629208418\n",
      "train loss:0.036025359115978095\n",
      "train loss:0.13337323245055516\n",
      "train loss:0.0844294123460705\n",
      "train loss:0.046525113044687066\n",
      "train loss:0.054177209135932186\n",
      "train loss:0.037446059209609\n",
      "train loss:0.054802106719006156\n",
      "train loss:0.04391340376615799\n",
      "train loss:0.10004808318763678\n",
      "train loss:0.07894381751034135\n",
      "train loss:0.06002693949488269\n",
      "train loss:0.07082623937748872\n",
      "train loss:0.11155624765951995\n",
      "train loss:0.06803663295450851\n",
      "train loss:0.01797234223265516\n",
      "train loss:0.10588645481254756\n",
      "train loss:0.11143047723744587\n",
      "train loss:0.04978779053749298\n",
      "train loss:0.028749484791454817\n",
      "train loss:0.09770354980561874\n",
      "train loss:0.049723260498377914\n",
      "train loss:0.08895080401577792\n",
      "train loss:0.024175393153669372\n",
      "train loss:0.07419017435571203\n",
      "train loss:0.04242675707411782\n",
      "train loss:0.049347631371355394\n",
      "train loss:0.06049778460083668\n",
      "train loss:0.1285751403841416\n",
      "train loss:0.10029215815036242\n",
      "train loss:0.10959205144294452\n",
      "train loss:0.09328033199456279\n",
      "train loss:0.04259894739869369\n",
      "train loss:0.05799195884755072\n",
      "train loss:0.10386189418137218\n",
      "train loss:0.06070981764614605\n",
      "train loss:0.043687767822032646\n",
      "train loss:0.05649015835070023\n",
      "train loss:0.04071835213029385\n",
      "train loss:0.0393178199356112\n",
      "train loss:0.0323275188430052\n",
      "train loss:0.03056022450237248\n",
      "=== epoch:3, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.04677147981849808\n",
      "train loss:0.03868133810364212\n",
      "train loss:0.044322547776856534\n",
      "train loss:0.0743688842928205\n",
      "train loss:0.04668691787444779\n",
      "train loss:0.07544933482326054\n",
      "train loss:0.08678312579217963\n",
      "train loss:0.07424541998894371\n",
      "train loss:0.033148072702827894\n",
      "train loss:0.03305872029042478\n",
      "train loss:0.04843558378121125\n",
      "train loss:0.06627816614346438\n",
      "train loss:0.09946722430911166\n",
      "train loss:0.029570403472687377\n",
      "train loss:0.08871533710317119\n",
      "train loss:0.04871347193909327\n",
      "train loss:0.05367288830755662\n",
      "train loss:0.04998645877047883\n",
      "train loss:0.03685412698654401\n",
      "train loss:0.05117324327507894\n",
      "train loss:0.02209866325215091\n",
      "train loss:0.09208654500243757\n",
      "train loss:0.08001600197468431\n",
      "train loss:0.05884269237877504\n",
      "train loss:0.02854989973707921\n",
      "train loss:0.07585352145817122\n",
      "train loss:0.0534103650122023\n",
      "train loss:0.05554451350659686\n",
      "train loss:0.08294608401276941\n",
      "train loss:0.048946772588130516\n",
      "train loss:0.07480922772899612\n",
      "train loss:0.0809090343978929\n",
      "train loss:0.032891625179896467\n",
      "train loss:0.21775134775836655\n",
      "train loss:0.03656715384651226\n",
      "train loss:0.051395407101051935\n",
      "train loss:0.03562973819138162\n",
      "train loss:0.07482242800395815\n",
      "train loss:0.02931407265780333\n",
      "train loss:0.04604096800214399\n",
      "train loss:0.12198307224660071\n",
      "train loss:0.05734035753968791\n",
      "train loss:0.03340377911109818\n",
      "train loss:0.08052062918752108\n",
      "train loss:0.051115108436395634\n",
      "train loss:0.026988334172374683\n",
      "train loss:0.055041057493551315\n",
      "train loss:0.08595325071832864\n",
      "train loss:0.06116428960531705\n",
      "train loss:0.042106282203559345\n",
      "train loss:0.07659958927782475\n",
      "train loss:0.06284160972302834\n",
      "train loss:0.05380597166138272\n",
      "train loss:0.036664938342934536\n",
      "train loss:0.07324903448584022\n",
      "train loss:0.07606917338015885\n",
      "train loss:0.03817546805088945\n",
      "train loss:0.024679485576424947\n",
      "train loss:0.05130881341882763\n",
      "train loss:0.04959078156609078\n",
      "train loss:0.0865691640087407\n",
      "train loss:0.05277073804890975\n",
      "train loss:0.07059853561645754\n",
      "train loss:0.1932487043336001\n",
      "train loss:0.06702820982512821\n",
      "train loss:0.09553247022737893\n",
      "train loss:0.02450330441881975\n",
      "train loss:0.059645355110453185\n",
      "train loss:0.046743548403081626\n",
      "train loss:0.07854775782107419\n",
      "train loss:0.05587503365480818\n",
      "train loss:0.08874634890613521\n",
      "train loss:0.06295255814147274\n",
      "train loss:0.05614713314202366\n",
      "train loss:0.0779358817311435\n",
      "train loss:0.07858280474294299\n",
      "train loss:0.04600264713438142\n",
      "train loss:0.03931120125938377\n",
      "train loss:0.048968698028441754\n",
      "train loss:0.024848063510166555\n",
      "train loss:0.04994885340990758\n",
      "train loss:0.07297061798203945\n",
      "train loss:0.07814093490618182\n",
      "train loss:0.10037426565026525\n",
      "train loss:0.0858456546902686\n",
      "train loss:0.08388466505998003\n",
      "train loss:0.02717002336007899\n",
      "train loss:0.046774230535979094\n",
      "train loss:0.023318080224197534\n",
      "train loss:0.055115009827279494\n",
      "train loss:0.07834911597219336\n",
      "train loss:0.08933126806859733\n",
      "train loss:0.07392392730415685\n",
      "train loss:0.02681347035639278\n",
      "train loss:0.04549953217888439\n",
      "train loss:0.04944258062997139\n",
      "train loss:0.053038146288426465\n",
      "train loss:0.14831176720353742\n",
      "train loss:0.032136238865998895\n",
      "train loss:0.05144339627937272\n",
      "train loss:0.06033834830798777\n",
      "train loss:0.10948452215607066\n",
      "train loss:0.10021466736154835\n",
      "train loss:0.05554668685668955\n",
      "train loss:0.06117069956469648\n",
      "train loss:0.026461048667488916\n",
      "train loss:0.05731830486914\n",
      "train loss:0.037467439184115665\n",
      "train loss:0.028678186581816503\n",
      "train loss:0.03384113672753168\n",
      "train loss:0.02723709443797102\n",
      "train loss:0.13281176320579594\n",
      "train loss:0.051684941609055415\n",
      "train loss:0.04173971576505739\n",
      "train loss:0.10463939052080509\n",
      "train loss:0.07852298440266405\n",
      "train loss:0.044079389121140206\n",
      "train loss:0.10760613307725946\n",
      "train loss:0.052006996934203444\n",
      "train loss:0.0447760901680649\n",
      "train loss:0.0666205586471978\n",
      "train loss:0.07579553064194443\n",
      "train loss:0.1025575601598006\n",
      "train loss:0.049853333335319994\n",
      "train loss:0.13503200233797905\n",
      "train loss:0.02389535561223274\n",
      "train loss:0.04233675173186357\n",
      "train loss:0.10548587998711843\n",
      "train loss:0.059016194977475206\n",
      "train loss:0.048856847545815534\n",
      "train loss:0.041639211507127856\n",
      "train loss:0.041442220542188525\n",
      "train loss:0.0448381816269978\n",
      "train loss:0.03080294567328548\n",
      "train loss:0.040484254929534905\n",
      "train loss:0.05727331507574276\n",
      "train loss:0.04859591970059109\n",
      "train loss:0.03131492751429241\n",
      "train loss:0.028806447218213856\n",
      "train loss:0.1466746448174561\n",
      "train loss:0.03866948271474377\n",
      "train loss:0.028448002268188725\n",
      "train loss:0.03574140464758528\n",
      "train loss:0.035315051947611004\n",
      "train loss:0.031042384609667465\n",
      "train loss:0.14341681106583481\n",
      "train loss:0.037563683727189996\n",
      "train loss:0.08620854382618953\n",
      "train loss:0.15924212725483616\n",
      "train loss:0.02329959374897682\n",
      "train loss:0.05022624871846898\n",
      "train loss:0.08088397594205542\n",
      "train loss:0.0610914979471034\n",
      "train loss:0.07781374767800028\n",
      "train loss:0.12683367141626303\n",
      "train loss:0.13744922583564606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.06169451513816901\n",
      "train loss:0.108074784570223\n",
      "train loss:0.03387458639401363\n",
      "train loss:0.047756310099446295\n",
      "train loss:0.033896873394920514\n",
      "train loss:0.07249992285037055\n",
      "train loss:0.08171180319689832\n",
      "train loss:0.03407426859042691\n",
      "train loss:0.06952592174384413\n",
      "train loss:0.10651114849217277\n",
      "train loss:0.10906617604226783\n",
      "train loss:0.04325613818660318\n",
      "train loss:0.026859014276278063\n",
      "train loss:0.01947973394156135\n",
      "train loss:0.02090045172017553\n",
      "train loss:0.04244765799371202\n",
      "train loss:0.03044536086972748\n",
      "train loss:0.0704604790959614\n",
      "train loss:0.0361092897744314\n",
      "train loss:0.09676323912230757\n",
      "train loss:0.07474025816548778\n",
      "train loss:0.03759874007931656\n",
      "train loss:0.05100250199678721\n",
      "train loss:0.04661272475371168\n",
      "train loss:0.04561234145269875\n",
      "train loss:0.06019991363402711\n",
      "train loss:0.08763203445209836\n",
      "train loss:0.046118711291191807\n",
      "train loss:0.03798091303582465\n",
      "train loss:0.037915870962138665\n",
      "train loss:0.062067254274972915\n",
      "train loss:0.034982427623051156\n",
      "train loss:0.0357078368267007\n",
      "train loss:0.08496215721017197\n",
      "train loss:0.04126406070295348\n",
      "train loss:0.1720885766691125\n",
      "train loss:0.07806280636768986\n",
      "train loss:0.04864388613043552\n",
      "train loss:0.016534880616586704\n",
      "train loss:0.1331235368221887\n",
      "train loss:0.09103400005774477\n",
      "train loss:0.05732052123133704\n",
      "train loss:0.03672967635899674\n",
      "train loss:0.03953347376961413\n",
      "train loss:0.029837949572822476\n",
      "train loss:0.13810900985252525\n",
      "train loss:0.05117953530290824\n",
      "train loss:0.043837503902304176\n",
      "train loss:0.0350552282536733\n",
      "train loss:0.03549993833131207\n",
      "train loss:0.07603818415301943\n",
      "train loss:0.06033098598600224\n",
      "train loss:0.0650509371238962\n",
      "train loss:0.07993301134698326\n",
      "train loss:0.11052220613022438\n",
      "train loss:0.06378980218186539\n",
      "train loss:0.031454078965183735\n",
      "train loss:0.04084591592652445\n",
      "train loss:0.023946325761787936\n",
      "train loss:0.02208886704454389\n",
      "train loss:0.02495974993989355\n",
      "train loss:0.08215320898731658\n",
      "train loss:0.1004528254935882\n",
      "train loss:0.08710665236618327\n",
      "train loss:0.05080547262582567\n",
      "train loss:0.058987092523042364\n",
      "train loss:0.04240019579804673\n",
      "train loss:0.08429763082630198\n",
      "train loss:0.10231388005080039\n",
      "train loss:0.12056040345913761\n",
      "train loss:0.0364292646412236\n",
      "train loss:0.037393498641189134\n",
      "train loss:0.09195508865898218\n",
      "train loss:0.0834925890301753\n",
      "train loss:0.06329385547390208\n",
      "train loss:0.0764540613110728\n",
      "train loss:0.06437042511372654\n",
      "train loss:0.05302788509315259\n",
      "train loss:0.043584451646678506\n",
      "train loss:0.057074114940327936\n",
      "train loss:0.05624178809534558\n",
      "train loss:0.03208849545709175\n",
      "train loss:0.015853872647470953\n",
      "train loss:0.03706114712453887\n",
      "train loss:0.07085505465386323\n",
      "train loss:0.030740942176513526\n",
      "train loss:0.09799721609341289\n",
      "train loss:0.05236838064523479\n",
      "train loss:0.08589189638498058\n",
      "train loss:0.044355823859034124\n",
      "train loss:0.029813037133775363\n",
      "train loss:0.08772929344073269\n",
      "train loss:0.09971644119681847\n",
      "train loss:0.059273447204482334\n",
      "train loss:0.08487999782197725\n",
      "train loss:0.08798969098995418\n",
      "train loss:0.08033772647456824\n",
      "train loss:0.053216523130849624\n",
      "train loss:0.04150897510586155\n",
      "train loss:0.053499526968283126\n",
      "train loss:0.023297761013694594\n",
      "train loss:0.04490785110387897\n",
      "train loss:0.04968768584022949\n",
      "train loss:0.08450709356943252\n",
      "train loss:0.09361368997315717\n",
      "train loss:0.04940874898660387\n",
      "train loss:0.051146274417109956\n",
      "train loss:0.07475313060214259\n",
      "train loss:0.020770547353484572\n",
      "train loss:0.09010299385065573\n",
      "train loss:0.0701835776776289\n",
      "train loss:0.03288653623231288\n",
      "train loss:0.050747335122737436\n",
      "train loss:0.031473503065719076\n",
      "train loss:0.11633266812925168\n",
      "train loss:0.027719831869421875\n",
      "train loss:0.05322919938976988\n",
      "train loss:0.05221952824125958\n",
      "train loss:0.05751109762897629\n",
      "train loss:0.0803253143608719\n",
      "train loss:0.0855088964606903\n",
      "train loss:0.027782357106030043\n",
      "train loss:0.023872263473081478\n",
      "train loss:0.08657433148509835\n",
      "train loss:0.0387389896405038\n",
      "train loss:0.06497765416789598\n",
      "train loss:0.03959923604212998\n",
      "train loss:0.0688699490126178\n",
      "train loss:0.061028230430782815\n",
      "train loss:0.09462010653317146\n",
      "train loss:0.04164659105237133\n",
      "train loss:0.05063245051654226\n",
      "train loss:0.0421545312808121\n",
      "train loss:0.04606345982312623\n",
      "train loss:0.045223540612046094\n",
      "train loss:0.03621403194540383\n",
      "train loss:0.054879427109385634\n",
      "train loss:0.08605424707349105\n",
      "train loss:0.05372441347343178\n",
      "train loss:0.09005189419139943\n",
      "train loss:0.06532157910965304\n",
      "train loss:0.0213287027165205\n",
      "train loss:0.06177361255642058\n",
      "train loss:0.06689357415971375\n",
      "train loss:0.09485724808779078\n",
      "train loss:0.05150592191486931\n",
      "train loss:0.06991720556214231\n",
      "train loss:0.02726058219371424\n",
      "train loss:0.028694512150391267\n",
      "train loss:0.054058787421765224\n",
      "train loss:0.030289558195099466\n",
      "train loss:0.014183265261879631\n",
      "train loss:0.025620579388424145\n",
      "train loss:0.10458723835755715\n",
      "train loss:0.04549573365914755\n",
      "train loss:0.11012935958139497\n",
      "train loss:0.08509604260374556\n",
      "train loss:0.049894055387330945\n",
      "train loss:0.043041526377834316\n",
      "train loss:0.05228036318162074\n",
      "train loss:0.03134487746772225\n",
      "train loss:0.021160700021254208\n",
      "train loss:0.08592249281805828\n",
      "train loss:0.024359140957673903\n",
      "train loss:0.1028976489652479\n",
      "train loss:0.0805266956413932\n",
      "train loss:0.04569222464705373\n",
      "train loss:0.016355486965838432\n",
      "train loss:0.03056923625246349\n",
      "train loss:0.022785532534097413\n",
      "train loss:0.07482821403283312\n",
      "train loss:0.043713498492214724\n",
      "train loss:0.060253799634403436\n",
      "train loss:0.11969816308806179\n",
      "train loss:0.07576278825220606\n",
      "train loss:0.07843754573263767\n",
      "train loss:0.04131456461446503\n",
      "train loss:0.06370861481199074\n",
      "train loss:0.01934369322373676\n",
      "train loss:0.03304059937777291\n",
      "train loss:0.06940360631770798\n",
      "train loss:0.04385760349798455\n",
      "train loss:0.025353631363279613\n",
      "train loss:0.030302651498127102\n",
      "train loss:0.03397994908681925\n",
      "train loss:0.025931433081564304\n",
      "train loss:0.0372391560488977\n",
      "train loss:0.03409971754354726\n",
      "train loss:0.039921364166772985\n",
      "train loss:0.046918275684261254\n",
      "train loss:0.10930943489047497\n",
      "train loss:0.027871605115965696\n",
      "train loss:0.02682466085302508\n",
      "train loss:0.04732061360714382\n",
      "train loss:0.011621136133938456\n",
      "train loss:0.023858608121351578\n",
      "train loss:0.03651267600015221\n",
      "train loss:0.10407739533279589\n",
      "train loss:0.02200973847188187\n",
      "train loss:0.048740101306530635\n",
      "train loss:0.04150928014877214\n",
      "train loss:0.10309077569425867\n",
      "train loss:0.0419463744095435\n",
      "train loss:0.016311762762842925\n",
      "train loss:0.12145185567011191\n",
      "train loss:0.03822900661005093\n",
      "train loss:0.07241380659986053\n",
      "train loss:0.01638714794938807\n",
      "train loss:0.028085322222316588\n",
      "train loss:0.04677091336237799\n",
      "train loss:0.05690436786238677\n",
      "train loss:0.04749614349588033\n",
      "train loss:0.07825823304643362\n",
      "train loss:0.150553365123013\n",
      "train loss:0.05529146281079003\n",
      "train loss:0.048266174978244585\n",
      "train loss:0.07345958924178463\n",
      "train loss:0.06032289890753419\n",
      "train loss:0.05902956971984089\n",
      "train loss:0.041681079760624115\n",
      "train loss:0.06603727860737876\n",
      "train loss:0.03629891270513719\n",
      "train loss:0.03994224922762325\n",
      "train loss:0.026562206766266968\n",
      "train loss:0.02476837563492389\n",
      "train loss:0.01830447371511376\n",
      "train loss:0.07652707637279532\n",
      "train loss:0.03878977143016827\n",
      "train loss:0.026773454127394278\n",
      "train loss:0.07596546072490312\n",
      "train loss:0.03669964872184493\n",
      "train loss:0.09017175584275636\n",
      "train loss:0.011217810973895204\n",
      "train loss:0.03877812672994697\n",
      "train loss:0.12388023086180615\n",
      "train loss:0.040327399317949854\n",
      "train loss:0.02609011578078752\n",
      "train loss:0.07398711451367106\n",
      "train loss:0.050660878960489965\n",
      "train loss:0.06463628602140722\n",
      "train loss:0.042377570928153814\n",
      "train loss:0.06443767203608529\n",
      "train loss:0.026199497941419327\n",
      "train loss:0.07012465184707777\n",
      "train loss:0.028224179339282424\n",
      "train loss:0.037341601211975474\n",
      "train loss:0.06388504603737644\n",
      "train loss:0.08554206841216797\n",
      "train loss:0.050763945535091407\n",
      "train loss:0.026934889343291125\n",
      "train loss:0.017236901156376307\n",
      "train loss:0.07954990925427305\n",
      "train loss:0.03644368370786189\n",
      "train loss:0.06384853678812055\n",
      "train loss:0.059096442450354135\n",
      "train loss:0.06593837604416442\n",
      "train loss:0.025605003254787585\n",
      "train loss:0.05275632990810347\n",
      "train loss:0.09785970847775857\n",
      "train loss:0.020503321568136917\n",
      "train loss:0.045626449550134965\n",
      "train loss:0.18025263863936833\n",
      "train loss:0.022893241817735076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03962636225305846\n",
      "train loss:0.018294617631787097\n",
      "train loss:0.04120702329386765\n",
      "train loss:0.06164438508781012\n",
      "train loss:0.05023891754207232\n",
      "train loss:0.15950085785694024\n",
      "train loss:0.11130599690298572\n",
      "train loss:0.04475941243357315\n",
      "train loss:0.025257010328474324\n",
      "train loss:0.051044678314410064\n",
      "train loss:0.05034666864805048\n",
      "train loss:0.023719231372909676\n",
      "train loss:0.12100292854154901\n",
      "train loss:0.03645021377621024\n",
      "train loss:0.0830198500947488\n",
      "train loss:0.13252887322906176\n",
      "train loss:0.06114536888981009\n",
      "train loss:0.09777955623996774\n",
      "train loss:0.029558427096585533\n",
      "train loss:0.03928049884725275\n",
      "train loss:0.04946563321520357\n",
      "train loss:0.04743836824469007\n",
      "train loss:0.04014751821246068\n",
      "train loss:0.030445574440351543\n",
      "train loss:0.08348619219657497\n",
      "train loss:0.08658403530473573\n",
      "train loss:0.07832467140717164\n",
      "train loss:0.07575820249531072\n",
      "train loss:0.02543858852387939\n",
      "train loss:0.04119731544908227\n",
      "train loss:0.06423674035283265\n",
      "=== epoch:4, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.013884101092241388\n",
      "train loss:0.0335260949374574\n",
      "train loss:0.022534596964269786\n",
      "train loss:0.053594604055816494\n",
      "train loss:0.05670975283152231\n",
      "train loss:0.058542613212043494\n",
      "train loss:0.04574887993489964\n",
      "train loss:0.02719243419970553\n",
      "train loss:0.05226272023177835\n",
      "train loss:0.06906574595808818\n",
      "train loss:0.052663303227210315\n",
      "train loss:0.25602606664381816\n",
      "train loss:0.03703899307596664\n",
      "train loss:0.032655243665919646\n",
      "train loss:0.06994641730393956\n",
      "train loss:0.10298990191307329\n",
      "train loss:0.06304685987385433\n",
      "train loss:0.021633042418180693\n",
      "train loss:0.027237322388149278\n",
      "train loss:0.04058100538340558\n",
      "train loss:0.05271636921920181\n",
      "train loss:0.021461829508669136\n",
      "train loss:0.07240523141511294\n",
      "train loss:0.038836880472662184\n",
      "train loss:0.03914372853046722\n",
      "train loss:0.014912489201330696\n",
      "train loss:0.03398576321634231\n",
      "train loss:0.032458970297459776\n",
      "train loss:0.02852047554948701\n",
      "train loss:0.06248315276809345\n",
      "train loss:0.04964893651219865\n",
      "train loss:0.04156871738852032\n",
      "train loss:0.10989437584218528\n",
      "train loss:0.017851840270280223\n",
      "train loss:0.03871138473069225\n",
      "train loss:0.0726873929791404\n",
      "train loss:0.09965459701884674\n",
      "train loss:0.029426340995968917\n",
      "train loss:0.052754064993800934\n",
      "train loss:0.025230895381745822\n",
      "train loss:0.02497858258206989\n",
      "train loss:0.03770477796802821\n",
      "train loss:0.01607614235049551\n",
      "train loss:0.08805187713332954\n",
      "train loss:0.03091220643403376\n",
      "train loss:0.1291900405746519\n",
      "train loss:0.017019061170650544\n",
      "train loss:0.13434857342709872\n",
      "train loss:0.06988511883027798\n",
      "train loss:0.014383544490900745\n",
      "train loss:0.07404964647354575\n",
      "train loss:0.024170011847769922\n",
      "train loss:0.027317381116041147\n",
      "train loss:0.028561955369420716\n",
      "train loss:0.028309619945803055\n",
      "train loss:0.025745137177916693\n",
      "train loss:0.0657031917425984\n",
      "train loss:0.03636894470269017\n",
      "train loss:0.05766661328085574\n",
      "train loss:0.023853504469389657\n",
      "train loss:0.03561372954237282\n",
      "train loss:0.07738934225643007\n",
      "train loss:0.03426149244181875\n",
      "train loss:0.03730059515481777\n",
      "train loss:0.04571951843494934\n",
      "train loss:0.06690553361659689\n",
      "train loss:0.021685984733675466\n",
      "train loss:0.06885425493551897\n",
      "train loss:0.03837690507617019\n",
      "train loss:0.03396460463334247\n",
      "train loss:0.04540105687176275\n",
      "train loss:0.029634344761147235\n",
      "train loss:0.043905853813541285\n",
      "train loss:0.02337740224269118\n",
      "train loss:0.022097899648780027\n",
      "train loss:0.04882758105244104\n",
      "train loss:0.07104966431853289\n",
      "train loss:0.0842520116560845\n",
      "train loss:0.016921575627494558\n",
      "train loss:0.06864310884494147\n",
      "train loss:0.03569014229988198\n",
      "train loss:0.08433567004916429\n",
      "train loss:0.0314475345874061\n",
      "train loss:0.09757197041758053\n",
      "train loss:0.04740657660886297\n",
      "train loss:0.054080454834424564\n",
      "train loss:0.054464971659328724\n",
      "train loss:0.06920097750911014\n",
      "train loss:0.02959993144314242\n",
      "train loss:0.04207963594905949\n",
      "train loss:0.05483050025006823\n",
      "train loss:0.014109202348343355\n",
      "train loss:0.07705295923554265\n",
      "train loss:0.09584217578425067\n",
      "train loss:0.025259212454406436\n",
      "train loss:0.11582136785805047\n",
      "train loss:0.011858970979557302\n",
      "train loss:0.05947673712669287\n",
      "train loss:0.04174009426996417\n",
      "train loss:0.09966528968566875\n",
      "train loss:0.04736703979966162\n",
      "train loss:0.02095054632551541\n",
      "train loss:0.030072035338283717\n",
      "train loss:0.10629122558654774\n",
      "train loss:0.0842889230659061\n",
      "train loss:0.041003199411954266\n",
      "train loss:0.22346405684109846\n",
      "train loss:0.04746273454936298\n",
      "train loss:0.09383316974097956\n",
      "train loss:0.04409430315464289\n",
      "train loss:0.08785019446463696\n",
      "train loss:0.022576130936679793\n",
      "train loss:0.0306702666987998\n",
      "train loss:0.1002031740635064\n",
      "train loss:0.03597216382721271\n",
      "train loss:0.03698143875151573\n",
      "train loss:0.03325536601982836\n",
      "train loss:0.04208109136117967\n",
      "train loss:0.03377453920191345\n",
      "train loss:0.0614707058283113\n",
      "train loss:0.1264945186994021\n",
      "train loss:0.05508836116564779\n",
      "train loss:0.044787917338955936\n",
      "train loss:0.05568209523901725\n",
      "train loss:0.05688683073301336\n",
      "train loss:0.03577765858641533\n",
      "train loss:0.06521207337783964\n",
      "train loss:0.055159916562039744\n",
      "train loss:0.019254145830713644\n",
      "train loss:0.025661536740064612\n",
      "train loss:0.027179265235417856\n",
      "train loss:0.04785461818964167\n",
      "train loss:0.04472360994200196\n",
      "train loss:0.019176800564212445\n",
      "train loss:0.03196968382115832\n",
      "train loss:0.024862930076453037\n",
      "train loss:0.03307104451954727\n",
      "train loss:0.07919887941581825\n",
      "train loss:0.032365665197659065\n",
      "train loss:0.037106109566942685\n",
      "train loss:0.03214769424363953\n",
      "train loss:0.014499447319610136\n",
      "train loss:0.015537077730935614\n",
      "train loss:0.10695234356714382\n",
      "train loss:0.04794683345029012\n",
      "train loss:0.06621727450330116\n",
      "train loss:0.05950312333194559\n",
      "train loss:0.02702624047361673\n",
      "train loss:0.05770170261153968\n",
      "train loss:0.0388584114923826\n",
      "train loss:0.015839085616125726\n",
      "train loss:0.08481966227909331\n",
      "train loss:0.02121378043313235\n",
      "train loss:0.05866558330556923\n",
      "train loss:0.028586095612665324\n",
      "train loss:0.024831131764010795\n",
      "train loss:0.03045165380146975\n",
      "train loss:0.01522599587155807\n",
      "train loss:0.07550377696761812\n",
      "train loss:0.04904397278269132\n",
      "train loss:0.041826683770058486\n",
      "train loss:0.046379163397162804\n",
      "train loss:0.04821076466801633\n",
      "train loss:0.1692569033311657\n",
      "train loss:0.07684139490800714\n",
      "train loss:0.02021645066589561\n",
      "train loss:0.04931495216577149\n",
      "train loss:0.08793186879332174\n",
      "train loss:0.04836930769827724\n",
      "train loss:0.05968084403649947\n",
      "train loss:0.02559865504355725\n",
      "train loss:0.10453345617649155\n",
      "train loss:0.02648746422406626\n",
      "train loss:0.1232806103415917\n",
      "train loss:0.06660841203126584\n",
      "train loss:0.09617671611597903\n",
      "train loss:0.03028446059761504\n",
      "train loss:0.04252198406004805\n",
      "train loss:0.06720162979750484\n",
      "train loss:0.059081513312176585\n",
      "train loss:0.05133046676379946\n",
      "train loss:0.03537739855056406\n",
      "train loss:0.049363063047613676\n",
      "train loss:0.03526458689549691\n",
      "train loss:0.016819264618092255\n",
      "train loss:0.023401900868340444\n",
      "train loss:0.04012041443818618\n",
      "train loss:0.041740288996167836\n",
      "train loss:0.07984165171866825\n",
      "train loss:0.0559781771943941\n",
      "train loss:0.014167656375698074\n",
      "train loss:0.04701950063903714\n",
      "train loss:0.04852699470783499\n",
      "train loss:0.04629450411288314\n",
      "train loss:0.060754629959882324\n",
      "train loss:0.04320453014251518\n",
      "train loss:0.021677242034175263\n",
      "train loss:0.07648143193091472\n",
      "train loss:0.03753760522417616\n",
      "train loss:0.022163116638366204\n",
      "train loss:0.023439482052813317\n",
      "train loss:0.035706958137441076\n",
      "train loss:0.04125945887646809\n",
      "train loss:0.01954208714534329\n",
      "train loss:0.03549914739560507\n",
      "train loss:0.02247796868219233\n",
      "train loss:0.020400027293213375\n",
      "train loss:0.10108239882329162\n",
      "train loss:0.09101832198948179\n",
      "train loss:0.041443676890768257\n",
      "train loss:0.04494814141567262\n",
      "train loss:0.03714288882886879\n",
      "train loss:0.07299846251665115\n",
      "train loss:0.01695964702237832\n",
      "train loss:0.016722442686168217\n",
      "train loss:0.019921085564661753\n",
      "train loss:0.044869540286399445\n",
      "train loss:0.06414028388137612\n",
      "train loss:0.022774572188756437\n",
      "train loss:0.027250169560067115\n",
      "train loss:0.11710040637613357\n",
      "train loss:0.08432972059403442\n",
      "train loss:0.04775353948101716\n",
      "train loss:0.07168119517738283\n",
      "train loss:0.07921071771353558\n",
      "train loss:0.09400604852239898\n",
      "train loss:0.030642521524568363\n",
      "train loss:0.027459220906230482\n",
      "train loss:0.023903220836201236\n",
      "train loss:0.03425883717315382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.014764747990445777\n",
      "train loss:0.028568697134899575\n",
      "train loss:0.02170671201997869\n",
      "train loss:0.06134523391687175\n",
      "train loss:0.05711518611181894\n",
      "train loss:0.011619832852406553\n",
      "train loss:0.03469408331265683\n",
      "train loss:0.02501201430043808\n",
      "train loss:0.03941786303643712\n",
      "train loss:0.02164834136293865\n",
      "train loss:0.06767567651041487\n",
      "train loss:0.022121283577236972\n",
      "train loss:0.09307218068233941\n",
      "train loss:0.04404240802997863\n",
      "train loss:0.2386776378135308\n",
      "train loss:0.028910938768369875\n",
      "train loss:0.04161650853097137\n",
      "train loss:0.022520307674414704\n",
      "train loss:0.06371519663309666\n",
      "train loss:0.07843968125621101\n",
      "train loss:0.10716498269499335\n",
      "train loss:0.03571330122988142\n",
      "train loss:0.05799176329659136\n",
      "train loss:0.018353138076783748\n",
      "train loss:0.03662363534904433\n",
      "train loss:0.03665838371210822\n",
      "train loss:0.11506699677998097\n",
      "train loss:0.1354061664607782\n",
      "train loss:0.035210299350986046\n",
      "train loss:0.02245060573717903\n",
      "train loss:0.07516437926881257\n",
      "train loss:0.048024785982103774\n",
      "train loss:0.06322082372767336\n",
      "train loss:0.040727651855091695\n",
      "train loss:0.05946616403783521\n",
      "train loss:0.08400209409934703\n",
      "train loss:0.03768834533112374\n",
      "train loss:0.08091180176844492\n",
      "train loss:0.06439287839957862\n",
      "train loss:0.04780134814686566\n",
      "train loss:0.06014420197544039\n",
      "train loss:0.016520378233073726\n",
      "train loss:0.05332901608822233\n",
      "train loss:0.031314795568990396\n",
      "train loss:0.03074589578761872\n",
      "train loss:0.03546243411602836\n",
      "train loss:0.04852207461098049\n",
      "train loss:0.0955150102474539\n",
      "train loss:0.034277379260715335\n",
      "train loss:0.03187848045771311\n",
      "train loss:0.049871889588350246\n",
      "train loss:0.05366502560501097\n",
      "train loss:0.032399520161364125\n",
      "train loss:0.01592929654477972\n",
      "train loss:0.08792741118995409\n",
      "train loss:0.07627068622816863\n",
      "train loss:0.06695271816177048\n",
      "train loss:0.04463361689641178\n",
      "train loss:0.19516505661064115\n",
      "train loss:0.0376831792535309\n",
      "train loss:0.02142325905330416\n",
      "train loss:0.08869699886838774\n",
      "train loss:0.04203900046356408\n",
      "train loss:0.061605469617877266\n",
      "train loss:0.0493598528073092\n",
      "train loss:0.04952067814326602\n",
      "train loss:0.01937245979769176\n",
      "train loss:0.020724978016077648\n",
      "train loss:0.052286088396662496\n",
      "train loss:0.04559002940967624\n",
      "train loss:0.017273332912559693\n",
      "train loss:0.040217034816157335\n",
      "train loss:0.028551801256232686\n",
      "train loss:0.05537215859930686\n",
      "train loss:0.03958269744616107\n",
      "train loss:0.03831933184976565\n",
      "train loss:0.020246090475912814\n",
      "train loss:0.031203461053463527\n",
      "train loss:0.04732985354413517\n",
      "train loss:0.029888326795249248\n",
      "train loss:0.026311257791471702\n",
      "train loss:0.033982738133933196\n",
      "train loss:0.044787264878187925\n",
      "train loss:0.05798137464767596\n",
      "train loss:0.05339371500307048\n",
      "train loss:0.019646919909729568\n",
      "train loss:0.026006548507235393\n",
      "train loss:0.03292412957944918\n",
      "train loss:0.0474535429167691\n",
      "train loss:0.030005712930962734\n",
      "train loss:0.11075995692190574\n",
      "train loss:0.02306304704922963\n",
      "train loss:0.023967584929303273\n",
      "train loss:0.06352930782676285\n",
      "train loss:0.05921899605302742\n",
      "train loss:0.046263037886387685\n",
      "train loss:0.037255653872933496\n",
      "train loss:0.07095623819032025\n",
      "train loss:0.08396501268885807\n",
      "train loss:0.07022156707877678\n",
      "train loss:0.03574448909633125\n",
      "train loss:0.026209171155241583\n",
      "train loss:0.08483776025738118\n",
      "train loss:0.04746657923612192\n",
      "train loss:0.08723871694500097\n",
      "train loss:0.05004409920438148\n",
      "train loss:0.02497376208286179\n",
      "train loss:0.04501295694182573\n",
      "train loss:0.02125369152977541\n",
      "train loss:0.05101700445532188\n",
      "train loss:0.049288962687776355\n",
      "train loss:0.0358598382727696\n",
      "train loss:0.03490680815214527\n",
      "train loss:0.0462085791528603\n",
      "train loss:0.11805409975179076\n",
      "train loss:0.03485978668962733\n",
      "train loss:0.05021966901821097\n",
      "train loss:0.14150182511131193\n",
      "train loss:0.06688146850339588\n",
      "train loss:0.04345756425454354\n",
      "train loss:0.05723942667947965\n",
      "train loss:0.02818213063479918\n",
      "train loss:0.041107987959415525\n",
      "train loss:0.024190369642846647\n",
      "train loss:0.04501154157496589\n",
      "train loss:0.04876738658581737\n",
      "train loss:0.11388684953929319\n",
      "train loss:0.05162708859593326\n",
      "train loss:0.04835945289106143\n",
      "train loss:0.02715421510709981\n",
      "train loss:0.05708075496945238\n",
      "train loss:0.03529034953658156\n",
      "train loss:0.026566423893603957\n",
      "train loss:0.042762017530361414\n",
      "train loss:0.019922680140891314\n",
      "train loss:0.03862689578438814\n",
      "train loss:0.0427162829316758\n",
      "train loss:0.058401479869875594\n",
      "train loss:0.03684946561714412\n",
      "train loss:0.009689011502452612\n",
      "train loss:0.05694186044496351\n",
      "train loss:0.029078711594120805\n",
      "train loss:0.2383457147651969\n",
      "train loss:0.0456661919425957\n",
      "train loss:0.068608965179599\n",
      "train loss:0.020358987083488295\n",
      "train loss:0.0876460050623455\n",
      "train loss:0.03857468431981923\n",
      "train loss:0.027207982688732195\n",
      "train loss:0.05229330394183166\n",
      "train loss:0.03457029213148917\n",
      "train loss:0.01309739338686144\n",
      "train loss:0.05131068855958398\n",
      "train loss:0.07735607333797292\n",
      "train loss:0.042186796119944824\n",
      "train loss:0.03938289245013683\n",
      "train loss:0.041224517485807424\n",
      "train loss:0.022466356642655193\n",
      "train loss:0.031108220398004875\n",
      "train loss:0.11005075658442479\n",
      "train loss:0.09920053913579581\n",
      "train loss:0.02703774282377547\n",
      "train loss:0.033124140977958044\n",
      "train loss:0.047487602014506705\n",
      "train loss:0.07224274375857079\n",
      "train loss:0.03256245526812404\n",
      "train loss:0.02825404540507319\n",
      "train loss:0.029792013268220025\n",
      "train loss:0.019105883101614728\n",
      "train loss:0.036222376428867935\n",
      "train loss:0.03328363343788456\n",
      "train loss:0.06305008397818114\n",
      "train loss:0.021306564727848566\n",
      "train loss:0.05010165085444176\n",
      "train loss:0.05337104233722992\n",
      "train loss:0.07058765669574206\n",
      "train loss:0.03867079562969691\n",
      "train loss:0.06028327000525055\n",
      "train loss:0.018788259062320276\n",
      "train loss:0.06436594862034632\n",
      "train loss:0.028435863933635744\n",
      "train loss:0.011755515404029667\n",
      "train loss:0.018836439342918704\n",
      "train loss:0.028151585286671582\n",
      "train loss:0.04840502944769164\n",
      "train loss:0.022088845189333798\n",
      "train loss:0.04443174201120776\n",
      "train loss:0.03804521030265799\n",
      "train loss:0.037488701177971456\n",
      "train loss:0.025459152889759036\n",
      "train loss:0.05231920749562928\n",
      "train loss:0.027128334105655454\n",
      "train loss:0.03345341107452162\n",
      "train loss:0.022484368332958703\n",
      "train loss:0.050480634540831286\n",
      "train loss:0.019460658059177627\n",
      "train loss:0.03644155339528834\n",
      "train loss:0.027927138799283605\n",
      "train loss:0.05370990884931461\n",
      "train loss:0.02262474158934163\n",
      "train loss:0.06465453677151725\n",
      "train loss:0.04634189176666019\n",
      "train loss:0.05390170286421388\n",
      "train loss:0.02475754606583133\n",
      "train loss:0.0215582882245655\n",
      "train loss:0.03854698847334716\n",
      "train loss:0.013644514688444617\n",
      "train loss:0.055379569098247164\n",
      "train loss:0.020509211501835187\n",
      "train loss:0.03607100495258125\n",
      "train loss:0.02700114756140097\n",
      "train loss:0.07327332712536164\n",
      "train loss:0.05149127302414604\n",
      "train loss:0.026170180606048032\n",
      "train loss:0.05793172783689943\n",
      "train loss:0.03723712204495481\n",
      "train loss:0.10319710757940818\n",
      "train loss:0.03270506403408103\n",
      "train loss:0.04232548127633053\n",
      "train loss:0.06790528029377427\n",
      "=== epoch:5, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.02689448036281673\n",
      "train loss:0.03925452616702372\n",
      "train loss:0.018999758999746985\n",
      "train loss:0.023230173557173807\n",
      "train loss:0.06335737841013561\n",
      "train loss:0.0543737437663011\n",
      "train loss:0.031743828877499845\n",
      "train loss:0.028610892392135146\n",
      "train loss:0.03027485624053761\n",
      "train loss:0.0681020013256857\n",
      "train loss:0.06430923588346445\n",
      "train loss:0.02736708740033175\n",
      "train loss:0.1001301601257557\n",
      "train loss:0.03520630613683122\n",
      "train loss:0.05065358372464759\n",
      "train loss:0.045385432364231225\n",
      "train loss:0.06749404072608774\n",
      "train loss:0.016888377309810913\n",
      "train loss:0.08952343187750868\n",
      "train loss:0.015443617307434612\n",
      "train loss:0.08888721558323134\n",
      "train loss:0.02965672049847046\n",
      "train loss:0.06071170747969186\n",
      "train loss:0.040646337326119705\n",
      "train loss:0.06532577564164783\n",
      "train loss:0.03944369107888754\n",
      "train loss:0.09071901797072553\n",
      "train loss:0.024054537157301086\n",
      "train loss:0.018447321166979773\n",
      "train loss:0.1323774619530881\n",
      "train loss:0.05807780317662604\n",
      "train loss:0.027461186709240745\n",
      "train loss:0.04158106940255404\n",
      "train loss:0.03934851100790235\n",
      "train loss:0.032415250943657804\n",
      "train loss:0.03007466949964877\n",
      "train loss:0.026329868373613095\n",
      "train loss:0.04674115080642983\n",
      "train loss:0.025864439433401373\n",
      "train loss:0.018044745108793753\n",
      "train loss:0.013250384279981145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.018678099161688708\n",
      "train loss:0.030519350999772366\n",
      "train loss:0.12705726868992007\n",
      "train loss:0.02589941879324241\n",
      "train loss:0.016966056337114864\n",
      "train loss:0.02377099891255782\n",
      "train loss:0.024740759134256482\n",
      "train loss:0.06793176576826318\n",
      "train loss:0.05371761133142262\n",
      "train loss:0.027374436980333074\n",
      "train loss:0.03953311643856906\n",
      "train loss:0.04417463359023069\n",
      "train loss:0.03079796710144005\n",
      "train loss:0.02927330984253447\n",
      "train loss:0.04568145722338713\n",
      "train loss:0.05377272368387155\n",
      "train loss:0.02507929820722723\n",
      "train loss:0.07804976245832608\n",
      "train loss:0.03229616853318441\n",
      "train loss:0.07024286158907977\n",
      "train loss:0.049088517175687005\n",
      "train loss:0.05858023658216051\n",
      "train loss:0.02709537330064491\n",
      "train loss:0.026640805448378803\n",
      "train loss:0.03475614882125342\n",
      "train loss:0.01739466654214016\n",
      "train loss:0.06627119665666326\n",
      "train loss:0.015483274556566267\n",
      "train loss:0.04013631592176934\n",
      "train loss:0.15781067815581504\n",
      "train loss:0.02498687971062836\n",
      "train loss:0.03942857480877111\n",
      "train loss:0.04876609471700364\n",
      "train loss:0.020902909524173365\n",
      "train loss:0.06654310668906184\n",
      "train loss:0.036481034658997574\n",
      "train loss:0.021460658269449037\n",
      "train loss:0.02933458892956295\n",
      "train loss:0.03264756698304718\n",
      "train loss:0.04892381103582453\n",
      "train loss:0.043753727122752294\n",
      "train loss:0.03445050632361159\n",
      "train loss:0.033726615688097865\n",
      "train loss:0.06784292921584864\n",
      "train loss:0.04890091010195026\n",
      "train loss:0.024180435296828305\n",
      "train loss:0.04854650182107643\n",
      "train loss:0.0795659714298547\n",
      "train loss:0.0416844422025388\n",
      "train loss:0.057662963576696924\n",
      "train loss:0.07554131918369726\n",
      "train loss:0.0798323041667048\n",
      "train loss:0.06308216135587325\n",
      "train loss:0.07443364202685938\n",
      "train loss:0.07274710338950509\n",
      "train loss:0.052113399619173766\n",
      "train loss:0.02915766569539166\n",
      "train loss:0.021232211057198448\n",
      "train loss:0.09057560379329367\n",
      "train loss:0.05212339409772754\n",
      "train loss:0.011265458102365204\n",
      "train loss:0.07607026365702739\n",
      "train loss:0.019467201763892144\n",
      "train loss:0.03978192419674544\n",
      "train loss:0.033008360902461904\n",
      "train loss:0.03793285236612819\n",
      "train loss:0.10636814343979065\n",
      "train loss:0.062064594220596625\n",
      "train loss:0.062010555967085795\n",
      "train loss:0.08364851738146967\n",
      "train loss:0.025725092637864995\n",
      "train loss:0.04549888534750342\n",
      "train loss:0.05388059513628484\n",
      "train loss:0.07577365614348104\n",
      "train loss:0.03914472769751739\n",
      "train loss:0.0221442257208506\n",
      "train loss:0.028851268037236425\n",
      "train loss:0.06164655210900818\n",
      "train loss:0.027287331171343056\n",
      "train loss:0.01834385687300985\n",
      "train loss:0.06387690375140076\n",
      "train loss:0.03342876876758947\n",
      "train loss:0.06444703507103922\n",
      "train loss:0.13835740861886076\n",
      "train loss:0.032518350650763646\n",
      "train loss:0.024967356949632397\n",
      "train loss:0.027427239091758072\n",
      "train loss:0.05443288494924738\n",
      "train loss:0.04498446313543341\n",
      "train loss:0.051420120023013584\n",
      "train loss:0.0772516651811377\n",
      "train loss:0.08313278197067776\n",
      "train loss:0.054010226306904524\n",
      "train loss:0.024673424909802354\n",
      "train loss:0.013696827887982679\n",
      "train loss:0.06873323810547069\n",
      "train loss:0.10515058228316984\n",
      "train loss:0.02866671682785665\n",
      "train loss:0.015217446798831934\n",
      "train loss:0.02049673269396393\n",
      "train loss:0.026757244615369637\n",
      "train loss:0.08428607421504573\n",
      "train loss:0.018049936077568572\n",
      "train loss:0.06952696729562324\n",
      "train loss:0.015988074274712133\n",
      "train loss:0.037882208486885824\n",
      "train loss:0.01393626398177553\n",
      "train loss:0.03408989677700265\n",
      "train loss:0.024621842584861883\n",
      "train loss:0.039324337904196864\n",
      "train loss:0.037592720920614935\n",
      "train loss:0.02292039941495886\n",
      "train loss:0.032179489385563156\n",
      "train loss:0.03596958695943595\n",
      "train loss:0.010201319710480024\n",
      "train loss:0.022762026217087214\n",
      "train loss:0.05713928221306047\n",
      "train loss:0.0812013605790012\n",
      "train loss:0.025981003962036345\n",
      "train loss:0.01609465101413605\n",
      "train loss:0.03791099426072693\n",
      "train loss:0.05797614507255224\n",
      "train loss:0.017790026022554785\n",
      "train loss:0.03563571901983603\n",
      "train loss:0.05431488462922225\n",
      "train loss:0.0235612691119327\n",
      "train loss:0.039322359578632914\n",
      "train loss:0.040160296145673936\n",
      "train loss:0.057067330494563494\n",
      "train loss:0.027432354240800493\n",
      "train loss:0.031837353954987\n",
      "train loss:0.03358984528003114\n",
      "train loss:0.050635784862269724\n",
      "train loss:0.026281637207732023\n",
      "train loss:0.05173681169178223\n",
      "train loss:0.026057318540813154\n",
      "train loss:0.02590765456941659\n",
      "train loss:0.044259242401219305\n",
      "train loss:0.045544352859929055\n",
      "train loss:0.04228205736937195\n",
      "train loss:0.056381869708230295\n",
      "train loss:0.013026414605855526\n",
      "train loss:0.023822678593831817\n",
      "train loss:0.017252573598998202\n",
      "train loss:0.026495778428943927\n",
      "train loss:0.04917794416419561\n",
      "train loss:0.044431230526095315\n",
      "train loss:0.03292721391765922\n",
      "train loss:0.042843037174051614\n",
      "train loss:0.015963240423989174\n",
      "train loss:0.07532838271215805\n",
      "train loss:0.03523251600463301\n",
      "train loss:0.06157944281674506\n",
      "train loss:0.03635444311060921\n",
      "train loss:0.022214344238334377\n",
      "train loss:0.02464197041363868\n",
      "train loss:0.043452569121770465\n",
      "train loss:0.04681049943213557\n",
      "train loss:0.020295856229380637\n",
      "train loss:0.05357222800489772\n",
      "train loss:0.02595313137059796\n",
      "train loss:0.03949729751704638\n",
      "train loss:0.01156621481212372\n",
      "train loss:0.017373335526006672\n",
      "train loss:0.07305422017842017\n",
      "train loss:0.044099387170745984\n",
      "train loss:0.014965645912644456\n",
      "train loss:0.027817062122663803\n",
      "train loss:0.02533784812656963\n",
      "train loss:0.024760546762897982\n",
      "train loss:0.032341045278868004\n",
      "train loss:0.017099615375584357\n",
      "train loss:0.02094097973330246\n",
      "train loss:0.029481002016150427\n",
      "train loss:0.03320069121974009\n",
      "train loss:0.036755407947048506\n",
      "train loss:0.04279207287931675\n",
      "train loss:0.022880204793075967\n",
      "train loss:0.08529589405946197\n",
      "train loss:0.03925498919327443\n",
      "train loss:0.05193743849961321\n",
      "train loss:0.04383361266473929\n",
      "train loss:0.06960698783068882\n",
      "train loss:0.04647989297775441\n",
      "train loss:0.03398686476484183\n",
      "train loss:0.056062330733684884\n",
      "train loss:0.08024920434598863\n",
      "train loss:0.04179150834515521\n",
      "train loss:0.026061604291019\n",
      "train loss:0.035882220915712186\n",
      "train loss:0.022167959009694614\n",
      "train loss:0.045452080306619756\n",
      "train loss:0.02162974178217136\n",
      "train loss:0.0483782420065673\n",
      "train loss:0.04012010489798548\n",
      "train loss:0.02174775696488366\n",
      "train loss:0.04714540690453787\n",
      "train loss:0.045005666044448375\n",
      "train loss:0.054399476107639816\n",
      "train loss:0.034337460546131425\n",
      "train loss:0.08322102613376693\n",
      "train loss:0.013802152750654068\n",
      "train loss:0.036844795036683975\n",
      "train loss:0.022752878863334033\n",
      "train loss:0.13185500483474966\n",
      "train loss:0.037887531645468125\n",
      "train loss:0.019496951347248333\n",
      "train loss:0.0508942915967484\n",
      "train loss:0.015813455929592095\n",
      "train loss:0.04480339729408461\n",
      "train loss:0.03699771538843819\n",
      "train loss:0.026157194645656457\n",
      "train loss:0.01737810281501111\n",
      "train loss:0.02612292042971373\n",
      "train loss:0.026568909330939593\n",
      "train loss:0.03430594124328438\n",
      "train loss:0.06127525689149769\n",
      "train loss:0.033869125988214135\n",
      "train loss:0.027230469197554025\n",
      "train loss:0.02008217922024962\n",
      "train loss:0.03113074550254187\n",
      "train loss:0.057791149907445966\n",
      "train loss:0.06143388426911482\n",
      "train loss:0.06414858575010202\n",
      "train loss:0.01129086221760593\n",
      "train loss:0.02290886212020403\n",
      "train loss:0.03563172469879151\n",
      "train loss:0.030091413128168796\n",
      "train loss:0.02154398717611705\n",
      "train loss:0.06677974669341401\n",
      "train loss:0.01570348324846044\n",
      "train loss:0.0353449426945123\n",
      "train loss:0.04507508325935564\n",
      "train loss:0.12497278201035275\n",
      "train loss:0.022129956675827307\n",
      "train loss:0.04121101414250115\n",
      "train loss:0.07191298524574362\n",
      "train loss:0.01710407360497361\n",
      "train loss:0.018703325064081525\n",
      "train loss:0.05894517888860453\n",
      "train loss:0.07142215511581854\n",
      "train loss:0.07385898590737013\n",
      "train loss:0.05656331455044656\n",
      "train loss:0.046018727190770256\n",
      "train loss:0.062467041353346714\n",
      "train loss:0.03910254641535094\n",
      "train loss:0.047663262451038364\n",
      "train loss:0.03229935363629706\n",
      "train loss:0.012058345176857499\n",
      "train loss:0.03999769892839111\n",
      "train loss:0.014147794775090055\n",
      "train loss:0.032872872359520106\n",
      "train loss:0.0962974599089749\n",
      "train loss:0.028802852338185844\n",
      "train loss:0.05029503051788438\n",
      "train loss:0.02235977193114344\n",
      "train loss:0.024620260793709606\n",
      "train loss:0.02154818881501135\n",
      "train loss:0.0522382152406251\n",
      "train loss:0.03364031335745613\n",
      "train loss:0.01747741757557104\n",
      "train loss:0.07119528898813987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02670356954843974\n",
      "train loss:0.0592458556077994\n",
      "train loss:0.018915910448993475\n",
      "train loss:0.04067610514997536\n",
      "train loss:0.014615977835023351\n",
      "train loss:0.025323116999482576\n",
      "train loss:0.055384898555727397\n",
      "train loss:0.055483530587659036\n",
      "train loss:0.027446995055953147\n",
      "train loss:0.1226973422445667\n",
      "train loss:0.04968869904002928\n",
      "train loss:0.019340489322983177\n",
      "train loss:0.045292080054313066\n",
      "train loss:0.014306237330494216\n",
      "train loss:0.016290657926215705\n",
      "train loss:0.03976350063349962\n",
      "train loss:0.0218374683205592\n",
      "train loss:0.035500524311654125\n",
      "train loss:0.03982731211821751\n",
      "train loss:0.01745242268652104\n",
      "train loss:0.015993395547284118\n",
      "train loss:0.04637469659729944\n",
      "train loss:0.01808635221960865\n",
      "train loss:0.030612724029830413\n",
      "train loss:0.033288446567286904\n",
      "train loss:0.03375649539200062\n",
      "train loss:0.08007312817890094\n",
      "train loss:0.03326917089105507\n",
      "train loss:0.016277000414725903\n",
      "train loss:0.08677270925225354\n",
      "train loss:0.011186445398815983\n",
      "train loss:0.03939721795363758\n",
      "train loss:0.054250869263803556\n",
      "train loss:0.023364395430771304\n",
      "train loss:0.020408033483921076\n",
      "train loss:0.02529252279472365\n",
      "train loss:0.025367048657531047\n",
      "train loss:0.040969094044433954\n",
      "train loss:0.05865817180894682\n",
      "train loss:0.03871075416069014\n",
      "train loss:0.027157001393183666\n",
      "train loss:0.026058638367510812\n",
      "train loss:0.0279496908645725\n",
      "train loss:0.05694912776785992\n",
      "train loss:0.03868018937803055\n",
      "train loss:0.025288485906241692\n",
      "train loss:0.036871507371457286\n",
      "train loss:0.07965046241710967\n",
      "train loss:0.03610418101765217\n",
      "train loss:0.03121907167281435\n",
      "train loss:0.01778934153118486\n",
      "train loss:0.018771502633156828\n",
      "train loss:0.08725112941121599\n",
      "train loss:0.018187155528097604\n",
      "train loss:0.04838709544922033\n",
      "train loss:0.06810507947111845\n",
      "train loss:0.10521870490671816\n",
      "train loss:0.010343418090675238\n",
      "train loss:0.03507297173904404\n",
      "train loss:0.0663870081850597\n",
      "train loss:0.03227011916256089\n",
      "train loss:0.015760150464956\n",
      "train loss:0.01839691636065176\n",
      "train loss:0.01495297500467558\n",
      "train loss:0.01879756531493221\n",
      "train loss:0.010125811694304465\n",
      "train loss:0.06496130363592055\n",
      "train loss:0.013404038148825275\n",
      "train loss:0.013648142117427932\n",
      "train loss:0.03363453926714565\n",
      "train loss:0.019953666741345705\n",
      "train loss:0.043270440221211584\n",
      "train loss:0.1526322275245334\n",
      "train loss:0.020348417252434282\n",
      "train loss:0.05795949025761166\n",
      "train loss:0.0509907867626577\n",
      "train loss:0.04562038314808797\n",
      "train loss:0.03620107070792236\n",
      "train loss:0.031961251341321245\n",
      "train loss:0.021491148232638292\n",
      "train loss:0.05175584137475384\n",
      "train loss:0.025096797126760873\n",
      "train loss:0.10926593051398716\n",
      "train loss:0.06747303205531634\n",
      "train loss:0.023660926601858437\n",
      "train loss:0.02676416538172918\n",
      "train loss:0.04058700833548864\n",
      "train loss:0.024649858953493947\n",
      "train loss:0.025070884209066162\n",
      "train loss:0.02046685887208149\n",
      "train loss:0.021942087120284226\n",
      "train loss:0.03180626722641118\n",
      "train loss:0.06721683229702273\n",
      "train loss:0.06291901045219314\n",
      "train loss:0.046823565583376074\n",
      "train loss:0.04028975566875109\n",
      "train loss:0.036455855701825295\n",
      "train loss:0.036077813149861096\n",
      "train loss:0.09987983783923557\n",
      "train loss:0.04253345974187767\n",
      "train loss:0.05566478365651908\n",
      "train loss:0.013384220654804404\n",
      "train loss:0.01744584733711705\n",
      "train loss:0.04981732573696007\n",
      "train loss:0.06790311834665688\n",
      "train loss:0.024867772132675263\n",
      "train loss:0.02932251528870654\n",
      "train loss:0.09850990627864077\n",
      "train loss:0.023157941000992503\n",
      "train loss:0.022999074840736166\n",
      "train loss:0.03485919605881924\n",
      "train loss:0.026000354911026147\n",
      "train loss:0.06654765161500677\n",
      "train loss:0.029746058581622443\n",
      "train loss:0.06604405318414837\n",
      "train loss:0.02105279339279783\n",
      "train loss:0.032546314079097895\n",
      "train loss:0.043421604517767304\n",
      "train loss:0.060875723787043805\n",
      "train loss:0.07176164386501827\n",
      "train loss:0.007563195948950638\n",
      "train loss:0.046142614254851264\n",
      "train loss:0.031649835841365714\n",
      "train loss:0.019349068576144372\n",
      "train loss:0.02997139453596022\n",
      "train loss:0.037964357314079614\n",
      "train loss:0.027426271329576365\n",
      "train loss:0.017232361120249576\n",
      "train loss:0.02154863131778327\n",
      "train loss:0.056220487988394546\n",
      "train loss:0.01562493878130407\n",
      "train loss:0.05487266594373989\n",
      "train loss:0.01749603210130284\n",
      "train loss:0.02360132979461706\n",
      "train loss:0.011053611408002142\n",
      "train loss:0.014776918567375074\n",
      "train loss:0.07827111465114542\n",
      "train loss:0.04609011310162594\n",
      "train loss:0.03386945323839428\n",
      "train loss:0.02169139206060209\n",
      "train loss:0.037149290735049065\n",
      "train loss:0.03643647485553713\n",
      "train loss:0.032419757876026634\n",
      "train loss:0.015163855107451232\n",
      "train loss:0.030349860024470522\n",
      "train loss:0.05873399432228013\n",
      "train loss:0.008852365431581972\n",
      "=== epoch:6, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.077989091684813\n",
      "train loss:0.04460845807253919\n",
      "train loss:0.05632018077340186\n",
      "train loss:0.02450608479683203\n",
      "train loss:0.05647388426271827\n",
      "train loss:0.026488050437802283\n",
      "train loss:0.037640548429634615\n",
      "train loss:0.03380020188721383\n",
      "train loss:0.016127702457082428\n",
      "train loss:0.033322034190981176\n",
      "train loss:0.02150826026449128\n",
      "train loss:0.08357937712932756\n",
      "train loss:0.04753834867404549\n",
      "train loss:0.050316409240189784\n",
      "train loss:0.019955369754068093\n",
      "train loss:0.06840372351676005\n",
      "train loss:0.01896517002035165\n",
      "train loss:0.04396340326162739\n",
      "train loss:0.03155608382564907\n",
      "train loss:0.028655244219682643\n",
      "train loss:0.015011216571219867\n",
      "train loss:0.034641168377568504\n",
      "train loss:0.01567921092009049\n",
      "train loss:0.01624319959098549\n",
      "train loss:0.0183598861919384\n",
      "train loss:0.024906658453029577\n",
      "train loss:0.03432540013581667\n",
      "train loss:0.02572304920858727\n",
      "train loss:0.20093000662927107\n",
      "train loss:0.05066224062964739\n",
      "train loss:0.11314954523725756\n",
      "train loss:0.058879730841646744\n",
      "train loss:0.028970918358671462\n",
      "train loss:0.02966085867263684\n",
      "train loss:0.01634142153449102\n",
      "train loss:0.035811470109878875\n",
      "train loss:0.013126995385052156\n",
      "train loss:0.031141039095513544\n",
      "train loss:0.020851792649273445\n",
      "train loss:0.034806166656158155\n",
      "train loss:0.0634136782690703\n",
      "train loss:0.03882486044249098\n",
      "train loss:0.06853996615353652\n",
      "train loss:0.025683862839168202\n",
      "train loss:0.10398104181019735\n",
      "train loss:0.03970085468638149\n",
      "train loss:0.02143176350935183\n",
      "train loss:0.02465544050991619\n",
      "train loss:0.06418286528955368\n",
      "train loss:0.03111289807305382\n",
      "train loss:0.03797848736805781\n",
      "train loss:0.01092234624571024\n",
      "train loss:0.02798580185579892\n",
      "train loss:0.060297197743435366\n",
      "train loss:0.02601804242073104\n",
      "train loss:0.01812209538833826\n",
      "train loss:0.011567707008097108\n",
      "train loss:0.02922172110951544\n",
      "train loss:0.05957655005297325\n",
      "train loss:0.013035000217257382\n",
      "train loss:0.043402493642743734\n",
      "train loss:0.039179642077911\n",
      "train loss:0.023915345319390352\n",
      "train loss:0.022234289844657376\n",
      "train loss:0.05326951912925279\n",
      "train loss:0.025030924265760514\n",
      "train loss:0.044747921087570336\n",
      "train loss:0.026582793240283405\n",
      "train loss:0.02428211271492442\n",
      "train loss:0.021724620561395024\n",
      "train loss:0.024948289402124374\n",
      "train loss:0.06359548248820802\n",
      "train loss:0.016222644175247172\n",
      "train loss:0.03613967973299081\n",
      "train loss:0.03150114827243663\n",
      "train loss:0.028878982723425685\n",
      "train loss:0.03675577549170329\n",
      "train loss:0.020308662509650344\n",
      "train loss:0.08560519589602465\n",
      "train loss:0.036563596257745724\n",
      "train loss:0.029898459212478287\n",
      "train loss:0.026597451231180513\n",
      "train loss:0.04691946660062686\n",
      "train loss:0.03661262353622878\n",
      "train loss:0.02558155015639274\n",
      "train loss:0.027571866981416938\n",
      "train loss:0.04519556274200979\n",
      "train loss:0.029275164132618093\n",
      "train loss:0.012814023556891048\n",
      "train loss:0.04303065233402412\n",
      "train loss:0.06332348100152681\n",
      "train loss:0.02385253664999563\n",
      "train loss:0.05003248196902001\n",
      "train loss:0.07482702558538609\n",
      "train loss:0.01683935258584738\n",
      "train loss:0.047979812830851426\n",
      "train loss:0.06798131381017716\n",
      "train loss:0.02083611684357067\n",
      "train loss:0.043995629221334855\n",
      "train loss:0.06869207202786187\n",
      "train loss:0.037255416837228696\n",
      "train loss:0.026419200020054042\n",
      "train loss:0.026916940651398536\n",
      "train loss:0.011081732294663988\n",
      "train loss:0.021975761706483056\n",
      "train loss:0.02273831976273676\n",
      "train loss:0.024654200673423766\n",
      "train loss:0.012736672546588916\n",
      "train loss:0.01551777654881495\n",
      "train loss:0.04258894663832933\n",
      "train loss:0.033045337938845884\n",
      "train loss:0.023049275870958997\n",
      "train loss:0.016718660076453964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.030101752330300387\n",
      "train loss:0.03302723753346654\n",
      "train loss:0.02455512112893162\n",
      "train loss:0.05552637725693032\n",
      "train loss:0.06255443988511718\n",
      "train loss:0.01570217535563997\n",
      "train loss:0.05278149362890205\n",
      "train loss:0.020772970767485063\n",
      "train loss:0.04842327063008619\n",
      "train loss:0.029346036326045283\n",
      "train loss:0.02406415352744652\n",
      "train loss:0.020453655622304848\n",
      "train loss:0.013396141535040318\n",
      "train loss:0.029887754372576562\n",
      "train loss:0.012714752856890799\n",
      "train loss:0.009588490879754882\n",
      "train loss:0.02865421801989062\n",
      "train loss:0.050173630146712014\n",
      "train loss:0.022324454723563255\n",
      "train loss:0.11302598863680037\n",
      "train loss:0.02986905238597741\n",
      "train loss:0.04366314999762508\n",
      "train loss:0.029636210657097362\n",
      "train loss:0.04538031285449266\n",
      "train loss:0.054010330377533215\n",
      "train loss:0.019701609379697105\n",
      "train loss:0.030704471005367884\n",
      "train loss:0.02082704674937141\n",
      "train loss:0.022685456406331968\n",
      "train loss:0.03456169887155187\n",
      "train loss:0.0385760456280566\n",
      "train loss:0.04107257039365474\n",
      "train loss:0.06486575393544172\n",
      "train loss:0.02674084267888555\n",
      "train loss:0.025808562779281972\n",
      "train loss:0.036035912414405694\n",
      "train loss:0.025116914865133323\n",
      "train loss:0.02163125693882244\n",
      "train loss:0.0695092822835257\n",
      "train loss:0.04559540761979146\n",
      "train loss:0.059186015515688736\n",
      "train loss:0.04612538268733302\n",
      "train loss:0.028571077206471344\n",
      "train loss:0.03814772283410204\n",
      "train loss:0.052322893992038925\n",
      "train loss:0.062225755053883766\n",
      "train loss:0.060598510655017576\n",
      "train loss:0.040072905899324\n",
      "train loss:0.01885196751133577\n",
      "train loss:0.051707064285285195\n",
      "train loss:0.08317603202172354\n",
      "train loss:0.057240758026890014\n",
      "train loss:0.011602088781733844\n",
      "train loss:0.03537196623360607\n",
      "train loss:0.03715213213072758\n",
      "train loss:0.022374335908961717\n",
      "train loss:0.02494298748874507\n",
      "train loss:0.04785039430386298\n",
      "train loss:0.016654876364508894\n",
      "train loss:0.05688317932491624\n",
      "train loss:0.037668618376682846\n",
      "train loss:0.02965903176560686\n",
      "train loss:0.08717681679928019\n",
      "train loss:0.0192672933801978\n",
      "train loss:0.034590055343070227\n",
      "train loss:0.036028586425765334\n",
      "train loss:0.038517543930600896\n",
      "train loss:0.01132204095196559\n",
      "train loss:0.02208479003268942\n",
      "train loss:0.026925599243535037\n",
      "train loss:0.013866786357024253\n",
      "train loss:0.041531878006781715\n",
      "train loss:0.02954518044109094\n",
      "train loss:0.03531396580386142\n",
      "train loss:0.03395962169897017\n",
      "train loss:0.008496576606128618\n",
      "train loss:0.038273009814319386\n",
      "train loss:0.016767359510342927\n",
      "train loss:0.03003421168009655\n",
      "train loss:0.036329126965540526\n",
      "train loss:0.028796869618944357\n",
      "train loss:0.018358073354863197\n",
      "train loss:0.01825259565787637\n",
      "train loss:0.01326404378074823\n",
      "train loss:0.02632843204380914\n",
      "train loss:0.020825953698543585\n",
      "train loss:0.021640488623904296\n",
      "train loss:0.017702810963280354\n",
      "train loss:0.020475971383483962\n",
      "train loss:0.02302513190531871\n",
      "train loss:0.029951581426560896\n",
      "train loss:0.054385417785595525\n",
      "train loss:0.03688874225206037\n",
      "train loss:0.04857151170086528\n",
      "train loss:0.03700764441666478\n",
      "train loss:0.013854937216126493\n",
      "train loss:0.05269806782614103\n",
      "train loss:0.02606607709527908\n",
      "train loss:0.04312389500706511\n",
      "train loss:0.02230458925549201\n",
      "train loss:0.08633815627418884\n",
      "train loss:0.037378195173488796\n",
      "train loss:0.06039706157851929\n",
      "train loss:0.027995856663582827\n",
      "train loss:0.04092532558826308\n",
      "train loss:0.12820377090378238\n",
      "train loss:0.039997104662389794\n",
      "train loss:0.06773021619719591\n",
      "train loss:0.03632881197161407\n",
      "train loss:0.021119492310085645\n",
      "train loss:0.025717739547704274\n",
      "train loss:0.03156488322056135\n",
      "train loss:0.012204909994208453\n",
      "train loss:0.08481810938517942\n",
      "train loss:0.038357567320224314\n",
      "train loss:0.013166854498614301\n",
      "train loss:0.00809667373864587\n",
      "train loss:0.06451516523155122\n",
      "train loss:0.05944274025718465\n",
      "train loss:0.03648397267057163\n",
      "train loss:0.024971630654034942\n",
      "train loss:0.021288477203789697\n",
      "train loss:0.022889507587842285\n",
      "train loss:0.03314735710672962\n",
      "train loss:0.017177624739158986\n",
      "train loss:0.06107111102018343\n",
      "train loss:0.03353863333857343\n",
      "train loss:0.017139380669061\n",
      "train loss:0.018814082406222205\n",
      "train loss:0.02318694537038503\n",
      "train loss:0.029774357989875745\n",
      "train loss:0.027290114720698\n",
      "train loss:0.057998879721542836\n",
      "train loss:0.016483873339154076\n",
      "train loss:0.02073596608837898\n",
      "train loss:0.0380900260044388\n",
      "train loss:0.045546927888438615\n",
      "train loss:0.019086737809577616\n",
      "train loss:0.03177448061240498\n",
      "train loss:0.025431796331850557\n",
      "train loss:0.05033353889765437\n",
      "train loss:0.11756239785378085\n",
      "train loss:0.1125604629006546\n",
      "train loss:0.040603370175877646\n",
      "train loss:0.014179079213045896\n",
      "train loss:0.02905266774289494\n",
      "train loss:0.033102399519103466\n",
      "train loss:0.011503879732468282\n",
      "train loss:0.04625665015259241\n",
      "train loss:0.031263105764450426\n",
      "train loss:0.03305456938820832\n",
      "train loss:0.019300681911276284\n",
      "train loss:0.027781145065010535\n",
      "train loss:0.01772645301960466\n",
      "train loss:0.03985680077744156\n",
      "train loss:0.01801965461363084\n",
      "train loss:0.07590947432356271\n",
      "train loss:0.04787523327707824\n",
      "train loss:0.048949877161046776\n",
      "train loss:0.03226326864404573\n",
      "train loss:0.05766547778037411\n",
      "train loss:0.04348834776709034\n",
      "train loss:0.03978733027832238\n",
      "train loss:0.02601034574064935\n",
      "train loss:0.020429717747786954\n",
      "train loss:0.06289822081590858\n",
      "train loss:0.03650521969120083\n",
      "train loss:0.033255043365192784\n",
      "train loss:0.04290955441716852\n",
      "train loss:0.03456092209761845\n",
      "train loss:0.01730144109728666\n",
      "train loss:0.0798860776711028\n",
      "train loss:0.07740738407463943\n",
      "train loss:0.029832387738857838\n",
      "train loss:0.018049948122968073\n",
      "train loss:0.021769677240168747\n",
      "train loss:0.0296270101506493\n",
      "train loss:0.0752114173126737\n",
      "train loss:0.010257951421700334\n",
      "train loss:0.05263033505299079\n",
      "train loss:0.02115761185872977\n",
      "train loss:0.0369237235296875\n",
      "train loss:0.06538864147744645\n",
      "train loss:0.017168913682714528\n",
      "train loss:0.12407879072944214\n",
      "train loss:0.013492827653330423\n",
      "train loss:0.037832300372055386\n",
      "train loss:0.0332371392902834\n",
      "train loss:0.02091834345950707\n",
      "train loss:0.017464920550523476\n",
      "train loss:0.03034201278032775\n",
      "train loss:0.017793960047090718\n",
      "train loss:0.03085477777887971\n",
      "train loss:0.024693725889917906\n",
      "train loss:0.06978800327512827\n",
      "train loss:0.017013336841852397\n",
      "train loss:0.02933025482278498\n",
      "train loss:0.032145630700433116\n",
      "train loss:0.02086705911200911\n",
      "train loss:0.02579547245313988\n",
      "train loss:0.024822230207901065\n",
      "train loss:0.0168389991455944\n",
      "train loss:0.020955662384761892\n",
      "train loss:0.052638957057691595\n",
      "train loss:0.022880158510676957\n",
      "train loss:0.02527357464998071\n",
      "train loss:0.06239687458282281\n",
      "train loss:0.007989816345731537\n",
      "train loss:0.02395814246328383\n",
      "train loss:0.0853995352785657\n",
      "train loss:0.06269538543333264\n",
      "train loss:0.013856574892606374\n",
      "train loss:0.016537261507914104\n",
      "train loss:0.03347705307951853\n",
      "train loss:0.0681093819073717\n",
      "train loss:0.04542225539806901\n",
      "train loss:0.016350309597338197\n",
      "train loss:0.08185113330945049\n",
      "train loss:0.04748496561970263\n",
      "train loss:0.03632076360973785\n",
      "train loss:0.017590226752266402\n",
      "train loss:0.04902902695172417\n",
      "train loss:0.03580323678706578\n",
      "train loss:0.010258483123756103\n",
      "train loss:0.01931995183599074\n",
      "train loss:0.05930396746989426\n",
      "train loss:0.012846458736318556\n",
      "train loss:0.023290280345914585\n",
      "train loss:0.020552756788040423\n",
      "train loss:0.021735379255208458\n",
      "train loss:0.058943179846560924\n",
      "train loss:0.03721662510306339\n",
      "train loss:0.030156594909679382\n",
      "train loss:0.037378094354834815\n",
      "train loss:0.036719295339764264\n",
      "train loss:0.012059894675507802\n",
      "train loss:0.039187468201768175\n",
      "train loss:0.015450314755950635\n",
      "train loss:0.005779377726229695\n",
      "train loss:0.07039503679706638\n",
      "train loss:0.036829489275354683\n",
      "train loss:0.04146496751295157\n",
      "train loss:0.07233838016430486\n",
      "train loss:0.014669663100438856\n",
      "train loss:0.022463274224350543\n",
      "train loss:0.03346603479975902\n",
      "train loss:0.028684138232484047\n",
      "train loss:0.02404057354891818\n",
      "train loss:0.03238751309263534\n",
      "train loss:0.018474244708133514\n",
      "train loss:0.013496629142926224\n",
      "train loss:0.025664245125490003\n",
      "train loss:0.026837710308552157\n",
      "train loss:0.028318464949980852\n",
      "train loss:0.03358402916563048\n",
      "train loss:0.014997860637978075\n",
      "train loss:0.04018216865694026\n",
      "train loss:0.05809693406082549\n",
      "train loss:0.07519461817161634\n",
      "train loss:0.02564965328308678\n",
      "train loss:0.020514474089268573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.021542524716206053\n",
      "train loss:0.038383337359485334\n",
      "train loss:0.02885732879215083\n",
      "train loss:0.05512102829778926\n",
      "train loss:0.018515223386736317\n",
      "train loss:0.010844117456505537\n",
      "train loss:0.015402974300937749\n",
      "train loss:0.06434703530230741\n",
      "train loss:0.013611260599508759\n",
      "train loss:0.008295080293223176\n",
      "train loss:0.019064050442861514\n",
      "train loss:0.021083522908981335\n",
      "train loss:0.05144953339271674\n",
      "train loss:0.0822052642518955\n",
      "train loss:0.043686938189659796\n",
      "train loss:0.05809008794289284\n",
      "train loss:0.03444323822937343\n",
      "train loss:0.035493170143320964\n",
      "train loss:0.03842725635505678\n",
      "train loss:0.059704647858678364\n",
      "train loss:0.03551111953030636\n",
      "train loss:0.015498448202182531\n",
      "train loss:0.08149723680811015\n",
      "train loss:0.030045943350051552\n",
      "train loss:0.012402017574988268\n",
      "train loss:0.07839108824470215\n",
      "train loss:0.09155789650414693\n",
      "train loss:0.02881564409963796\n",
      "train loss:0.030632720438208808\n",
      "train loss:0.02269601415584426\n",
      "train loss:0.0371490542237831\n",
      "train loss:0.02533980559531783\n",
      "train loss:0.04166610643065202\n",
      "train loss:0.01484924472776453\n",
      "train loss:0.024410144306279875\n",
      "train loss:0.033659640592791036\n",
      "train loss:0.011819111434012337\n",
      "train loss:0.02054305817943517\n",
      "train loss:0.04599440201086443\n",
      "train loss:0.03968370514066399\n",
      "train loss:0.051517429770632096\n",
      "train loss:0.045443283780745566\n",
      "train loss:0.013754334874556082\n",
      "train loss:0.03251871889825154\n",
      "train loss:0.05682579626634255\n",
      "train loss:0.01678230082205684\n",
      "train loss:0.024681043612625082\n",
      "train loss:0.05537269729249263\n",
      "train loss:0.017788105724488242\n",
      "train loss:0.01792620175939389\n",
      "train loss:0.06127275001048644\n",
      "train loss:0.034277479718021135\n",
      "train loss:0.040647746006408\n",
      "train loss:0.043506892031310695\n",
      "train loss:0.033981873973728004\n",
      "train loss:0.0416632644082019\n",
      "train loss:0.02425447079403789\n",
      "train loss:0.012831442445851281\n",
      "train loss:0.02360399076507552\n",
      "train loss:0.05305635553358989\n",
      "train loss:0.06278930814239721\n",
      "train loss:0.019957314426840525\n",
      "train loss:0.03034397141870791\n",
      "train loss:0.006939257061726413\n",
      "train loss:0.011587229147624924\n",
      "train loss:0.03905799939069877\n",
      "train loss:0.043393027426722186\n",
      "train loss:0.049891069245432886\n",
      "train loss:0.02345674168224813\n",
      "train loss:0.023226761432270107\n",
      "train loss:0.01966828609453068\n",
      "train loss:0.07001488104468974\n",
      "train loss:0.03820687504483943\n",
      "train loss:0.008931431497996446\n",
      "train loss:0.02639550637779407\n",
      "train loss:0.010710268438412043\n",
      "=== epoch:7, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.019962191742960066\n",
      "train loss:0.01740211391420089\n",
      "train loss:0.00809453822015238\n",
      "train loss:0.018322551978899565\n",
      "train loss:0.06202639678975152\n",
      "train loss:0.030788812711041676\n",
      "train loss:0.016269753572097086\n",
      "train loss:0.030907878587361135\n",
      "train loss:0.013961327878137536\n",
      "train loss:0.03942456424672622\n",
      "train loss:0.02402640105482376\n",
      "train loss:0.016739549503592684\n",
      "train loss:0.03982211668156031\n",
      "train loss:0.011057079060499629\n",
      "train loss:0.022024575217314556\n",
      "train loss:0.03976229452434938\n",
      "train loss:0.025488295548359755\n",
      "train loss:0.025142725908107413\n",
      "train loss:0.016679550975495446\n",
      "train loss:0.009421966513851106\n",
      "train loss:0.007218460495551262\n",
      "train loss:0.11186944401612113\n",
      "train loss:0.011149220593325413\n",
      "train loss:0.020089197387394394\n",
      "train loss:0.02235875302698528\n",
      "train loss:0.020933040549541863\n",
      "train loss:0.027222620019638488\n",
      "train loss:0.020563614867597967\n",
      "train loss:0.11493624113941696\n",
      "train loss:0.03341119288562223\n",
      "train loss:0.021813392115052496\n",
      "train loss:0.02101067181378394\n",
      "train loss:0.010033960073216337\n",
      "train loss:0.029019129627455228\n",
      "train loss:0.027884433332729008\n",
      "train loss:0.01839488494158579\n",
      "train loss:0.014125347478765976\n",
      "train loss:0.03607581596472413\n",
      "train loss:0.049955403346085836\n",
      "train loss:0.035308929024887185\n",
      "train loss:0.01083424028754408\n",
      "train loss:0.04985856985912841\n",
      "train loss:0.08056526423721735\n",
      "train loss:0.014828725884170937\n",
      "train loss:0.0541079467071639\n",
      "train loss:0.03417748509636881\n",
      "train loss:0.046649021886178234\n",
      "train loss:0.014211425972665925\n",
      "train loss:0.031362158504753816\n",
      "train loss:0.0464208888089562\n",
      "train loss:0.0478449866650257\n",
      "train loss:0.014851786369969732\n",
      "train loss:0.019965043968587236\n",
      "train loss:0.0225094172747904\n",
      "train loss:0.013394746955100938\n",
      "train loss:0.026269972141309973\n",
      "train loss:0.021358012286676827\n",
      "train loss:0.047888346178859474\n",
      "train loss:0.020949020617113217\n",
      "train loss:0.03489287144997307\n",
      "train loss:0.01623819612558728\n",
      "train loss:0.013031094592346573\n",
      "train loss:0.04362655513576854\n",
      "train loss:0.031051147937493563\n",
      "train loss:0.009831713434646536\n",
      "train loss:0.016788566849678405\n",
      "train loss:0.0422837744966539\n",
      "train loss:0.033751804578459776\n",
      "train loss:0.04285480700049478\n",
      "train loss:0.016092273013267683\n",
      "train loss:0.012280010490642262\n",
      "train loss:0.03678422757147994\n",
      "train loss:0.019478709404126354\n",
      "train loss:0.0422851180816222\n",
      "train loss:0.045375075622890025\n",
      "train loss:0.03138497152096077\n",
      "train loss:0.026884912056487565\n",
      "train loss:0.0639540124647784\n",
      "train loss:0.05743291308231714\n",
      "train loss:0.015392287135042142\n",
      "train loss:0.07372568122387219\n",
      "train loss:0.020451461320636424\n",
      "train loss:0.05826885064455697\n",
      "train loss:0.06744855674838188\n",
      "train loss:0.03670498283677985\n",
      "train loss:0.016662823597280763\n",
      "train loss:0.027303831328846204\n",
      "train loss:0.032976366410076965\n",
      "train loss:0.0236874708170626\n",
      "train loss:0.043749283888436\n",
      "train loss:0.05330806280795352\n",
      "train loss:0.023601777192375904\n",
      "train loss:0.033212068714394596\n",
      "train loss:0.019941144432076566\n",
      "train loss:0.0058859917687218745\n",
      "train loss:0.010879961141781677\n",
      "train loss:0.03410275147547228\n",
      "train loss:0.014938552812131743\n",
      "train loss:0.09422680388198364\n",
      "train loss:0.019175656678150757\n",
      "train loss:0.021720238381692247\n",
      "train loss:0.024058347674499922\n",
      "train loss:0.01904367163554566\n",
      "train loss:0.03285528375187981\n",
      "train loss:0.04895180778933465\n",
      "train loss:0.05048784490470186\n",
      "train loss:0.02845405976321698\n",
      "train loss:0.012130305850258667\n",
      "train loss:0.08485817746335743\n",
      "train loss:0.017566143104847085\n",
      "train loss:0.05330569652655604\n",
      "train loss:0.023072506281664188\n",
      "train loss:0.022250490263933494\n",
      "train loss:0.055987143152648675\n",
      "train loss:0.04585658651574886\n",
      "train loss:0.010924310396203101\n",
      "train loss:0.033648374293543754\n",
      "train loss:0.053716362254790484\n",
      "train loss:0.037155273515991995\n",
      "train loss:0.03896457752888249\n",
      "train loss:0.016703177958202378\n",
      "train loss:0.05140139281854264\n",
      "train loss:0.04722635519642559\n",
      "train loss:0.022220542168592487\n",
      "train loss:0.05750952642071782\n",
      "train loss:0.020797031241106656\n",
      "train loss:0.02377760459404079\n",
      "train loss:0.03221613887915346\n",
      "train loss:0.028632929028217912\n",
      "train loss:0.029093123403356812\n",
      "train loss:0.031301307357445365\n",
      "train loss:0.02214939287729325\n",
      "train loss:0.008021109090060012\n",
      "train loss:0.016532122912183658\n",
      "train loss:0.021716934923001783\n",
      "train loss:0.018434579018511518\n",
      "train loss:0.017603334130831528\n",
      "train loss:0.03391634985929126\n",
      "train loss:0.02039601351085432\n",
      "train loss:0.11126603415373677\n",
      "train loss:0.054511945055969416\n",
      "train loss:0.02699427542529626\n",
      "train loss:0.02099944236491015\n",
      "train loss:0.08811804307992756\n",
      "train loss:0.03910839034089657\n",
      "train loss:0.01762815270184941\n",
      "train loss:0.012418334144412544\n",
      "train loss:0.04863774965388571\n",
      "train loss:0.026288124359786013\n",
      "train loss:0.01568587625441205\n",
      "train loss:0.07549494281018733\n",
      "train loss:0.028804090180897454\n",
      "train loss:0.05269482561694552\n",
      "train loss:0.08473349888780991\n",
      "train loss:0.021215429826867416\n",
      "train loss:0.042803764897564406\n",
      "train loss:0.02185020312582578\n",
      "train loss:0.03761116064253376\n",
      "train loss:0.026457495978563932\n",
      "train loss:0.02623614040166162\n",
      "train loss:0.05315575108622208\n",
      "train loss:0.013100109634314228\n",
      "train loss:0.019165008876629207\n",
      "train loss:0.023508640086716773\n",
      "train loss:0.04082882622353579\n",
      "train loss:0.054315270348937794\n",
      "train loss:0.022120053783250337\n",
      "train loss:0.06927260093322403\n",
      "train loss:0.02359761171369962\n",
      "train loss:0.07795705565964046\n",
      "train loss:0.06498096556455059\n",
      "train loss:0.05959494489128537\n",
      "train loss:0.019552145038727767\n",
      "train loss:0.037762526099957866\n",
      "train loss:0.02052643695788346\n",
      "train loss:0.04174176240839711\n",
      "train loss:0.029840315992834793\n",
      "train loss:0.040195196443300885\n",
      "train loss:0.03702411130690029\n",
      "train loss:0.015403485413065505\n",
      "train loss:0.025570847470533974\n",
      "train loss:0.06562116557466652\n",
      "train loss:0.0076585584397574616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.05086582694457877\n",
      "train loss:0.08427158278983575\n",
      "train loss:0.05892645102690396\n",
      "train loss:0.0157326414825087\n",
      "train loss:0.02883638246128738\n",
      "train loss:0.014984100299397894\n",
      "train loss:0.03344879576944946\n",
      "train loss:0.1040255295272991\n",
      "train loss:0.02787015680404388\n",
      "train loss:0.024503063487172225\n",
      "train loss:0.009580794961573227\n",
      "train loss:0.04758182293758592\n",
      "train loss:0.0276710601521121\n",
      "train loss:0.015208077103755763\n",
      "train loss:0.023074866822541965\n",
      "train loss:0.020662302536438658\n",
      "train loss:0.08975128462354767\n",
      "train loss:0.04278476762832191\n",
      "train loss:0.028346096415677646\n",
      "train loss:0.02737685139530774\n",
      "train loss:0.06754931269907229\n",
      "train loss:0.04345298673395864\n",
      "train loss:0.05064701790676858\n",
      "train loss:0.021688727255602367\n",
      "train loss:0.08874993353768038\n",
      "train loss:0.1283262521629489\n",
      "train loss:0.0320930409756167\n",
      "train loss:0.055769998239445326\n",
      "train loss:0.019460036295261204\n",
      "train loss:0.011041385073695249\n",
      "train loss:0.06676822159885488\n",
      "train loss:0.07323539533627345\n",
      "train loss:0.014622517335184498\n",
      "train loss:0.04318283934918368\n",
      "train loss:0.0064955458776719925\n",
      "train loss:0.08147050856707423\n",
      "train loss:0.009375448849045896\n",
      "train loss:0.038825913343881815\n",
      "train loss:0.03606284888694903\n",
      "train loss:0.05958771191836892\n",
      "train loss:0.006420123321420148\n",
      "train loss:0.06067703291882362\n",
      "train loss:0.01880779453645031\n",
      "train loss:0.04372054391370971\n",
      "train loss:0.01819543484534511\n",
      "train loss:0.02042573449378739\n",
      "train loss:0.005919365818845478\n",
      "train loss:0.026169693737176255\n",
      "train loss:0.04395571101589869\n",
      "train loss:0.017684978170525487\n",
      "train loss:0.02714642710146721\n",
      "train loss:0.03180993533179278\n",
      "train loss:0.023993424827726345\n",
      "train loss:0.01572000643385692\n",
      "train loss:0.035802471805455795\n",
      "train loss:0.03065057078743082\n",
      "train loss:0.02618174074478568\n",
      "train loss:0.05557071771867617\n",
      "train loss:0.037079436084014296\n",
      "train loss:0.019777517427621218\n",
      "train loss:0.02254840316586666\n",
      "train loss:0.006955934509925985\n",
      "train loss:0.0241301417741576\n",
      "train loss:0.028978710534815818\n",
      "train loss:0.04499783119264891\n",
      "train loss:0.03732189678041806\n",
      "train loss:0.015242620538971706\n",
      "train loss:0.049126721084671955\n",
      "train loss:0.019130542024421035\n",
      "train loss:0.02579215187839865\n",
      "train loss:0.016214191259376686\n",
      "train loss:0.02058905366785648\n",
      "train loss:0.12230542223851394\n",
      "train loss:0.024247455901084645\n",
      "train loss:0.032805806667664805\n",
      "train loss:0.037464440244846464\n",
      "train loss:0.01457769571862884\n",
      "train loss:0.018551421647275624\n",
      "train loss:0.025981034775686928\n",
      "train loss:0.014852742254605866\n",
      "train loss:0.0129199968937226\n",
      "train loss:0.041699215213297905\n",
      "train loss:0.014805528962935655\n",
      "train loss:0.04750294779684747\n",
      "train loss:0.06688633938063053\n",
      "train loss:0.021115689218689893\n",
      "train loss:0.030976447573803693\n",
      "train loss:0.04213689352756858\n",
      "train loss:0.027043848929314763\n",
      "train loss:0.029701555309100184\n",
      "train loss:0.08304319662522548\n",
      "train loss:0.03908110062150883\n",
      "train loss:0.10125069993123709\n",
      "train loss:0.006390492578727729\n",
      "train loss:0.02348283732935801\n",
      "train loss:0.08029414931156374\n",
      "train loss:0.029599996360351107\n",
      "train loss:0.03895990094419838\n",
      "train loss:0.027723750233623178\n",
      "train loss:0.02398112515575574\n",
      "train loss:0.15663374952498885\n",
      "train loss:0.10918314775725667\n",
      "train loss:0.0406919747612326\n",
      "train loss:0.01618692975323061\n",
      "train loss:0.07070546710506931\n",
      "train loss:0.02889302302342025\n",
      "train loss:0.02849976234904141\n",
      "train loss:0.023510595802464954\n",
      "train loss:0.04413962675805484\n",
      "train loss:0.02069041428598066\n",
      "train loss:0.011709524028786788\n",
      "train loss:0.012139379910960686\n",
      "train loss:0.06482757971736618\n",
      "train loss:0.08568765944625664\n",
      "train loss:0.036355182857905774\n",
      "train loss:0.02345393031708366\n",
      "train loss:0.02376498536665728\n",
      "train loss:0.05187529668996687\n",
      "train loss:0.025224440565785433\n",
      "train loss:0.03857546363140363\n",
      "train loss:0.06393457137449254\n",
      "train loss:0.016921134641791048\n",
      "train loss:0.005747340843310057\n",
      "train loss:0.05365982946980246\n",
      "train loss:0.030671353097654532\n",
      "train loss:0.029035237909500296\n",
      "train loss:0.034035815098404955\n",
      "train loss:0.06617055259147281\n",
      "train loss:0.01848835849430025\n",
      "train loss:0.046852741444505755\n",
      "train loss:0.023085628323038006\n",
      "train loss:0.0153540483199981\n",
      "train loss:0.029138086508699982\n",
      "train loss:0.022996103893000174\n",
      "train loss:0.02386799721865197\n",
      "train loss:0.023643499884252656\n",
      "train loss:0.05785624024815267\n",
      "train loss:0.09770531935508284\n",
      "train loss:0.02334875849710362\n",
      "train loss:0.015588346831516916\n",
      "train loss:0.0263978960249998\n",
      "train loss:0.05909388681031096\n",
      "train loss:0.03527584571294669\n",
      "train loss:0.019338316509043548\n",
      "train loss:0.0974727312009453\n",
      "train loss:0.04673627385715365\n",
      "train loss:0.01713293095906736\n",
      "train loss:0.026467500650870234\n",
      "train loss:0.011406818750308991\n",
      "train loss:0.07344739281460315\n",
      "train loss:0.01661034512579742\n",
      "train loss:0.026075859736368823\n",
      "train loss:0.013155099204561739\n",
      "train loss:0.033102981312666084\n",
      "train loss:0.04076422109441209\n",
      "train loss:0.02051026116002151\n",
      "train loss:0.017887127968394857\n",
      "train loss:0.035662193650895256\n",
      "train loss:0.1371146340226779\n",
      "train loss:0.017203170953324905\n",
      "train loss:0.04170778323607846\n",
      "train loss:0.043759277925635685\n",
      "train loss:0.014250495665552556\n",
      "train loss:0.021661717284031705\n",
      "train loss:0.05417284310400834\n",
      "train loss:0.023294920438248323\n",
      "train loss:0.014139216997841033\n",
      "train loss:0.017768305469873104\n",
      "train loss:0.04083130595313984\n",
      "train loss:0.011493986696476976\n",
      "train loss:0.047977641115674985\n",
      "train loss:0.04068056596858493\n",
      "train loss:0.028688215155097136\n",
      "train loss:0.03780261767709368\n",
      "train loss:0.04225608851299886\n",
      "train loss:0.010246766038673628\n",
      "train loss:0.027355579123086272\n",
      "train loss:0.07129660024413083\n",
      "train loss:0.022817203532630326\n",
      "train loss:0.03919252853801502\n",
      "train loss:0.017891973953090726\n",
      "train loss:0.007802417866424041\n",
      "train loss:0.06416926330489126\n",
      "train loss:0.08770826476093217\n",
      "train loss:0.08935154673746216\n",
      "train loss:0.0761705181715963\n",
      "train loss:0.03482933028094887\n",
      "train loss:0.033407304877287186\n",
      "train loss:0.022361490706269548\n",
      "train loss:0.011733717033131318\n",
      "train loss:0.023057818961890183\n",
      "train loss:0.03995803466139112\n",
      "train loss:0.020893241091533173\n",
      "train loss:0.03406048042152848\n",
      "train loss:0.054832878735064775\n",
      "train loss:0.03411125300679747\n",
      "train loss:0.020193481843408582\n",
      "train loss:0.011916798557414676\n",
      "train loss:0.03366672533021067\n",
      "train loss:0.09030783312418084\n",
      "train loss:0.019168682965973894\n",
      "train loss:0.05118456010931641\n",
      "train loss:0.03387273435337714\n",
      "train loss:0.014459403464158072\n",
      "train loss:0.04146488876290037\n",
      "train loss:0.023715364598010217\n",
      "train loss:0.011023229121658502\n",
      "train loss:0.012277926143077045\n",
      "train loss:0.031020930255604356\n",
      "train loss:0.05328046546893119\n",
      "train loss:0.03618577177395506\n",
      "train loss:0.03499382300122809\n",
      "train loss:0.028475221474175628\n",
      "train loss:0.008977301082175577\n",
      "train loss:0.018664773707914722\n",
      "train loss:0.019354984260217346\n",
      "train loss:0.01935738275503942\n",
      "train loss:0.04078537852710249\n",
      "train loss:0.024881517975942336\n",
      "train loss:0.016575743444375606\n",
      "train loss:0.019222309107887362\n",
      "train loss:0.01354016002730127\n",
      "train loss:0.0215900172858875\n",
      "train loss:0.019506906522366085\n",
      "train loss:0.016030412017050997\n",
      "train loss:0.040545558330074234\n",
      "train loss:0.07414803548996644\n",
      "train loss:0.028546362909660793\n",
      "train loss:0.11158100038212382\n",
      "train loss:0.017140245181464432\n",
      "train loss:0.013088317697745742\n",
      "train loss:0.035514162021510726\n",
      "train loss:0.009374428404343159\n",
      "train loss:0.01134561311388852\n",
      "train loss:0.03484666070529005\n",
      "train loss:0.026048377407517004\n",
      "train loss:0.028644632627469645\n",
      "train loss:0.03831480701610116\n",
      "train loss:0.013591256435428346\n",
      "train loss:0.07975514936672556\n",
      "train loss:0.016365189831467543\n",
      "train loss:0.020232230877619092\n",
      "train loss:0.018790004072147953\n",
      "train loss:0.019841412028404098\n",
      "train loss:0.01857670251907288\n",
      "train loss:0.06791132902683646\n",
      "train loss:0.03657363603500816\n",
      "train loss:0.012020718869759561\n",
      "train loss:0.015447794442194896\n",
      "train loss:0.011888824776090299\n",
      "train loss:0.047497622847308775\n",
      "train loss:0.02087622916235192\n",
      "train loss:0.009764501170425042\n",
      "train loss:0.055870805754385904\n",
      "train loss:0.005543874791800424\n",
      "train loss:0.015904818444655685\n",
      "train loss:0.06796216883026404\n",
      "train loss:0.06915734905479849\n",
      "train loss:0.03927115395079328\n",
      "train loss:0.015489848494039934\n",
      "train loss:0.044615941178658976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.013621348513949819\n",
      "train loss:0.05973333276702242\n",
      "train loss:0.021398642111209818\n",
      "train loss:0.088840180086137\n",
      "train loss:0.06208554508838098\n",
      "train loss:0.03390735591669941\n",
      "=== epoch:8, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.02773668169301466\n",
      "train loss:0.028945842804525373\n",
      "train loss:0.019276427684144542\n",
      "train loss:0.033935428686832705\n",
      "train loss:0.066515323123288\n",
      "train loss:0.020013243987073544\n",
      "train loss:0.04000242413402357\n",
      "train loss:0.03242337149561808\n",
      "train loss:0.12149771314312349\n",
      "train loss:0.027679562238956342\n",
      "train loss:0.01534927675963621\n",
      "train loss:0.05734545879096457\n",
      "train loss:0.016524511340887237\n",
      "train loss:0.033466848006674994\n",
      "train loss:0.08620751669780599\n",
      "train loss:0.01017794211880193\n",
      "train loss:0.04088249103164436\n",
      "train loss:0.019626813844132122\n",
      "train loss:0.01270611905244823\n",
      "train loss:0.032022910070490976\n",
      "train loss:0.023636492480469288\n",
      "train loss:0.038647942096567835\n",
      "train loss:0.013848978537374556\n",
      "train loss:0.02313487240341987\n",
      "train loss:0.05600925628418898\n",
      "train loss:0.025544118175219385\n",
      "train loss:0.19262781917789154\n",
      "train loss:0.01230242314485719\n",
      "train loss:0.024788621189154304\n",
      "train loss:0.037100741613032985\n",
      "train loss:0.019578363599993365\n",
      "train loss:0.012653633036267629\n",
      "train loss:0.05236012606534688\n",
      "train loss:0.015235924698588303\n",
      "train loss:0.018300815838067593\n",
      "train loss:0.0347262003632789\n",
      "train loss:0.03383766064189894\n",
      "train loss:0.020674162074849952\n",
      "train loss:0.04600951156988535\n",
      "train loss:0.04377966561252275\n",
      "train loss:0.015885074900675287\n",
      "train loss:0.026721889885995432\n",
      "train loss:0.01210026851049875\n",
      "train loss:0.03077666384338097\n",
      "train loss:0.011683846749624978\n",
      "train loss:0.05127062339640144\n",
      "train loss:0.05073044585351503\n",
      "train loss:0.023012846802225755\n",
      "train loss:0.019091401095395113\n",
      "train loss:0.011799203946517029\n",
      "train loss:0.019953428086199782\n",
      "train loss:0.009783086614781144\n",
      "train loss:0.02836381660042677\n",
      "train loss:0.035690379757119646\n",
      "train loss:0.04687675992923972\n",
      "train loss:0.02581676081271063\n",
      "train loss:0.014974181045860966\n",
      "train loss:0.03838717444369366\n",
      "train loss:0.019347663162068502\n",
      "train loss:0.05809189676779356\n",
      "train loss:0.09977307808147023\n",
      "train loss:0.025686227461374136\n",
      "train loss:0.017279908288259677\n",
      "train loss:0.08431808639378792\n",
      "train loss:0.01729876373218962\n",
      "train loss:0.06097440563019\n",
      "train loss:0.03188094340093941\n",
      "train loss:0.017474678029418104\n",
      "train loss:0.03993356915149907\n",
      "train loss:0.007637669151153524\n",
      "train loss:0.02921712458978164\n",
      "train loss:0.009006031727861732\n",
      "train loss:0.04004604394390634\n",
      "train loss:0.02022721286923512\n",
      "train loss:0.04150558538215867\n",
      "train loss:0.042099350795220085\n",
      "train loss:0.023785457494239735\n",
      "train loss:0.028744194479699548\n",
      "train loss:0.009790232745317963\n",
      "train loss:0.032997757256075365\n",
      "train loss:0.013364544804937352\n",
      "train loss:0.011039373515707227\n",
      "train loss:0.027881046018495566\n",
      "train loss:0.01738352134388226\n",
      "train loss:0.04212378744329662\n",
      "train loss:0.023166285119401202\n",
      "train loss:0.01277854018118699\n",
      "train loss:0.04948665476338804\n",
      "train loss:0.019102241872035772\n",
      "train loss:0.02060008625469729\n",
      "train loss:0.02469204124643476\n",
      "train loss:0.011491323127030778\n",
      "train loss:0.01333887120423177\n",
      "train loss:0.05136563256441882\n",
      "train loss:0.016508887755981565\n",
      "train loss:0.013641164601516056\n",
      "train loss:0.01625981990326571\n",
      "train loss:0.08531519721907113\n",
      "train loss:0.02023358905510277\n",
      "train loss:0.1345781954834908\n",
      "train loss:0.018919716832456752\n",
      "train loss:0.04562261522873367\n",
      "train loss:0.062332619971007024\n",
      "train loss:0.02672026683424514\n",
      "train loss:0.03479596242170207\n",
      "train loss:0.021425593638076706\n",
      "train loss:0.029717853941265374\n",
      "train loss:0.005274108253534473\n",
      "train loss:0.05392958329676532\n",
      "train loss:0.027168967822839108\n",
      "train loss:0.027127244360172983\n",
      "train loss:0.03710954159582884\n",
      "train loss:0.030584189427812894\n",
      "train loss:0.022434902592719633\n",
      "train loss:0.018351519496333853\n",
      "train loss:0.021590808086748677\n",
      "train loss:0.05541559725572034\n",
      "train loss:0.03475939927151912\n",
      "train loss:0.03754375017846754\n",
      "train loss:0.02812995153599197\n",
      "train loss:0.019192433732204788\n",
      "train loss:0.011324668450176798\n",
      "train loss:0.019878690212936367\n",
      "train loss:0.04886413728554764\n",
      "train loss:0.014540012338241844\n",
      "train loss:0.02470493164008971\n",
      "train loss:0.08817383387082675\n",
      "train loss:0.011734389198951868\n",
      "train loss:0.017217493243362893\n",
      "train loss:0.026936843778949126\n",
      "train loss:0.036589113116854775\n",
      "train loss:0.04515451756695238\n",
      "train loss:0.054138219077797735\n",
      "train loss:0.02323573450990511\n",
      "train loss:0.10722547907762467\n",
      "train loss:0.009276650526609406\n",
      "train loss:0.008846850699205667\n",
      "train loss:0.04153089602874873\n",
      "train loss:0.013555596635044085\n",
      "train loss:0.03506868002487548\n",
      "train loss:0.07272778445999446\n",
      "train loss:0.01933617943682274\n",
      "train loss:0.025542953384393913\n",
      "train loss:0.011950342242225274\n",
      "train loss:0.02103508177700569\n",
      "train loss:0.05598743664239966\n",
      "train loss:0.0317289476964309\n",
      "train loss:0.038047537309789325\n",
      "train loss:0.01034153246538851\n",
      "train loss:0.03158768942170962\n",
      "train loss:0.036232151136059716\n",
      "train loss:0.029209415084203987\n",
      "train loss:0.0255715862832296\n",
      "train loss:0.023742913518567957\n",
      "train loss:0.01453708101435147\n",
      "train loss:0.010997325357393655\n",
      "train loss:0.01182500075487239\n",
      "train loss:0.035329241763544315\n",
      "train loss:0.026385034367681227\n",
      "train loss:0.05295728462846215\n",
      "train loss:0.02525539085297899\n",
      "train loss:0.02367665779373969\n",
      "train loss:0.020138289954325782\n",
      "train loss:0.02452277297665999\n",
      "train loss:0.024077627894854955\n",
      "train loss:0.012315164345914143\n",
      "train loss:0.034292180300388185\n",
      "train loss:0.01887642438284573\n",
      "train loss:0.03042955406314823\n",
      "train loss:0.01882886841176344\n",
      "train loss:0.02758264395602608\n",
      "train loss:0.008999533372191032\n",
      "train loss:0.06756901975586967\n",
      "train loss:0.005401561514283436\n",
      "train loss:0.010944403753965581\n",
      "train loss:0.019852140453997375\n",
      "train loss:0.0197095662703853\n",
      "train loss:0.0561428191989371\n",
      "train loss:0.029436851943315644\n",
      "train loss:0.014806188406844065\n",
      "train loss:0.021267876939583097\n",
      "train loss:0.01717138381986918\n",
      "train loss:0.05084853939207001\n",
      "train loss:0.012947494070381064\n",
      "train loss:0.01792813987286828\n",
      "train loss:0.01568971499280064\n",
      "train loss:0.014664921011381069\n",
      "train loss:0.009242116329881596\n",
      "train loss:0.03678154568486432\n",
      "train loss:0.04357728154200141\n",
      "train loss:0.03070685454519926\n",
      "train loss:0.04137943771100679\n",
      "train loss:0.015173160625769393\n",
      "train loss:0.07306244009593453\n",
      "train loss:0.04472658204991463\n",
      "train loss:0.037724268614484885\n",
      "train loss:0.012100187313232045\n",
      "train loss:0.008442140279978565\n",
      "train loss:0.021401711882038738\n",
      "train loss:0.02586799437033432\n",
      "train loss:0.02195061109273955\n",
      "train loss:0.026425799593527362\n",
      "train loss:0.0217788522849545\n",
      "train loss:0.055014309171111914\n",
      "train loss:0.011777151905984128\n",
      "train loss:0.06172329954013711\n",
      "train loss:0.07852937940062649\n",
      "train loss:0.027669220486206455\n",
      "train loss:0.03162789260135343\n",
      "train loss:0.011898477604416742\n",
      "train loss:0.02797800506620123\n",
      "train loss:0.027635464946594235\n",
      "train loss:0.01745141304176812\n",
      "train loss:0.06978298017723363\n",
      "train loss:0.04932054563186614\n",
      "train loss:0.04344105338851311\n",
      "train loss:0.09476065967427663\n",
      "train loss:0.01524389213218598\n",
      "train loss:0.023312762561724966\n",
      "train loss:0.005014915165986589\n",
      "train loss:0.015150814431269824\n",
      "train loss:0.018601858178002823\n",
      "train loss:0.007272106010817684\n",
      "train loss:0.010609438768517858\n",
      "train loss:0.07399960677581324\n",
      "train loss:0.054670622039948935\n",
      "train loss:0.014745322522401566\n",
      "train loss:0.04377890994999637\n",
      "train loss:0.029676591440203223\n",
      "train loss:0.06308689786229427\n",
      "train loss:0.043673879869052794\n",
      "train loss:0.03343764482234044\n",
      "train loss:0.024802500415166965\n",
      "train loss:0.017305404153127354\n",
      "train loss:0.040266747731245285\n",
      "train loss:0.02410683152961152\n",
      "train loss:0.01238314784783727\n",
      "train loss:0.03922662740331664\n",
      "train loss:0.016025614004833537\n",
      "train loss:0.01374778558011742\n",
      "train loss:0.01970054073339382\n",
      "train loss:0.07314103093447137\n",
      "train loss:0.03187508324019\n",
      "train loss:0.008185949754039993\n",
      "train loss:0.027279068792091136\n",
      "train loss:0.010863129420829211\n",
      "train loss:0.054936513871662485\n",
      "train loss:0.03708896943212661\n",
      "train loss:0.016788474613803304\n",
      "train loss:0.013475241434089432\n",
      "train loss:0.02237231621819396\n",
      "train loss:0.04621043642788839\n",
      "train loss:0.014234054292719298\n",
      "train loss:0.006507656653238981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03619513543349479\n",
      "train loss:0.02012979433878251\n",
      "train loss:0.013331340985987247\n",
      "train loss:0.012936358670823454\n",
      "train loss:0.05443991560423908\n",
      "train loss:0.012417984429699302\n",
      "train loss:0.046535280943541625\n",
      "train loss:0.016503332915197632\n",
      "train loss:0.020191995111303623\n",
      "train loss:0.0253425438478162\n",
      "train loss:0.021218069274100006\n",
      "train loss:0.007782897578153466\n",
      "train loss:0.01468745843861302\n",
      "train loss:0.06648388647137066\n",
      "train loss:0.028587597704185053\n",
      "train loss:0.022425158922578837\n",
      "train loss:0.016762738982669455\n",
      "train loss:0.021843224572900173\n",
      "train loss:0.03454047388324674\n",
      "train loss:0.02054344684927505\n",
      "train loss:0.014568324560961094\n",
      "train loss:0.01091279227785473\n",
      "train loss:0.055823704434338234\n",
      "train loss:0.07388494852456973\n",
      "train loss:0.0432335430117952\n",
      "train loss:0.009732111775721596\n",
      "train loss:0.027001474750845952\n",
      "train loss:0.029421241184382087\n",
      "train loss:0.011446473682003324\n",
      "train loss:0.03148347694692036\n",
      "train loss:0.012348958156577834\n",
      "train loss:0.019680941030938526\n",
      "train loss:0.022857482687384058\n",
      "train loss:0.012037285442849517\n",
      "train loss:0.011097346478032764\n",
      "train loss:0.012676480577737579\n",
      "train loss:0.015469257515134602\n",
      "train loss:0.04011250204957485\n",
      "train loss:0.0077057538246419425\n",
      "train loss:0.016810252891870936\n",
      "train loss:0.045836810804478205\n",
      "train loss:0.022137150540091074\n",
      "train loss:0.01561164185771663\n",
      "train loss:0.038913677745695006\n",
      "train loss:0.10369968605442699\n",
      "train loss:0.02034657844261316\n",
      "train loss:0.019769438866465528\n",
      "train loss:0.014561229168059799\n",
      "train loss:0.011305960985589045\n",
      "train loss:0.05270412215712908\n",
      "train loss:0.04836922579015454\n",
      "train loss:0.018436956476983762\n",
      "train loss:0.0293804741536501\n",
      "train loss:0.011877794079377231\n",
      "train loss:0.019327054391306043\n",
      "train loss:0.015892345344612115\n",
      "train loss:0.014767813759503338\n",
      "train loss:0.09840010427603949\n",
      "train loss:0.045157581891918014\n",
      "train loss:0.05953119939800082\n",
      "train loss:0.025271801471560208\n",
      "train loss:0.03624034652663779\n",
      "train loss:0.023265772678228273\n",
      "train loss:0.017003785314675945\n",
      "train loss:0.04131146240741583\n",
      "train loss:0.03273011018859218\n",
      "train loss:0.01661115119025686\n",
      "train loss:0.06321586404262863\n",
      "train loss:0.023735374934046826\n",
      "train loss:0.01973362152999618\n",
      "train loss:0.02586651692856843\n",
      "train loss:0.006806321145109817\n",
      "train loss:0.01690049295290731\n",
      "train loss:0.016340476396250393\n",
      "train loss:0.011015070588732478\n",
      "train loss:0.03798335286093291\n",
      "train loss:0.02744048495537959\n",
      "train loss:0.09284579458614214\n",
      "train loss:0.057923265267541116\n",
      "train loss:0.029645428708958296\n",
      "train loss:0.02463798444496427\n",
      "train loss:0.02858202081684129\n",
      "train loss:0.01845944122699024\n",
      "train loss:0.03447478695875616\n",
      "train loss:0.026126247683280715\n",
      "train loss:0.015316959359776306\n",
      "train loss:0.007921578195342727\n",
      "train loss:0.01800590120019652\n",
      "train loss:0.014901721674107986\n",
      "train loss:0.12263942969112696\n",
      "train loss:0.016118887526064524\n",
      "train loss:0.00914091476858208\n",
      "train loss:0.012886593810515785\n",
      "train loss:0.03598966693845517\n",
      "train loss:0.025548312857520638\n",
      "train loss:0.02514184990602923\n",
      "train loss:0.025219816110519676\n",
      "train loss:0.03377897817344992\n",
      "train loss:0.017260398018438577\n",
      "train loss:0.01937889072333817\n",
      "train loss:0.012493778319789985\n",
      "train loss:0.03400228530527509\n",
      "train loss:0.031721224631752795\n",
      "train loss:0.015973133352931875\n",
      "train loss:0.012443680570218933\n",
      "train loss:0.05392367641482737\n",
      "train loss:0.05758442917611309\n",
      "train loss:0.02016129534047589\n",
      "train loss:0.02658309409361258\n",
      "train loss:0.039346415143281466\n",
      "train loss:0.02517331075309511\n",
      "train loss:0.06671646362008658\n",
      "train loss:0.029223227502243074\n",
      "train loss:0.02610552074480784\n",
      "train loss:0.033823071610015465\n",
      "train loss:0.01898473890634434\n",
      "train loss:0.014900611965751123\n",
      "train loss:0.03478938181136232\n",
      "train loss:0.02219335174651949\n",
      "train loss:0.01203850082585835\n",
      "train loss:0.019749985946532322\n",
      "train loss:0.026745647719058678\n",
      "train loss:0.04090723221847214\n",
      "train loss:0.07240911898526506\n",
      "train loss:0.04106066469316891\n",
      "train loss:0.049153435255063604\n",
      "train loss:0.009616454047080501\n",
      "train loss:0.03318546754122143\n",
      "train loss:0.008042791128177913\n",
      "train loss:0.02377923437228562\n",
      "train loss:0.021181334852115326\n",
      "train loss:0.021575874776629686\n",
      "train loss:0.17974693524618793\n",
      "train loss:0.02427394689448783\n",
      "train loss:0.008283432929818457\n",
      "train loss:0.01223545757504359\n",
      "train loss:0.033516133037754615\n",
      "train loss:0.0138853120937723\n",
      "train loss:0.017825052401876915\n",
      "train loss:0.02262683605591836\n",
      "train loss:0.01702676076716579\n",
      "train loss:0.036999280362011305\n",
      "train loss:0.012827392988500583\n",
      "train loss:0.10901995297774919\n",
      "train loss:0.05004168739225557\n",
      "train loss:0.024108862011352445\n",
      "train loss:0.03273435452580279\n",
      "train loss:0.021032982833489377\n",
      "train loss:0.030346078539929086\n",
      "train loss:0.012628705252172459\n",
      "train loss:0.015890081237279884\n",
      "train loss:0.02183654217970211\n",
      "train loss:0.0234486385345278\n",
      "train loss:0.04219158118507815\n",
      "train loss:0.01754690748231065\n",
      "train loss:0.06449353476778288\n",
      "train loss:0.015970812350551394\n",
      "train loss:0.021978149993744665\n",
      "train loss:0.016889531931783604\n",
      "train loss:0.06842052913402107\n",
      "train loss:0.014131336806007127\n",
      "train loss:0.026914965886812463\n",
      "train loss:0.07383663464849839\n",
      "train loss:0.010028742267610056\n",
      "train loss:0.017467915470228697\n",
      "train loss:0.022763028223991424\n",
      "train loss:0.05631523314246019\n",
      "train loss:0.053530826566204046\n",
      "train loss:0.018860042652325257\n",
      "train loss:0.012182526375037011\n",
      "train loss:0.025790822963922176\n",
      "train loss:0.033548117127550243\n",
      "train loss:0.03975733392710656\n",
      "train loss:0.04285945627677159\n",
      "train loss:0.04340768862030509\n",
      "train loss:0.031693238484252616\n",
      "train loss:0.02142403235068881\n",
      "train loss:0.027816085632610014\n",
      "train loss:0.0122373364787579\n",
      "train loss:0.022495125567398425\n",
      "train loss:0.027548169556363772\n",
      "train loss:0.023418111088581203\n",
      "train loss:0.05858432532764227\n",
      "train loss:0.022437084859886117\n",
      "train loss:0.007280887396157116\n",
      "train loss:0.007250316428645335\n",
      "train loss:0.017782695438804633\n",
      "train loss:0.025246069404933166\n",
      "train loss:0.017114114389852414\n",
      "train loss:0.04944949419334893\n",
      "train loss:0.027244994649030244\n",
      "train loss:0.01668297240980008\n",
      "train loss:0.0247830813035081\n",
      "train loss:0.042197680712353344\n",
      "train loss:0.024520447345224246\n",
      "train loss:0.029328376387955175\n",
      "=== epoch:9, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.03247705402770748\n",
      "train loss:0.02325572945881273\n",
      "train loss:0.03579301018881549\n",
      "train loss:0.024512419690770653\n",
      "train loss:0.021679089342745747\n",
      "train loss:0.019055898345435994\n",
      "train loss:0.0619745657191791\n",
      "train loss:0.021181523877014635\n",
      "train loss:0.013682126115957965\n",
      "train loss:0.0127173923369451\n",
      "train loss:0.02513477051734009\n",
      "train loss:0.01423989867860677\n",
      "train loss:0.03008126307917485\n",
      "train loss:0.029551595044705933\n",
      "train loss:0.010298592827378186\n",
      "train loss:0.021224104400162074\n",
      "train loss:0.014145599883183226\n",
      "train loss:0.005213379522991913\n",
      "train loss:0.05255169591242389\n",
      "train loss:0.08935305790982649\n",
      "train loss:0.009198942907057039\n",
      "train loss:0.025363529766887365\n",
      "train loss:0.010594640016344286\n",
      "train loss:0.027001556206159513\n",
      "train loss:0.052296998293400454\n",
      "train loss:0.015983287899570806\n",
      "train loss:0.015801198056359694\n",
      "train loss:0.0273022141181241\n",
      "train loss:0.006122069384761596\n",
      "train loss:0.018872817846955416\n",
      "train loss:0.03172381278113195\n",
      "train loss:0.026065881081848756\n",
      "train loss:0.008480601728963031\n",
      "train loss:0.028371276849288262\n",
      "train loss:0.029865915170382843\n",
      "train loss:0.027313573457590338\n",
      "train loss:0.014879167679990717\n",
      "train loss:0.03575559693744001\n",
      "train loss:0.059580599662010354\n",
      "train loss:0.02711398014188093\n",
      "train loss:0.013299319013190203\n",
      "train loss:0.01700337912262858\n",
      "train loss:0.03323166325413966\n",
      "train loss:0.02105313564060962\n",
      "train loss:0.031917287510867955\n",
      "train loss:0.03254487637579607\n",
      "train loss:0.021672968471730735\n",
      "train loss:0.01621060536025799\n",
      "train loss:0.014499645852683906\n",
      "train loss:0.009749736346839767\n",
      "train loss:0.012314282735372334\n",
      "train loss:0.02340611196138176\n",
      "train loss:0.03988161056449125\n",
      "train loss:0.08302705925112519\n",
      "train loss:0.027734283536777444\n",
      "train loss:0.018068550668718687\n",
      "train loss:0.025712547347618506\n",
      "train loss:0.010807217290398227\n",
      "train loss:0.05460061782083888\n",
      "train loss:0.07184046874758217\n",
      "train loss:0.01749087319310628\n",
      "train loss:0.016993510225621632\n",
      "train loss:0.022510359228815537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.034611672295710154\n",
      "train loss:0.014698809941926204\n",
      "train loss:0.011373798129888342\n",
      "train loss:0.0261679711732749\n",
      "train loss:0.0903025968191743\n",
      "train loss:0.035913498011195796\n",
      "train loss:0.015075151877016791\n",
      "train loss:0.06496621907751257\n",
      "train loss:0.08543574417780322\n",
      "train loss:0.022242467806457288\n",
      "train loss:0.0228033130633381\n",
      "train loss:0.011592512715280747\n",
      "train loss:0.019433110099027002\n",
      "train loss:0.03824313340638913\n",
      "train loss:0.010967989862710843\n",
      "train loss:0.017869601812437748\n",
      "train loss:0.06956086500081585\n",
      "train loss:0.023508257680726157\n",
      "train loss:0.03359055555325832\n",
      "train loss:0.01269721895001706\n",
      "train loss:0.019798324898386475\n",
      "train loss:0.028225763375909298\n",
      "train loss:0.01051073523242206\n",
      "train loss:0.018299407786837883\n",
      "train loss:0.019888288300085103\n",
      "train loss:0.012556057089409281\n",
      "train loss:0.02717374126485441\n",
      "train loss:0.06225637875923448\n",
      "train loss:0.06176298425201146\n",
      "train loss:0.02267042049577138\n",
      "train loss:0.021891557179136326\n",
      "train loss:0.018383400644475044\n",
      "train loss:0.009403500059550008\n",
      "train loss:0.013782060400491555\n",
      "train loss:0.01169132159090296\n",
      "train loss:0.015640620957122087\n",
      "train loss:0.025908383748764773\n",
      "train loss:0.013108267486959577\n",
      "train loss:0.012263820089991934\n",
      "train loss:0.027132792917847044\n",
      "train loss:0.025179538304922457\n",
      "train loss:0.055366396537327583\n",
      "train loss:0.02597096745324524\n",
      "train loss:0.02879402786777674\n",
      "train loss:0.013750070673939319\n",
      "train loss:0.007752522535486322\n",
      "train loss:0.027771782168158246\n",
      "train loss:0.0099797929466461\n",
      "train loss:0.020519729245112715\n",
      "train loss:0.073099961143534\n",
      "train loss:0.026307185196781043\n",
      "train loss:0.03371649652829265\n",
      "train loss:0.008150530731673024\n",
      "train loss:0.19474442588333105\n",
      "train loss:0.053932067069762696\n",
      "train loss:0.014746962036422603\n",
      "train loss:0.093333506367365\n",
      "train loss:0.01883958859316365\n",
      "train loss:0.008945290952805487\n",
      "train loss:0.04032679733034136\n",
      "train loss:0.05367431585753762\n",
      "train loss:0.03461946609440724\n",
      "train loss:0.021558388307440005\n",
      "train loss:0.07918018548513114\n",
      "train loss:0.04063394007618427\n",
      "train loss:0.016485039169725667\n",
      "train loss:0.038188071271777285\n",
      "train loss:0.01240364933179545\n",
      "train loss:0.014222618680266377\n",
      "train loss:0.016082440296989714\n",
      "train loss:0.03192580112996777\n",
      "train loss:0.012985591272516068\n",
      "train loss:0.0417884908648525\n",
      "train loss:0.01623258160969478\n",
      "train loss:0.03026463501080659\n",
      "train loss:0.01760604245592389\n",
      "train loss:0.01944584013580237\n",
      "train loss:0.018566796373344973\n",
      "train loss:0.022933344541346483\n",
      "train loss:0.02094241992070263\n",
      "train loss:0.047093840889342244\n",
      "train loss:0.021398686950766726\n",
      "train loss:0.028607647301326918\n",
      "train loss:0.013976464041177375\n",
      "train loss:0.02416990243355579\n",
      "train loss:0.012868427292211954\n",
      "train loss:0.009063615624904944\n",
      "train loss:0.05016198908828582\n",
      "train loss:0.16224947658946323\n",
      "train loss:0.02420351669455646\n",
      "train loss:0.011993792737812986\n",
      "train loss:0.05324987931527311\n",
      "train loss:0.05057773857293975\n",
      "train loss:0.01237463281929352\n",
      "train loss:0.021271706667637683\n",
      "train loss:0.06837217435587027\n",
      "train loss:0.05450094367087483\n",
      "train loss:0.017772113958756205\n",
      "train loss:0.014773388035564076\n",
      "train loss:0.045410358108862126\n",
      "train loss:0.06252983454018592\n",
      "train loss:0.018981624889345364\n",
      "train loss:0.032319906726503395\n",
      "train loss:0.01824307622312373\n",
      "train loss:0.028934491198892208\n",
      "train loss:0.011515554837810643\n",
      "train loss:0.01808853326559296\n",
      "train loss:0.025520839320847963\n",
      "train loss:0.018481762417086378\n",
      "train loss:0.026476465880019326\n",
      "train loss:0.004530711411627684\n",
      "train loss:0.020596140837679515\n",
      "train loss:0.029491185855464074\n",
      "train loss:0.07420963453637816\n",
      "train loss:0.011852949412891174\n",
      "train loss:0.03249288558402627\n",
      "train loss:0.01469416490400304\n",
      "train loss:0.10107144924917164\n",
      "train loss:0.03388826109592906\n",
      "train loss:0.04369936549261695\n",
      "train loss:0.018516118673873868\n",
      "train loss:0.016459895158605447\n",
      "train loss:0.014885422304439228\n",
      "train loss:0.010114410798359867\n",
      "train loss:0.017498667455210448\n",
      "train loss:0.056040012467765674\n",
      "train loss:0.04578788134923987\n",
      "train loss:0.014409710354964172\n",
      "train loss:0.020609830257795142\n",
      "train loss:0.037833886971570106\n",
      "train loss:0.027827135405489044\n",
      "train loss:0.06446121246939363\n",
      "train loss:0.013330040771591752\n",
      "train loss:0.015709451001810114\n",
      "train loss:0.015914778705146057\n",
      "train loss:0.015820712397608946\n",
      "train loss:0.008051219872739402\n",
      "train loss:0.029972347162105416\n",
      "train loss:0.08042797820666486\n",
      "train loss:0.10872696145660121\n",
      "train loss:0.02845275530096127\n",
      "train loss:0.04416515589580986\n",
      "train loss:0.03555837374256622\n",
      "train loss:0.024228149729471217\n",
      "train loss:0.029383134155136416\n",
      "train loss:0.016730536089552845\n",
      "train loss:0.019005366519002143\n",
      "train loss:0.013703368743635107\n",
      "train loss:0.014435519567289702\n",
      "train loss:0.11731493271934453\n",
      "train loss:0.030244946321426807\n",
      "train loss:0.10535725298904727\n",
      "train loss:0.02802582559408156\n",
      "train loss:0.019490282600928433\n",
      "train loss:0.04092145556077401\n",
      "train loss:0.029462170210614404\n",
      "train loss:0.046960747661059295\n",
      "train loss:0.016997943036977402\n",
      "train loss:0.03388796531111129\n",
      "train loss:0.012983348538433184\n",
      "train loss:0.015565827035350194\n",
      "train loss:0.006006493391825237\n",
      "train loss:0.016921527557601266\n",
      "train loss:0.021943735853401855\n",
      "train loss:0.06862937833492377\n",
      "train loss:0.020189609384623358\n",
      "train loss:0.016779592188864163\n",
      "train loss:0.020588737503697722\n",
      "train loss:0.018229555857420067\n",
      "train loss:0.06844961136554609\n",
      "train loss:0.06617750290467611\n",
      "train loss:0.01210928845957593\n",
      "train loss:0.024863147356873984\n",
      "train loss:0.023580217516443142\n",
      "train loss:0.042398131695310405\n",
      "train loss:0.025302289741296672\n",
      "train loss:0.009412618503803451\n",
      "train loss:0.008980794894149437\n",
      "train loss:0.009697616934090976\n",
      "train loss:0.031487161531116276\n",
      "train loss:0.006422540245314789\n",
      "train loss:0.011478431356744407\n",
      "train loss:0.03443014356781791\n",
      "train loss:0.012637377769723384\n",
      "train loss:0.016520157810772533\n",
      "train loss:0.02467850396303585\n",
      "train loss:0.029161691389737544\n",
      "train loss:0.019991281586187203\n",
      "train loss:0.031952465161896766\n",
      "train loss:0.01715541442677235\n",
      "train loss:0.06585899748189047\n",
      "train loss:0.01255321412375308\n",
      "train loss:0.009264313252601348\n",
      "train loss:0.00970348529500753\n",
      "train loss:0.011533281477803393\n",
      "train loss:0.023748420532643637\n",
      "train loss:0.02214802190326634\n",
      "train loss:0.03323839702518503\n",
      "train loss:0.09384943367170523\n",
      "train loss:0.03220421642614543\n",
      "train loss:0.06887116747125653\n",
      "train loss:0.021006161311363715\n",
      "train loss:0.05809656027331725\n",
      "train loss:0.03661529472583876\n",
      "train loss:0.018385393456346755\n",
      "train loss:0.04086420735663521\n",
      "train loss:0.02618568415254647\n",
      "train loss:0.012231820744868824\n",
      "train loss:0.01616766449266429\n",
      "train loss:0.08980605429950314\n",
      "train loss:0.015246302396748804\n",
      "train loss:0.032720027624211605\n",
      "train loss:0.02988730611547774\n",
      "train loss:0.026666564367981097\n",
      "train loss:0.06778293512766367\n",
      "train loss:0.025906198815455386\n",
      "train loss:0.04494384869937242\n",
      "train loss:0.04327074977896758\n",
      "train loss:0.039231558511791886\n",
      "train loss:0.025344274519820634\n",
      "train loss:0.05871243086277872\n",
      "train loss:0.011293643398679607\n",
      "train loss:0.009287758777314376\n",
      "train loss:0.03165179274326376\n",
      "train loss:0.02918072905369844\n",
      "train loss:0.02262277924534127\n",
      "train loss:0.024513390750705154\n",
      "train loss:0.083201037285613\n",
      "train loss:0.010579278031033465\n",
      "train loss:0.020070852181706584\n",
      "train loss:0.030500929820285586\n",
      "train loss:0.022297444143765263\n",
      "train loss:0.01422898166221851\n",
      "train loss:0.02338419227646139\n",
      "train loss:0.03242891752921071\n",
      "train loss:0.018269182020139586\n",
      "train loss:0.028848874940749263\n",
      "train loss:0.016681140933918034\n",
      "train loss:0.01619279753307685\n",
      "train loss:0.043586242420719554\n",
      "train loss:0.05696900921173444\n",
      "train loss:0.025710547580045824\n",
      "train loss:0.016799506964850823\n",
      "train loss:0.027153951117483675\n",
      "train loss:0.006481236850834023\n",
      "train loss:0.013620815099591075\n",
      "train loss:0.03814437197395842\n",
      "train loss:0.017342206958454818\n",
      "train loss:0.02145455250673255\n",
      "train loss:0.0213263610784861\n",
      "train loss:0.01229054910219709\n",
      "train loss:0.034385676356752444\n",
      "train loss:0.04287360482177558\n",
      "train loss:0.022252442261490212\n",
      "train loss:0.025113575386226616\n",
      "train loss:0.0390445925229485\n",
      "train loss:0.02101956951501857\n",
      "train loss:0.013780962815187883\n",
      "train loss:0.0777964806560762\n",
      "train loss:0.015289540968222555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02942939823945559\n",
      "train loss:0.03586550633993671\n",
      "train loss:0.061688514852849596\n",
      "train loss:0.06756908788995408\n",
      "train loss:0.031119830522901334\n",
      "train loss:0.010832003002325815\n",
      "train loss:0.0205350396880714\n",
      "train loss:0.016403352250627917\n",
      "train loss:0.06773878790406272\n",
      "train loss:0.02603161880275398\n",
      "train loss:0.008170507756385123\n",
      "train loss:0.025790723699025207\n",
      "train loss:0.055885687902952945\n",
      "train loss:0.00796361927397508\n",
      "train loss:0.022006119262915423\n",
      "train loss:0.016309316216484338\n",
      "train loss:0.009934764745944012\n",
      "train loss:0.014841685854473364\n",
      "train loss:0.03068930581956048\n",
      "train loss:0.03945150931246177\n",
      "train loss:0.016608790210821348\n",
      "train loss:0.03857394813830908\n",
      "train loss:0.02420160566486464\n",
      "train loss:0.02107712072353708\n",
      "train loss:0.009918255480920023\n",
      "train loss:0.018203123655067658\n",
      "train loss:0.0415355981211392\n",
      "train loss:0.007208417896573897\n",
      "train loss:0.01433049034224742\n",
      "train loss:0.036494704820589766\n",
      "train loss:0.06876570186059562\n",
      "train loss:0.00995942558371989\n",
      "train loss:0.017646061756844925\n",
      "train loss:0.007793450843182008\n",
      "train loss:0.016622185790880956\n",
      "train loss:0.023135720092520233\n",
      "train loss:0.016784399354245895\n",
      "train loss:0.03326796376666196\n",
      "train loss:0.032233914155323194\n",
      "train loss:0.05384112453195486\n",
      "train loss:0.05020182792875212\n",
      "train loss:0.03293277819658614\n",
      "train loss:0.02309766992869454\n",
      "train loss:0.018542237759482463\n",
      "train loss:0.00510606746752294\n",
      "train loss:0.014083315146480056\n",
      "train loss:0.004683938079140755\n",
      "train loss:0.018923009456199492\n",
      "train loss:0.010497476009303532\n",
      "train loss:0.06432224352201482\n",
      "train loss:0.028285364933374458\n",
      "train loss:0.04336749627605721\n",
      "train loss:0.019017467807840017\n",
      "train loss:0.00896840085915181\n",
      "train loss:0.07086392141363661\n",
      "train loss:0.017521807796575438\n",
      "train loss:0.03012993467809836\n",
      "train loss:0.014660422146164587\n",
      "train loss:0.013133203181746655\n",
      "train loss:0.054969281372327074\n",
      "train loss:0.04625530009532468\n",
      "train loss:0.03460557232735583\n",
      "train loss:0.012209589905742013\n",
      "train loss:0.013000879003835371\n",
      "train loss:0.028918648381490563\n",
      "train loss:0.047348766704055904\n",
      "train loss:0.007671339477473434\n",
      "train loss:0.10780094192688594\n",
      "train loss:0.0405096962769534\n",
      "train loss:0.025759505030969368\n",
      "train loss:0.010939858889662741\n",
      "train loss:0.015410687589461252\n",
      "train loss:0.06199055482948524\n",
      "train loss:0.009036133386311184\n",
      "train loss:0.04243815648601762\n",
      "train loss:0.013988554744407643\n",
      "train loss:0.019141771968162122\n",
      "train loss:0.02128158859434999\n",
      "train loss:0.00706078534790539\n",
      "train loss:0.010662144974140465\n",
      "train loss:0.039248555997449655\n",
      "train loss:0.04259152789475596\n",
      "train loss:0.026737651543124658\n",
      "train loss:0.014392242558095498\n",
      "train loss:0.018583569657920775\n",
      "train loss:0.09477734402698927\n",
      "train loss:0.0511127234912756\n",
      "train loss:0.024647907872207026\n",
      "train loss:0.013231794861885495\n",
      "train loss:0.028878556067149473\n",
      "train loss:0.018793331064034705\n",
      "train loss:0.01249719939249018\n",
      "train loss:0.09412182052305665\n",
      "train loss:0.019686860308249742\n",
      "train loss:0.034819838985031784\n",
      "train loss:0.019739981381319483\n",
      "train loss:0.013929766994068851\n",
      "train loss:0.022696380232466472\n",
      "train loss:0.018667321915858202\n",
      "train loss:0.011230157018497642\n",
      "train loss:0.023774800937244128\n",
      "train loss:0.033987807358282184\n",
      "train loss:0.010616396572071281\n",
      "train loss:0.05981521144773197\n",
      "train loss:0.03349848063222549\n",
      "train loss:0.01649907708308641\n",
      "train loss:0.02391524862456412\n",
      "train loss:0.00676395975299183\n",
      "train loss:0.043977005597498356\n",
      "train loss:0.02040117906843173\n",
      "train loss:0.06226850869699873\n",
      "train loss:0.03656873586474987\n",
      "train loss:0.017164885809684803\n",
      "train loss:0.02808735122497484\n",
      "train loss:0.038718598654644866\n",
      "train loss:0.013710001075275445\n",
      "train loss:0.03329472916272346\n",
      "train loss:0.06367136364570514\n",
      "train loss:0.033948548506007585\n",
      "train loss:0.01889065837685015\n",
      "train loss:0.029875436640974237\n",
      "train loss:0.018465906667434976\n",
      "train loss:0.017736743269873415\n",
      "train loss:0.017927309583541686\n",
      "train loss:0.04580738755518582\n",
      "train loss:0.011721149076734159\n",
      "train loss:0.013838312648827768\n",
      "=== epoch:10, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.03868039993261082\n",
      "train loss:0.01826473923251064\n",
      "train loss:0.0237759823245241\n",
      "train loss:0.013759994776674717\n",
      "train loss:0.012814189632751178\n",
      "train loss:0.033929575910144054\n",
      "train loss:0.022606542713900203\n",
      "train loss:0.015578879885360372\n",
      "train loss:0.03413045404602868\n",
      "train loss:0.013670296573962307\n",
      "train loss:0.02278811662455331\n",
      "train loss:0.01693665473658553\n",
      "train loss:0.017260036111621514\n",
      "train loss:0.03337298479301684\n",
      "train loss:0.024100138637840787\n",
      "train loss:0.011254246531189964\n",
      "train loss:0.01332404122401218\n",
      "train loss:0.01033115339890563\n",
      "train loss:0.0442424651110581\n",
      "train loss:0.02258378659239516\n",
      "train loss:0.009001615485274844\n",
      "train loss:0.04641058317582246\n",
      "train loss:0.02689137116217132\n",
      "train loss:0.0535322457351025\n",
      "train loss:0.024180266909618508\n",
      "train loss:0.051350983456079205\n",
      "train loss:0.018022421254469533\n",
      "train loss:0.01406860815368022\n",
      "train loss:0.06638655982064406\n",
      "train loss:0.03972850879118878\n",
      "train loss:0.046662564385796516\n",
      "train loss:0.02176694768483904\n",
      "train loss:0.028540617932403638\n",
      "train loss:0.02914913654912529\n",
      "train loss:0.019457492320990814\n",
      "train loss:0.006519080071902436\n",
      "train loss:0.0702323331368337\n",
      "train loss:0.014340163305127161\n",
      "train loss:0.015408612618847357\n",
      "train loss:0.07775392027314465\n",
      "train loss:0.03677865206684702\n",
      "train loss:0.030152433515085997\n",
      "train loss:0.013674673877481322\n",
      "train loss:0.0264450556344534\n",
      "train loss:0.03876185317965247\n",
      "train loss:0.01883409635916274\n",
      "train loss:0.023311654789228937\n",
      "train loss:0.029770112349510117\n",
      "train loss:0.015816349050732887\n",
      "train loss:0.01465268972073799\n",
      "train loss:0.010719983631568532\n",
      "train loss:0.017521041845578202\n",
      "train loss:0.020371011282220863\n",
      "train loss:0.017554914094340787\n",
      "train loss:0.03473162959078285\n",
      "train loss:0.0184495184867877\n",
      "train loss:0.0431783892217975\n",
      "train loss:0.03402317709120159\n",
      "train loss:0.038610475229779166\n",
      "train loss:0.02104054954961556\n",
      "train loss:0.018693263702183466\n",
      "train loss:0.036497072219978756\n",
      "train loss:0.030083098750805965\n",
      "train loss:0.023758299443168006\n",
      "train loss:0.013972864985445364\n",
      "train loss:0.024598216296539025\n",
      "train loss:0.017054483510497084\n",
      "train loss:0.01989024568474217\n",
      "train loss:0.03521429533038829\n",
      "train loss:0.039852325261677056\n",
      "train loss:0.025744666059369613\n",
      "train loss:0.01941852761233505\n",
      "train loss:0.020744948460458965\n",
      "train loss:0.015263561754631312\n",
      "train loss:0.01337161683777806\n",
      "train loss:0.03371880262657919\n",
      "train loss:0.05446329751260965\n",
      "train loss:0.011022360585480897\n",
      "train loss:0.008251876694553463\n",
      "train loss:0.027290633092669013\n",
      "train loss:0.029903141849935637\n",
      "train loss:0.01446722311792841\n",
      "train loss:0.01110106476705599\n",
      "train loss:0.04783085561947721\n",
      "train loss:0.023696378165670648\n",
      "train loss:0.021162851880334056\n",
      "train loss:0.03457446780339808\n",
      "train loss:0.03446370500084204\n",
      "train loss:0.03408291186173274\n",
      "train loss:0.03331696299079947\n",
      "train loss:0.04994557216238837\n",
      "train loss:0.03940026062112542\n",
      "train loss:0.019670198959776994\n",
      "train loss:0.0375132290083292\n",
      "train loss:0.013380194412372435\n",
      "train loss:0.04736314391850477\n",
      "train loss:0.03225874626749483\n",
      "train loss:0.018330923659312465\n",
      "train loss:0.01685054285803585\n",
      "train loss:0.011958983198847495\n",
      "train loss:0.012069682863537008\n",
      "train loss:0.013513042221489985\n",
      "train loss:0.018371897196717467\n",
      "train loss:0.019170941053031677\n",
      "train loss:0.026424473863080066\n",
      "train loss:0.011910371672186186\n",
      "train loss:0.035021626013364\n",
      "train loss:0.025796087255345868\n",
      "train loss:0.01773898022933072\n",
      "train loss:0.018675660446343674\n",
      "train loss:0.022090400110963148\n",
      "train loss:0.02505057745430888\n",
      "train loss:0.028304141689472605\n",
      "train loss:0.01605186149686623\n",
      "train loss:0.027476880572919513\n",
      "train loss:0.01629439906650671\n",
      "train loss:0.008994588190035159\n",
      "train loss:0.06491968332438494\n",
      "train loss:0.018044240695248705\n",
      "train loss:0.017001665109826525\n",
      "train loss:0.017500903785288652\n",
      "train loss:0.018978047117303644\n",
      "train loss:0.05059936618273726\n",
      "train loss:0.01394305153736971\n",
      "train loss:0.017376290209857904\n",
      "train loss:0.05700555555954822\n",
      "train loss:0.013156557478593803\n",
      "train loss:0.01124057163850009\n",
      "train loss:0.019592668567889954\n",
      "train loss:0.0195137924554553\n",
      "train loss:0.03696248321036504\n",
      "train loss:0.011141811615003184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.058774392578365385\n",
      "train loss:0.011876456593062188\n",
      "train loss:0.016721150758640908\n",
      "train loss:0.013506073656967923\n",
      "train loss:0.012209605275976919\n",
      "train loss:0.011396153047946106\n",
      "train loss:0.008154778918736323\n",
      "train loss:0.0269126558881316\n",
      "train loss:0.025165818475457548\n",
      "train loss:0.021752925470636467\n",
      "train loss:0.019548067606142375\n",
      "train loss:0.02718033298174724\n",
      "train loss:0.048313250362738556\n",
      "train loss:0.01861902136428786\n",
      "train loss:0.029059381268727442\n",
      "train loss:0.011044826508374456\n",
      "train loss:0.01316029865059841\n",
      "train loss:0.026567930534719418\n",
      "train loss:0.009291903372555307\n",
      "train loss:0.054590311018625756\n",
      "train loss:0.032431680733324815\n",
      "train loss:0.014945063949964527\n",
      "train loss:0.024038732922281723\n",
      "train loss:0.010248705827611605\n",
      "train loss:0.010914797346378019\n",
      "train loss:0.02930265445365982\n",
      "train loss:0.011924765844476225\n",
      "train loss:0.02043879292609747\n",
      "train loss:0.013084614205840284\n",
      "train loss:0.04518630707273694\n",
      "train loss:0.0098367198700435\n",
      "train loss:0.03452570869760177\n",
      "train loss:0.012301375130521977\n",
      "train loss:0.012019536271133145\n",
      "train loss:0.011612870237687192\n",
      "train loss:0.015972462417103154\n",
      "train loss:0.01728503326451164\n",
      "train loss:0.027408201430564723\n",
      "train loss:0.017066387984120135\n",
      "train loss:0.012719891241795438\n",
      "train loss:0.022289386362462506\n",
      "train loss:0.031774660806767704\n",
      "train loss:0.01112149644561046\n",
      "train loss:0.021499948044840914\n",
      "train loss:0.009671186005416256\n",
      "train loss:0.012148827454215447\n",
      "train loss:0.02711901455371998\n",
      "train loss:0.02963824059864955\n",
      "train loss:0.016729187002098987\n",
      "train loss:0.035557481537604116\n",
      "train loss:0.015744652655495323\n",
      "train loss:0.021282577153948927\n",
      "train loss:0.09062472157033045\n",
      "train loss:0.004555629590904304\n",
      "train loss:0.020944148298796605\n",
      "train loss:0.01578938170105279\n",
      "train loss:0.014689696982394503\n",
      "train loss:0.056411386876076636\n",
      "train loss:0.021092174749911746\n",
      "train loss:0.010724461801238145\n",
      "train loss:0.008398018223370423\n",
      "train loss:0.009946284117880651\n",
      "train loss:0.039581891401293284\n",
      "train loss:0.006859506552453841\n",
      "train loss:0.025333544997818196\n",
      "train loss:0.04175721086736432\n",
      "train loss:0.021290163757369723\n",
      "train loss:0.024248044035355316\n",
      "train loss:0.017078631409551997\n",
      "train loss:0.013095592587552782\n",
      "train loss:0.027461012959446175\n",
      "train loss:0.027049910881893582\n",
      "train loss:0.031382618752304955\n",
      "train loss:0.028527241465552692\n",
      "train loss:0.02002470110005259\n",
      "train loss:0.06764425143081441\n",
      "train loss:0.012423106562489003\n",
      "train loss:0.015774226603088495\n",
      "train loss:0.024738870568209767\n",
      "train loss:0.042523695940350235\n",
      "train loss:0.029386360595591233\n",
      "train loss:0.013745428231964195\n",
      "train loss:0.03715267703201885\n",
      "train loss:0.013545362679250123\n",
      "train loss:0.014651862249110359\n",
      "train loss:0.01021197818903555\n",
      "train loss:0.022347662188566784\n",
      "train loss:0.016739495441097506\n",
      "train loss:0.024741945750465323\n",
      "train loss:0.011428936786236741\n",
      "train loss:0.01308403021502738\n",
      "train loss:0.04154397565109335\n",
      "train loss:0.005501706099500806\n",
      "train loss:0.012442191493349295\n",
      "train loss:0.02479267475765099\n",
      "train loss:0.033306878936076284\n",
      "train loss:0.02857174463348087\n",
      "train loss:0.02361819368149639\n",
      "train loss:0.07782171759614424\n",
      "train loss:0.03465529146617335\n",
      "train loss:0.0334664371566859\n",
      "train loss:0.013403979103013053\n",
      "train loss:0.019484777339551115\n",
      "train loss:0.013636183929951579\n",
      "train loss:0.009354574581057162\n",
      "train loss:0.017223797333739573\n",
      "train loss:0.04821593541227045\n",
      "train loss:0.025713833371015102\n",
      "train loss:0.0080264448971138\n",
      "train loss:0.014156829678589878\n",
      "train loss:0.023031531316516644\n",
      "train loss:0.03720776979668749\n",
      "train loss:0.04995372715046936\n",
      "train loss:0.03560024418797268\n",
      "train loss:0.020555873026148447\n",
      "train loss:0.02608513685193436\n",
      "train loss:0.04090769806472868\n",
      "train loss:0.00992258809037056\n",
      "train loss:0.020009359276580957\n",
      "train loss:0.007856140245146604\n",
      "train loss:0.05644241488525917\n",
      "train loss:0.04593113896548623\n",
      "train loss:0.011579366965361357\n",
      "train loss:0.01793111955979607\n",
      "train loss:0.04054131091843783\n",
      "train loss:0.014558410404021359\n",
      "train loss:0.014532014297136985\n",
      "train loss:0.019746604966053596\n",
      "train loss:0.03828698647683619\n",
      "train loss:0.03763748942056461\n",
      "train loss:0.009374481602446189\n",
      "train loss:0.006729140676999249\n",
      "train loss:0.08840091909155089\n",
      "train loss:0.015048021624249855\n",
      "train loss:0.018347471579640097\n",
      "train loss:0.049572171565375835\n",
      "train loss:0.017412886312665937\n",
      "train loss:0.015200594154110382\n",
      "train loss:0.02202544564542069\n",
      "train loss:0.02533898806546667\n",
      "train loss:0.017573716980664414\n",
      "train loss:0.008737370013183372\n",
      "train loss:0.0506658131367516\n",
      "train loss:0.010993267044901217\n",
      "train loss:0.007301306906634327\n",
      "train loss:0.022114586245416637\n",
      "train loss:0.014668871328706583\n",
      "train loss:0.02725801051111153\n",
      "train loss:0.04354905089609537\n",
      "train loss:0.026114731481676184\n",
      "train loss:0.026504987806892134\n",
      "train loss:0.036096996188002974\n",
      "train loss:0.045685305552100194\n",
      "train loss:0.021246889575292603\n",
      "train loss:0.02193401771946755\n",
      "train loss:0.015052990535522576\n",
      "train loss:0.034963795871958345\n",
      "train loss:0.015029269931954949\n",
      "train loss:0.010215502361299622\n",
      "train loss:0.019544765169154914\n",
      "train loss:0.03593737728280257\n",
      "train loss:0.04621315623972908\n",
      "train loss:0.014768919056105296\n",
      "train loss:0.010860317162084436\n",
      "train loss:0.022431201520934137\n",
      "train loss:0.03317366976820426\n",
      "train loss:0.006504315049988825\n",
      "train loss:0.01104605841246862\n",
      "train loss:0.013983685752228936\n",
      "train loss:0.04274302957284788\n",
      "train loss:0.022543987080354818\n",
      "train loss:0.05225634201776553\n",
      "train loss:0.06915547581320425\n",
      "train loss:0.016741001325279555\n",
      "train loss:0.027661389357995395\n",
      "train loss:0.018697444922904177\n",
      "train loss:0.045610057540693055\n",
      "train loss:0.024698526889041443\n",
      "train loss:0.01233278168974316\n",
      "train loss:0.011566032278371979\n",
      "train loss:0.01141845864623802\n",
      "train loss:0.012704488056024182\n",
      "train loss:0.029032405716410716\n",
      "train loss:0.007369498966405195\n",
      "train loss:0.010975650577603113\n",
      "train loss:0.04575719983030315\n",
      "train loss:0.03728547648063958\n",
      "train loss:0.018566916229641194\n",
      "train loss:0.017354173597592913\n",
      "train loss:0.020015354138413183\n",
      "train loss:0.008719010598955608\n",
      "train loss:0.02483565503837414\n",
      "train loss:0.030548952888969548\n",
      "train loss:0.016012164244284314\n",
      "train loss:0.006875359713111067\n",
      "train loss:0.01838152848797786\n",
      "train loss:0.00985476729490112\n",
      "train loss:0.016676266988636446\n",
      "train loss:0.007421517382034429\n",
      "train loss:0.10776076213115149\n",
      "train loss:0.09767189868031002\n",
      "train loss:0.015023909355257204\n",
      "train loss:0.09122391407562828\n",
      "train loss:0.05462076833346344\n",
      "train loss:0.015576038289300017\n",
      "train loss:0.006771182020024549\n",
      "train loss:0.010991208918515162\n",
      "train loss:0.013995693746670115\n",
      "train loss:0.02255126612803536\n",
      "train loss:0.03943344799785899\n",
      "train loss:0.0161193588067864\n",
      "train loss:0.033694658382109645\n",
      "train loss:0.029493131786235617\n",
      "train loss:0.008451643555294994\n",
      "train loss:0.033205278855791595\n",
      "train loss:0.028794281298989685\n",
      "train loss:0.020499610731770028\n",
      "train loss:0.011285595247108683\n",
      "train loss:0.009789375901937519\n",
      "train loss:0.0073300300462645586\n",
      "train loss:0.021360952362054517\n",
      "train loss:0.022466579706128448\n",
      "train loss:0.012696467666378795\n",
      "train loss:0.01109830427123548\n",
      "train loss:0.09356582934975334\n",
      "train loss:0.022240158576435534\n",
      "train loss:0.011770302480610293\n",
      "train loss:0.011399906185162867\n",
      "train loss:0.04311610603866591\n",
      "train loss:0.008000306768910628\n",
      "train loss:0.0340684241923126\n",
      "train loss:0.007777761679038373\n",
      "train loss:0.007918012297347222\n",
      "train loss:0.02304227380899161\n",
      "train loss:0.010465189538199552\n",
      "train loss:0.02182077543453493\n",
      "train loss:0.0186263680019358\n",
      "train loss:0.009785673438886023\n",
      "train loss:0.05057735906613121\n",
      "train loss:0.02966629530105588\n",
      "train loss:0.015678923323895085\n",
      "train loss:0.028922102353588098\n",
      "train loss:0.008277358247436452\n",
      "train loss:0.023362414190149918\n",
      "train loss:0.01849401511975501\n",
      "train loss:0.016187308136317517\n",
      "train loss:0.019756855169253806\n",
      "train loss:0.016893706382250163\n",
      "train loss:0.027593070904681896\n",
      "train loss:0.032934626391510916\n",
      "train loss:0.017595151905466528\n",
      "train loss:0.029241372798385585\n",
      "train loss:0.011819460665652506\n",
      "train loss:0.020104495316102457\n",
      "train loss:0.05737729783733661\n",
      "train loss:0.006978096941277604\n",
      "train loss:0.030226356410210205\n",
      "train loss:0.0318534902683294\n",
      "train loss:0.007493715003763324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.007009746517692272\n",
      "train loss:0.03570717036311432\n",
      "train loss:0.023983820450953725\n",
      "train loss:0.010336568029186725\n",
      "train loss:0.008891739314754923\n",
      "train loss:0.021681367641449113\n",
      "train loss:0.049429121930686055\n",
      "train loss:0.01676289064617681\n",
      "train loss:0.01464487061400567\n",
      "train loss:0.010398990144338361\n",
      "train loss:0.05390273166880527\n",
      "train loss:0.012646528786145125\n",
      "train loss:0.03201455595266063\n",
      "train loss:0.01877304588785916\n",
      "train loss:0.06233563656303749\n",
      "train loss:0.009597047259160144\n",
      "train loss:0.009791329159012725\n",
      "train loss:0.014486211944803828\n",
      "train loss:0.014499779158172646\n",
      "train loss:0.01746785520609018\n",
      "train loss:0.08379422514446543\n",
      "train loss:0.008804075135934545\n",
      "train loss:0.009669969793260774\n",
      "train loss:0.010029684963779914\n",
      "train loss:0.007017627824430807\n",
      "train loss:0.017676207309246003\n",
      "train loss:0.016061588993486312\n",
      "train loss:0.030493204431420336\n",
      "train loss:0.06310668161007535\n",
      "train loss:0.04730753589892033\n",
      "train loss:0.007374928655855213\n",
      "train loss:0.01142483745101576\n",
      "train loss:0.03876512956306102\n",
      "train loss:0.007962436437029619\n",
      "train loss:0.05881476055675818\n",
      "train loss:0.056883538656381134\n",
      "train loss:0.030750772928044432\n",
      "train loss:0.04006053852178784\n",
      "train loss:0.07202099695677983\n",
      "train loss:0.0212524791507956\n",
      "train loss:0.07480083170635321\n",
      "train loss:0.07856616079757794\n",
      "train loss:0.015217658372643357\n",
      "train loss:0.04495392125165283\n",
      "train loss:0.020181581592867218\n",
      "train loss:0.022003791295880416\n",
      "train loss:0.022309392850032216\n",
      "train loss:0.05281958498938465\n",
      "train loss:0.03620657287170498\n",
      "train loss:0.02113793602153055\n",
      "train loss:0.013233220033222708\n",
      "train loss:0.030972302644987272\n",
      "train loss:0.012204525258123681\n",
      "train loss:0.03781317061326278\n",
      "train loss:0.011584466448767674\n",
      "train loss:0.018028798832009963\n",
      "train loss:0.017211794293861295\n",
      "train loss:0.06861360314086612\n",
      "train loss:0.012672432177940037\n",
      "=== epoch:11, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.01587053551936547\n",
      "train loss:0.016515355953483704\n",
      "train loss:0.02736104677211997\n",
      "train loss:0.07995750758165014\n",
      "train loss:0.0067362375171642775\n",
      "train loss:0.036715046397122364\n",
      "train loss:0.028132101971776763\n",
      "train loss:0.003514469040633112\n",
      "train loss:0.010391606485968872\n",
      "train loss:0.011018196883758786\n",
      "train loss:0.015904769338247058\n",
      "train loss:0.01174361552515401\n",
      "train loss:0.015951350788995556\n",
      "train loss:0.03891366930168678\n",
      "train loss:0.016142473174534436\n",
      "train loss:0.016816995499726185\n",
      "train loss:0.028758902081931565\n",
      "train loss:0.013203797802650012\n",
      "train loss:0.00758403880291917\n",
      "train loss:0.016764252776985053\n",
      "train loss:0.019340278622455763\n",
      "train loss:0.0440898824643001\n",
      "train loss:0.021901218785724062\n",
      "train loss:0.005747625130220776\n",
      "train loss:0.014310890018300331\n",
      "train loss:0.01592185512058836\n",
      "train loss:0.024560618162426048\n",
      "train loss:0.0662282210293053\n",
      "train loss:0.04864365267399759\n",
      "train loss:0.012557515767819289\n",
      "train loss:0.006168990039912859\n",
      "train loss:0.035762380108876037\n",
      "train loss:0.034934607825750676\n",
      "train loss:0.023041258255511024\n",
      "train loss:0.006107205846928622\n",
      "train loss:0.03007185569800389\n",
      "train loss:0.00780775455737228\n",
      "train loss:0.06793316332632365\n",
      "train loss:0.005881049075063581\n",
      "train loss:0.017072669432405926\n",
      "train loss:0.02520146986922378\n",
      "train loss:0.013266076366819953\n",
      "train loss:0.008942943335985718\n",
      "train loss:0.10630388015162769\n",
      "train loss:0.05735925109743121\n",
      "train loss:0.051297495193924056\n",
      "train loss:0.011697134146234247\n",
      "train loss:0.03989748400221926\n",
      "train loss:0.012785721827211749\n",
      "train loss:0.005958823067668346\n",
      "train loss:0.03481163177704579\n",
      "train loss:0.030728248414847253\n",
      "train loss:0.008962867284061626\n",
      "train loss:0.03602724221399712\n",
      "train loss:0.011377844370525367\n",
      "train loss:0.04657151809151233\n",
      "train loss:0.021989063210322964\n",
      "train loss:0.03360414395378961\n",
      "train loss:0.022570794222249616\n",
      "train loss:0.03311947922645049\n",
      "train loss:0.017532520240516356\n",
      "train loss:0.022401923602875438\n",
      "train loss:0.020565695166010926\n",
      "train loss:0.05185547570643506\n",
      "train loss:0.008596819805344387\n",
      "train loss:0.03259663430395928\n",
      "train loss:0.035919009897759556\n",
      "train loss:0.05181193583886953\n",
      "train loss:0.060497143026359786\n",
      "train loss:0.011037122887240321\n",
      "train loss:0.009894049055515424\n",
      "train loss:0.008229534764549315\n",
      "train loss:0.01986397465071316\n",
      "train loss:0.014380577687197048\n",
      "train loss:0.053062688649054286\n",
      "train loss:0.01627211677382208\n",
      "train loss:0.027759043531709562\n",
      "train loss:0.048700354262422835\n",
      "train loss:0.007443793668480227\n",
      "train loss:0.05605014375338748\n",
      "train loss:0.016578610419664106\n",
      "train loss:0.01847028501238556\n",
      "train loss:0.01160696431644634\n",
      "train loss:0.011256764228641431\n",
      "train loss:0.014459118290612517\n",
      "train loss:0.011241088485647565\n",
      "train loss:0.03536577900741408\n",
      "train loss:0.0494778885986538\n",
      "train loss:0.03450933749356719\n",
      "train loss:0.025633075876743412\n",
      "train loss:0.059526861869988235\n",
      "train loss:0.020599113422754032\n",
      "train loss:0.03457414661588375\n",
      "train loss:0.017423696372960137\n",
      "train loss:0.013080111782735753\n",
      "train loss:0.041167917078992866\n",
      "train loss:0.014908437492592566\n",
      "train loss:0.018948661609799765\n",
      "train loss:0.06124200001745393\n",
      "train loss:0.01592314996219919\n",
      "train loss:0.011359447887453128\n",
      "train loss:0.025978766977958232\n",
      "train loss:0.02495762662805309\n",
      "train loss:0.029769388151557042\n",
      "train loss:0.03148737851472869\n",
      "train loss:0.020536264458670173\n",
      "train loss:0.02513583334484842\n",
      "train loss:0.042822321463887425\n",
      "train loss:0.11833097561761202\n",
      "train loss:0.01239731508425851\n",
      "train loss:0.012352829619137161\n",
      "train loss:0.02324031539684869\n",
      "train loss:0.03633991933790218\n",
      "train loss:0.009764884817421868\n",
      "train loss:0.020611653579873424\n",
      "train loss:0.01710395051795173\n",
      "train loss:0.012605807137395118\n",
      "train loss:0.013605334310572391\n",
      "train loss:0.020227364057191114\n",
      "train loss:0.015437507836752906\n",
      "train loss:0.06090297114401928\n",
      "train loss:0.019637089443313195\n",
      "train loss:0.021039971280886857\n",
      "train loss:0.013215511887871047\n",
      "train loss:0.013276590583062343\n",
      "train loss:0.021700870785601508\n",
      "train loss:0.012382757533540366\n",
      "train loss:0.057222556402282757\n",
      "train loss:0.029316445908935986\n",
      "train loss:0.03783437546120756\n",
      "train loss:0.019830565027395868\n",
      "train loss:0.00733911129902517\n",
      "train loss:0.03453744487014407\n",
      "train loss:0.019005343984027718\n",
      "train loss:0.06649768140389786\n",
      "train loss:0.018681747654694564\n",
      "train loss:0.02531594831951091\n",
      "train loss:0.008029801227678445\n",
      "train loss:0.009802394584756142\n",
      "train loss:0.023726702868331576\n",
      "train loss:0.01676286154687695\n",
      "train loss:0.01968471500825389\n",
      "train loss:0.02293376967216094\n",
      "train loss:0.023074114180816126\n",
      "train loss:0.0067530018525014225\n",
      "train loss:0.008857223198109028\n",
      "train loss:0.03303237525806073\n",
      "train loss:0.04517822690142904\n",
      "train loss:0.019442772496926414\n",
      "train loss:0.014694276674815973\n",
      "train loss:0.02191666899856755\n",
      "train loss:0.01597556996523657\n",
      "train loss:0.031181936001327536\n",
      "train loss:0.015471723519357581\n",
      "train loss:0.03393612474694982\n",
      "train loss:0.03491964476557808\n",
      "train loss:0.006219412940741111\n",
      "train loss:0.06300497673804056\n",
      "train loss:0.030576614341565084\n",
      "train loss:0.01730287571873328\n",
      "train loss:0.01465844479681696\n",
      "train loss:0.004264834000934737\n",
      "train loss:0.016564563710888557\n",
      "train loss:0.013659414848522691\n",
      "train loss:0.028831694078190015\n",
      "train loss:0.0321071527009442\n",
      "train loss:0.029463949007461517\n",
      "train loss:0.009415819109482812\n",
      "train loss:0.0541822004451673\n",
      "train loss:0.06966478563700255\n",
      "train loss:0.015231271540941121\n",
      "train loss:0.03863945565032552\n",
      "train loss:0.039357041293761025\n",
      "train loss:0.010918752992988888\n",
      "train loss:0.025004143787308995\n",
      "train loss:0.025963742696118137\n",
      "train loss:0.018166209840215983\n",
      "train loss:0.06926180550967594\n",
      "train loss:0.10391143570565067\n",
      "train loss:0.03819219005747408\n",
      "train loss:0.02244755808822779\n",
      "train loss:0.026251368219800723\n",
      "train loss:0.03167427784057904\n",
      "train loss:0.034694280104129144\n",
      "train loss:0.05820742422026907\n",
      "train loss:0.01586622741547061\n",
      "train loss:0.027381553359865852\n",
      "train loss:0.04757119722515828\n",
      "train loss:0.021542270670983434\n",
      "train loss:0.006134895244636276\n",
      "train loss:0.018822451891794257\n",
      "train loss:0.02672970132608203\n",
      "train loss:0.01360800427274609\n",
      "train loss:0.0173171531639318\n",
      "train loss:0.013917973690053204\n",
      "train loss:0.026056399711951183\n",
      "train loss:0.03192681564838883\n",
      "train loss:0.02908749705633129\n",
      "train loss:0.015833003771098694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.025966826266207558\n",
      "train loss:0.027973081762134602\n",
      "train loss:0.017354004973110085\n",
      "train loss:0.01846812914922897\n",
      "train loss:0.05283305553871953\n",
      "train loss:0.06197405062073655\n",
      "train loss:0.009506952998509092\n",
      "train loss:0.04150370476837662\n",
      "train loss:0.1274957895501707\n",
      "train loss:0.02294975373465731\n",
      "train loss:0.03784359918180422\n",
      "train loss:0.013296388355060132\n",
      "train loss:0.02042373062426966\n",
      "train loss:0.012545858560485705\n",
      "train loss:0.00714916746940587\n",
      "train loss:0.03247754140315351\n",
      "train loss:0.007180492703942408\n",
      "train loss:0.033094021820243666\n",
      "train loss:0.011772398643847737\n",
      "train loss:0.025050864374327717\n",
      "train loss:0.02241644403002031\n",
      "train loss:0.017616784439584993\n",
      "train loss:0.02747481979560828\n",
      "train loss:0.035718891453821684\n",
      "train loss:0.0456929757845689\n",
      "train loss:0.010003504365834076\n",
      "train loss:0.005618488967433141\n",
      "train loss:0.01499936824130081\n",
      "train loss:0.011120059656874157\n",
      "train loss:0.018914205232930033\n",
      "train loss:0.014374810186573635\n",
      "train loss:0.016977501645063054\n",
      "train loss:0.0687831688251642\n",
      "train loss:0.020654304386335726\n",
      "train loss:0.012703322987705235\n",
      "train loss:0.015519711945513312\n",
      "train loss:0.048812374999575955\n",
      "train loss:0.03909969459628694\n",
      "train loss:0.02148442501809951\n",
      "train loss:0.0377240582177259\n",
      "train loss:0.014876910254538628\n",
      "train loss:0.012819635238337648\n",
      "train loss:0.02638460759875725\n",
      "train loss:0.0062387060114289135\n",
      "train loss:0.017264752538040384\n",
      "train loss:0.04305616291700599\n",
      "train loss:0.039263725293880905\n",
      "train loss:0.008844166831485468\n",
      "train loss:0.07117811031382425\n",
      "train loss:0.02917581602476968\n",
      "train loss:0.019161573959466546\n",
      "train loss:0.024100550840153483\n",
      "train loss:0.023522850094347132\n",
      "train loss:0.028456339759054813\n",
      "train loss:0.012076482398195143\n",
      "train loss:0.011659389364172127\n",
      "train loss:0.01829158817702181\n",
      "train loss:0.07750239144212395\n",
      "train loss:0.016240423738201518\n",
      "train loss:0.0320341984305935\n",
      "train loss:0.018915726906172072\n",
      "train loss:0.060436200919449116\n",
      "train loss:0.03119159636783001\n",
      "train loss:0.022324172398431544\n",
      "train loss:0.019032543265929496\n",
      "train loss:0.020835807300222674\n",
      "train loss:0.013104275214895067\n",
      "train loss:0.02023890531951407\n",
      "train loss:0.010417797529439846\n",
      "train loss:0.04666684244457573\n",
      "train loss:0.009098823241835814\n",
      "train loss:0.019224035194126833\n",
      "train loss:0.024821976847862564\n",
      "train loss:0.018022024038902663\n",
      "train loss:0.03564248165993312\n",
      "train loss:0.026461459244317632\n",
      "train loss:0.011314232948245191\n",
      "train loss:0.00896390327733658\n",
      "train loss:0.013407539134109076\n",
      "train loss:0.03600274798625375\n",
      "train loss:0.04350798004353566\n",
      "train loss:0.01333670124913935\n",
      "train loss:0.020555139018333765\n",
      "train loss:0.007837685256646527\n",
      "train loss:0.019118086377467824\n",
      "train loss:0.021727653920657987\n",
      "train loss:0.015590185257767057\n",
      "train loss:0.026250182298899007\n",
      "train loss:0.01100805426721869\n",
      "train loss:0.010183574841540886\n",
      "train loss:0.026665626083646737\n",
      "train loss:0.0082531554520681\n",
      "train loss:0.05479317784816667\n",
      "train loss:0.011489491072910543\n",
      "train loss:0.010023313585947874\n",
      "train loss:0.005348512972935222\n",
      "train loss:0.024008499427005024\n",
      "train loss:0.010251995661407562\n",
      "train loss:0.019400217540794528\n",
      "train loss:0.031220132595201976\n",
      "train loss:0.003914102905701811\n",
      "train loss:0.06938206049440014\n",
      "train loss:0.015509716755529847\n",
      "train loss:0.03832658953950954\n",
      "train loss:0.01473737610865978\n",
      "train loss:0.0779219002921566\n",
      "train loss:0.009633555427054992\n",
      "train loss:0.035237201636637516\n",
      "train loss:0.011256247356742528\n",
      "train loss:0.056618818247255474\n",
      "train loss:0.020976439099446832\n",
      "train loss:0.010126467435157993\n",
      "train loss:0.02012375793359279\n",
      "train loss:0.016082505097008124\n",
      "train loss:0.029164615066214355\n",
      "train loss:0.017173447123093766\n",
      "train loss:0.006294863518188923\n",
      "train loss:0.03487074505866773\n",
      "train loss:0.021481762014515824\n",
      "train loss:0.06393679002547423\n",
      "train loss:0.022353203049918075\n",
      "train loss:0.010396912802336115\n",
      "train loss:0.03132263521801868\n",
      "train loss:0.011058560519071588\n",
      "train loss:0.03554624987457152\n",
      "train loss:0.033410879896632766\n",
      "train loss:0.0528597836323496\n",
      "train loss:0.018591221052174277\n",
      "train loss:0.028572177381908023\n",
      "train loss:0.023447584978517847\n",
      "train loss:0.011219577754109995\n",
      "train loss:0.020036661740277076\n",
      "train loss:0.017722804905322045\n",
      "train loss:0.02923862788264083\n",
      "train loss:0.00916926471569401\n",
      "train loss:0.011257058799219654\n",
      "train loss:0.04222800041638755\n",
      "train loss:0.04385379314914753\n",
      "train loss:0.06471519036592269\n",
      "train loss:0.022421645675826718\n",
      "train loss:0.02869857562338594\n",
      "train loss:0.09770370061877644\n",
      "train loss:0.007544891008538259\n",
      "train loss:0.03275772324759329\n",
      "train loss:0.027649105093375956\n",
      "train loss:0.023483796534520044\n",
      "train loss:0.014957365173897555\n",
      "train loss:0.03525813445842288\n",
      "train loss:0.04142765127322758\n",
      "train loss:0.008151769209834653\n",
      "train loss:0.06811169506176334\n",
      "train loss:0.018617697484936947\n",
      "train loss:0.011417735152581791\n",
      "train loss:0.023280508328456698\n",
      "train loss:0.03352878818180843\n",
      "train loss:0.04244832860073092\n",
      "train loss:0.025736849153901568\n",
      "train loss:0.019302105859059313\n",
      "train loss:0.05071537540789026\n",
      "train loss:0.058814740539220825\n",
      "train loss:0.014580852752786661\n",
      "train loss:0.041589789708386074\n",
      "train loss:0.030325525160111402\n",
      "train loss:0.07066007733939822\n",
      "train loss:0.00935509181752542\n",
      "train loss:0.037022652567652935\n",
      "train loss:0.028184860481984274\n",
      "train loss:0.011708260953561896\n",
      "train loss:0.00914529231035639\n",
      "train loss:0.019820558706305523\n",
      "train loss:0.08424281469342895\n",
      "train loss:0.005984260112010504\n",
      "train loss:0.010080768390354126\n",
      "train loss:0.04320554904743885\n",
      "train loss:0.16868895297973963\n",
      "train loss:0.012043588092389042\n",
      "train loss:0.04374177404720572\n",
      "train loss:0.04735287154802584\n",
      "train loss:0.022046691997897282\n",
      "train loss:0.020682239662992107\n",
      "train loss:0.016776704605640465\n",
      "train loss:0.06397767881230677\n",
      "train loss:0.005480532811220935\n",
      "train loss:0.0195670442438954\n",
      "train loss:0.032406782409771226\n",
      "train loss:0.007617234244219835\n",
      "train loss:0.007272910927434575\n",
      "train loss:0.01594415085971084\n",
      "train loss:0.02641009513943295\n",
      "train loss:0.015373657251235386\n",
      "train loss:0.00923840955105987\n",
      "train loss:0.015330819772572663\n",
      "train loss:0.04471914976670322\n",
      "train loss:0.019902908618951057\n",
      "train loss:0.010295817775979814\n",
      "train loss:0.044064459430871936\n",
      "train loss:0.013130140958540844\n",
      "train loss:0.005384805820847317\n",
      "train loss:0.04574806255779136\n",
      "train loss:0.015240869462844362\n",
      "train loss:0.014432272341323554\n",
      "train loss:0.020681051944877637\n",
      "train loss:0.020604213606005218\n",
      "train loss:0.013305706157393958\n",
      "train loss:0.016792945939998468\n",
      "train loss:0.009423699714824303\n",
      "train loss:0.02970369633208563\n",
      "train loss:0.01694846228886658\n",
      "train loss:0.03232693298524143\n",
      "train loss:0.02304495396882007\n",
      "train loss:0.038550493313150816\n",
      "train loss:0.01695383226161848\n",
      "train loss:0.02150130959220815\n",
      "train loss:0.00810593765703088\n",
      "train loss:0.012260462047001017\n",
      "train loss:0.011822158004464602\n",
      "train loss:0.017597767174725148\n",
      "train loss:0.03147014467164544\n",
      "train loss:0.04209745024080342\n",
      "train loss:0.015016267043210536\n",
      "train loss:0.0068771663108341645\n",
      "train loss:0.016510998390249866\n",
      "train loss:0.02656674884854604\n",
      "train loss:0.01389577280184095\n",
      "train loss:0.02661915286598366\n",
      "train loss:0.01717623261639336\n",
      "train loss:0.012174147246667526\n",
      "train loss:0.01571262715940926\n",
      "train loss:0.00782902363188962\n",
      "train loss:0.0187456701432461\n",
      "train loss:0.01940407910345081\n",
      "train loss:0.013632323646998306\n",
      "train loss:0.011026815758569761\n",
      "train loss:0.015460426901278114\n",
      "train loss:0.023879845232054938\n",
      "train loss:0.01657475520947597\n",
      "train loss:0.024740627159195328\n",
      "train loss:0.030703273842146647\n",
      "train loss:0.04109502879099972\n",
      "train loss:0.03467414191320869\n",
      "train loss:0.02479559106467354\n",
      "train loss:0.010254930926208362\n",
      "train loss:0.014197120486758192\n",
      "train loss:0.02127614524186131\n",
      "train loss:0.010943124026013447\n",
      "train loss:0.05009968928333293\n",
      "train loss:0.013696442444048598\n",
      "train loss:0.028731097779278802\n",
      "train loss:0.061703729733622786\n",
      "train loss:0.008561103293358872\n",
      "train loss:0.011172907107271106\n",
      "=== epoch:12, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.019295573778700132\n",
      "train loss:0.018915893691780743\n",
      "train loss:0.05572692170453685\n",
      "train loss:0.030941933934110714\n",
      "train loss:0.041247571983925174\n",
      "train loss:0.021549009278123074\n",
      "train loss:0.014042325931966027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.07466913330549622\n",
      "train loss:0.035096076614387205\n",
      "train loss:0.034293814621362294\n",
      "train loss:0.017666703188760825\n",
      "train loss:0.012783251953870287\n",
      "train loss:0.056454786591554784\n",
      "train loss:0.0383683733643908\n",
      "train loss:0.016404764922307385\n",
      "train loss:0.0066641004982887385\n",
      "train loss:0.02599144716924035\n",
      "train loss:0.01841477545798198\n",
      "train loss:0.011090276734585507\n",
      "train loss:0.014573152970897665\n",
      "train loss:0.01517162536915826\n",
      "train loss:0.016503390420918706\n",
      "train loss:0.05857735767716107\n",
      "train loss:0.053439989429999175\n",
      "train loss:0.017505693123790175\n",
      "train loss:0.011737988827513207\n",
      "train loss:0.018933950499386183\n",
      "train loss:0.017147521530859057\n",
      "train loss:0.05317462783879238\n",
      "train loss:0.020665407458859537\n",
      "train loss:0.026245849764760916\n",
      "train loss:0.046781858972591454\n",
      "train loss:0.056379270719308885\n",
      "train loss:0.023937280619092505\n",
      "train loss:0.012312537671323705\n",
      "train loss:0.011554514711081607\n",
      "train loss:0.0490144020989908\n",
      "train loss:0.02184509472913635\n",
      "train loss:0.023087915654610706\n",
      "train loss:0.023677389416670316\n",
      "train loss:0.028032295189287598\n",
      "train loss:0.027774839625146385\n",
      "train loss:0.009781233395690866\n",
      "train loss:0.02582519653999233\n",
      "train loss:0.028694209085814357\n",
      "train loss:0.033814921395192904\n",
      "train loss:0.05638521699433575\n",
      "train loss:0.020287216433264095\n",
      "train loss:0.0644500250448992\n",
      "train loss:0.03214442243952569\n",
      "train loss:0.025909492097997283\n",
      "train loss:0.010265185705447197\n",
      "train loss:0.026594698084057365\n",
      "train loss:0.048681905776099335\n",
      "train loss:0.013952740349565722\n",
      "train loss:0.06017380978861423\n",
      "train loss:0.02349059611616402\n",
      "train loss:0.010529642146519424\n",
      "train loss:0.013333351290994454\n",
      "train loss:0.0070501876125740985\n",
      "train loss:0.07106090788382993\n",
      "train loss:0.018022729600809407\n",
      "train loss:0.01780672323954541\n",
      "train loss:0.013637854893570002\n",
      "train loss:0.016745394158817692\n",
      "train loss:0.02346257967906808\n",
      "train loss:0.017716815959119232\n",
      "train loss:0.017015242264892254\n",
      "train loss:0.03349206269527629\n",
      "train loss:0.00853980051599067\n",
      "train loss:0.04696760283555487\n",
      "train loss:0.06102932035946978\n",
      "train loss:0.010611249373393285\n",
      "train loss:0.08628141501586527\n",
      "train loss:0.03171177568881052\n",
      "train loss:0.00674005259262158\n",
      "train loss:0.035632546077523135\n",
      "train loss:0.014433686999925867\n",
      "train loss:0.024797138041235796\n",
      "train loss:0.03400309830139829\n",
      "train loss:0.02963961068122943\n",
      "train loss:0.03571463207571207\n",
      "train loss:0.017427201067768693\n",
      "train loss:0.0563372089510369\n",
      "train loss:0.02025510150639577\n",
      "train loss:0.012478727425872669\n",
      "train loss:0.018545653702387598\n",
      "train loss:0.035339834253182816\n",
      "train loss:0.01477275798918174\n",
      "train loss:0.020161105328754366\n",
      "train loss:0.03100623876303052\n",
      "train loss:0.015120099266406648\n",
      "train loss:0.017368679323663486\n",
      "train loss:0.013545752824585488\n",
      "train loss:0.014024837056961574\n",
      "train loss:0.014659322124553384\n",
      "train loss:0.02247838101925783\n",
      "train loss:0.04613539462028828\n",
      "train loss:0.0070323669501685905\n",
      "train loss:0.02427097004814506\n",
      "train loss:0.012764426124565142\n",
      "train loss:0.08916671284738895\n",
      "train loss:0.02867737695401997\n",
      "train loss:0.024503479429364292\n",
      "train loss:0.035423902414291726\n",
      "train loss:0.025235229039623767\n",
      "train loss:0.017931914677657475\n",
      "train loss:0.009480861099478493\n",
      "train loss:0.023105400621947433\n",
      "train loss:0.0244637730564393\n",
      "train loss:0.03263897356759059\n",
      "train loss:0.014560840009548926\n",
      "train loss:0.021630884366332693\n",
      "train loss:0.020771770391501923\n",
      "train loss:0.007331999826370875\n",
      "train loss:0.02987404207902329\n",
      "train loss:0.015127717144524164\n",
      "train loss:0.038946786403070165\n",
      "train loss:0.029661521467796964\n",
      "train loss:0.029355584117308275\n",
      "train loss:0.02755029821001468\n",
      "train loss:0.013504068885082194\n",
      "train loss:0.04735058793795671\n",
      "train loss:0.04415844304918202\n",
      "train loss:0.010643112199771567\n",
      "train loss:0.02043404343605144\n",
      "train loss:0.0146829798491965\n",
      "train loss:0.015585812159067604\n",
      "train loss:0.03730448476274559\n",
      "train loss:0.01896991338818215\n",
      "train loss:0.0161884429804296\n",
      "train loss:0.010175428857181082\n",
      "train loss:0.029434886814055643\n",
      "train loss:0.028001523660469294\n",
      "train loss:0.04438372111209321\n",
      "train loss:0.032455096951389414\n",
      "train loss:0.0032449989595842222\n",
      "train loss:0.012880246845648058\n",
      "train loss:0.05786343260758086\n",
      "train loss:0.01097893161400939\n",
      "train loss:0.016004401465021006\n",
      "train loss:0.010892940522907632\n",
      "train loss:0.015022442368250906\n",
      "train loss:0.024764830697085446\n",
      "train loss:0.005246897071545049\n",
      "train loss:0.016900993846061165\n",
      "train loss:0.019508759388120845\n",
      "train loss:0.019977799654731673\n",
      "train loss:0.014533537656400097\n",
      "train loss:0.043984901772274904\n",
      "train loss:0.05151081919324868\n",
      "train loss:0.019411183307299115\n",
      "train loss:0.023910005583960378\n",
      "train loss:0.024583230230153644\n",
      "train loss:0.006797732207518213\n",
      "train loss:0.020497082247726707\n",
      "train loss:0.010817629600072662\n",
      "train loss:0.01616682500152653\n",
      "train loss:0.015492662439894292\n",
      "train loss:0.012356270742234608\n",
      "train loss:0.009818063311819272\n",
      "train loss:0.011636024134860106\n",
      "train loss:0.014848629000873536\n",
      "train loss:0.02339764982937764\n",
      "train loss:0.03865705701149209\n",
      "train loss:0.04180724469176963\n",
      "train loss:0.018113159816872827\n",
      "train loss:0.011423795504617975\n",
      "train loss:0.06630823475825892\n",
      "train loss:0.022243867693197736\n",
      "train loss:0.019190997832393385\n",
      "train loss:0.022328009149134735\n",
      "train loss:0.017052479678749514\n",
      "train loss:0.03278103561404537\n",
      "train loss:0.006588260695228786\n",
      "train loss:0.014727863491778695\n",
      "train loss:0.06037549033591741\n",
      "train loss:0.011814971999925754\n",
      "train loss:0.03037627682597786\n",
      "train loss:0.020209606290557732\n",
      "train loss:0.06280393380157992\n",
      "train loss:0.022594113424084745\n",
      "train loss:0.019285257677060578\n",
      "train loss:0.01504668716705386\n",
      "train loss:0.028871381949411537\n",
      "train loss:0.019655785713423204\n",
      "train loss:0.014555268908511823\n",
      "train loss:0.01814591680269891\n",
      "train loss:0.013837170884417658\n",
      "train loss:0.05109953049551052\n",
      "train loss:0.015627777608385206\n",
      "train loss:0.011791245213747026\n",
      "train loss:0.022588203239929886\n",
      "train loss:0.009004502881307039\n",
      "train loss:0.01560273545916081\n",
      "train loss:0.024923881599989546\n",
      "train loss:0.00885544066887608\n",
      "train loss:0.0292067201463074\n",
      "train loss:0.0114474235862595\n",
      "train loss:0.014598883803551042\n",
      "train loss:0.029731922420247973\n",
      "train loss:0.02863696147681467\n",
      "train loss:0.06239944604814595\n",
      "train loss:0.0072746217790943535\n",
      "train loss:0.0165806550147739\n",
      "train loss:0.03675987293917368\n",
      "train loss:0.007811400731028757\n",
      "train loss:0.004831011350621093\n",
      "train loss:0.015185933984349425\n",
      "train loss:0.024612345202663817\n",
      "train loss:0.029394756266880626\n",
      "train loss:0.019376713561730464\n",
      "train loss:0.019177918781335086\n",
      "train loss:0.020353004083166176\n",
      "train loss:0.043562509593234315\n",
      "train loss:0.007006620868759316\n",
      "train loss:0.009364028585567946\n",
      "train loss:0.02226993147315741\n",
      "train loss:0.016438607858394634\n",
      "train loss:0.07523864973406048\n",
      "train loss:0.03667670097706186\n",
      "train loss:0.00816306743818923\n",
      "train loss:0.17237713332168675\n",
      "train loss:0.008845759916979066\n",
      "train loss:0.03575584231089504\n",
      "train loss:0.015789307354571055\n",
      "train loss:0.019567005711566287\n",
      "train loss:0.031129658233626088\n",
      "train loss:0.015767780889772695\n",
      "train loss:0.03312390315302926\n",
      "train loss:0.056749470198501226\n",
      "train loss:0.01570836023791853\n",
      "train loss:0.01803975353177852\n",
      "train loss:0.008851668478530245\n",
      "train loss:0.010864997759711326\n",
      "train loss:0.009105583707331933\n",
      "train loss:0.019430234961550896\n",
      "train loss:0.018384664251329966\n",
      "train loss:0.008139708853742635\n",
      "train loss:0.014552736489522535\n",
      "train loss:0.010475276805143494\n",
      "train loss:0.01810874892160735\n",
      "train loss:0.020199424870211324\n",
      "train loss:0.015345394656537416\n",
      "train loss:0.023512896790830733\n",
      "train loss:0.009737897818495137\n",
      "train loss:0.012374661139036519\n",
      "train loss:0.06294311535557315\n",
      "train loss:0.023779782445043307\n",
      "train loss:0.008731755350819336\n",
      "train loss:0.013234564010349352\n",
      "train loss:0.018106721849318975\n",
      "train loss:0.028781475565669224\n",
      "train loss:0.011711755657748431\n",
      "train loss:0.01742954513341823\n",
      "train loss:0.014960115162513167\n",
      "train loss:0.01464738291023877\n",
      "train loss:0.05129467583551821\n",
      "train loss:0.013709060577729793\n",
      "train loss:0.020168013811799394\n",
      "train loss:0.0373976099178838\n",
      "train loss:0.05315588742588791\n",
      "train loss:0.014401595653205299\n",
      "train loss:0.012205379825032957\n",
      "train loss:0.0036545499506699746\n",
      "train loss:0.013206958021517879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.012830937517121619\n",
      "train loss:0.05734941472755465\n",
      "train loss:0.01786310120139385\n",
      "train loss:0.01420946875774385\n",
      "train loss:0.013698383382021334\n",
      "train loss:0.024891225066419803\n",
      "train loss:0.031399616562955666\n",
      "train loss:0.008694379240356818\n",
      "train loss:0.040465622801565806\n",
      "train loss:0.012987901078546216\n",
      "train loss:0.021652874921749644\n",
      "train loss:0.05022884344902809\n",
      "train loss:0.011477022016601335\n",
      "train loss:0.017728254356668676\n",
      "train loss:0.05097520962384678\n",
      "train loss:0.014019607395500702\n",
      "train loss:0.05839684542101332\n",
      "train loss:0.034111071239270346\n",
      "train loss:0.007067239241916739\n",
      "train loss:0.01893816358770086\n",
      "train loss:0.01758836287275465\n",
      "train loss:0.01652860480781516\n",
      "train loss:0.022200480592724822\n",
      "train loss:0.030132606332533497\n",
      "train loss:0.02171480750281846\n",
      "train loss:0.04626098195902616\n",
      "train loss:0.004925166108309937\n",
      "train loss:0.01874760650724934\n",
      "train loss:0.023365706494587597\n",
      "train loss:0.039869934800456745\n",
      "train loss:0.018853356666636942\n",
      "train loss:0.010918156200953558\n",
      "train loss:0.04485665101805047\n",
      "train loss:0.04173980593176317\n",
      "train loss:0.016843791061493432\n",
      "train loss:0.014093987036710165\n",
      "train loss:0.011888220402248684\n",
      "train loss:0.012129267583820458\n",
      "train loss:0.07116065167690167\n",
      "train loss:0.0209912228752946\n",
      "train loss:0.01882193630803399\n",
      "train loss:0.023278987332717813\n",
      "train loss:0.011234512958578202\n",
      "train loss:0.018259062740446162\n",
      "train loss:0.015968673050690187\n",
      "train loss:0.058863372043243496\n",
      "train loss:0.021336966405722687\n",
      "train loss:0.053042631192698114\n",
      "train loss:0.011353282126480955\n",
      "train loss:0.04520010956679084\n",
      "train loss:0.008775786364929634\n",
      "train loss:0.019277933335846206\n",
      "train loss:0.0403098549445646\n",
      "train loss:0.0345682205694087\n",
      "train loss:0.026936235853615823\n",
      "train loss:0.021874394057824195\n",
      "train loss:0.03165052503446736\n",
      "train loss:0.009525592089591209\n",
      "train loss:0.017940643617442797\n",
      "train loss:0.016755760164120344\n",
      "train loss:0.06578166917279557\n",
      "train loss:0.011566509272214576\n",
      "train loss:0.01635259132765046\n",
      "train loss:0.005732697875939692\n",
      "train loss:0.03564929997862636\n",
      "train loss:0.021221385532009203\n",
      "train loss:0.007423910564917504\n",
      "train loss:0.011049926671394953\n",
      "train loss:0.010327934107932283\n",
      "train loss:0.06979058398688939\n",
      "train loss:0.01894134987591503\n",
      "train loss:0.02444352761873749\n",
      "train loss:0.022844039170869735\n",
      "train loss:0.030562037080943388\n",
      "train loss:0.01730545029326294\n",
      "train loss:0.04423338689857495\n",
      "train loss:0.007792830444379877\n",
      "train loss:0.11327778943277832\n",
      "train loss:0.02656660828769304\n",
      "train loss:0.01675101934981304\n",
      "train loss:0.0037057781089743207\n",
      "train loss:0.006594003442571739\n",
      "train loss:0.05865874728079227\n",
      "train loss:0.010423024950470008\n",
      "train loss:0.00972855704312914\n",
      "train loss:0.07974793754193626\n",
      "train loss:0.02001745120455101\n",
      "train loss:0.045716759749483596\n",
      "train loss:0.01095938809640757\n",
      "train loss:0.009111961125957415\n",
      "train loss:0.03235176088628003\n",
      "train loss:0.02078959357643735\n",
      "train loss:0.026566484062288606\n",
      "train loss:0.057436123200260536\n",
      "train loss:0.024572248906590443\n",
      "train loss:0.01179643207116063\n",
      "train loss:0.021443930556865044\n",
      "train loss:0.01919939144424728\n",
      "train loss:0.009889100392216753\n",
      "train loss:0.1672949200821898\n",
      "train loss:0.018965417799823015\n",
      "train loss:0.040869441960715444\n",
      "train loss:0.037942983130268455\n",
      "train loss:0.05224844632502532\n",
      "train loss:0.08316469135080005\n",
      "train loss:0.008247284753263221\n",
      "train loss:0.058173807327343695\n",
      "train loss:0.015646698983463044\n",
      "train loss:0.008190650366221321\n",
      "train loss:0.03965645293399223\n",
      "train loss:0.010833282354074483\n",
      "train loss:0.04440489523467174\n",
      "train loss:0.060165012834036434\n",
      "train loss:0.0257268768222538\n",
      "train loss:0.01053377106128307\n",
      "train loss:0.015189001812294751\n",
      "train loss:0.014927453827883757\n",
      "train loss:0.01516217527440738\n",
      "train loss:0.01253505909403586\n",
      "train loss:0.01848645335198953\n",
      "train loss:0.016177921731824436\n",
      "train loss:0.013338691946220822\n",
      "train loss:0.04302476525850827\n",
      "train loss:0.015145125134974966\n",
      "train loss:0.014318434164105453\n",
      "train loss:0.013237072476056353\n",
      "train loss:0.07999585959230708\n",
      "train loss:0.02827600926979005\n",
      "train loss:0.015688213377319408\n",
      "train loss:0.018026449770040615\n",
      "train loss:0.007549402070288338\n",
      "train loss:0.009569365425884256\n",
      "train loss:0.02856546490099513\n",
      "train loss:0.010144672136914062\n",
      "train loss:0.014574045342036698\n",
      "train loss:0.005487251128256657\n",
      "train loss:0.008178200199292636\n",
      "train loss:0.0518104882913557\n",
      "train loss:0.016454912071775185\n",
      "train loss:0.027174289017248148\n",
      "train loss:0.03192088399404947\n",
      "train loss:0.00921429194479802\n",
      "train loss:0.03602857317474018\n",
      "train loss:0.010850575301242844\n",
      "train loss:0.022913684054865137\n",
      "train loss:0.010869018350882734\n",
      "train loss:0.01699594146280222\n",
      "train loss:0.021937674342272194\n",
      "train loss:0.006203266205297866\n",
      "train loss:0.007762524213584548\n",
      "train loss:0.008776026299076976\n",
      "train loss:0.015234173288705093\n",
      "train loss:0.015310739272534781\n",
      "train loss:0.014534387276204309\n",
      "train loss:0.018253866403521952\n",
      "train loss:0.030407298118762586\n",
      "train loss:0.03318665625977885\n",
      "train loss:0.016948206554869796\n",
      "train loss:0.03291919521684841\n",
      "train loss:0.008874300109817518\n",
      "train loss:0.012827197101399285\n",
      "train loss:0.010615890949225082\n",
      "train loss:0.08445571323248005\n",
      "train loss:0.006315286792422762\n",
      "train loss:0.033706773486377434\n",
      "train loss:0.03183920557567428\n",
      "train loss:0.01505235692781214\n",
      "train loss:0.015426580313952531\n",
      "train loss:0.042198056585230584\n",
      "train loss:0.012951976477978942\n",
      "train loss:0.010127936607257491\n",
      "train loss:0.012974483609285407\n",
      "train loss:0.024631033086283026\n",
      "train loss:0.024132132098255214\n",
      "train loss:0.017420308408166604\n",
      "train loss:0.008645189332960514\n",
      "train loss:0.033628017785323754\n",
      "train loss:0.015510227816368285\n",
      "train loss:0.03728471171035701\n",
      "train loss:0.021776703870839062\n",
      "train loss:0.012483405763306582\n",
      "train loss:0.008770965330988485\n",
      "train loss:0.01973964241358107\n",
      "train loss:0.019736796938049153\n",
      "=== epoch:13, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.010704885460185195\n",
      "train loss:0.008668732183312428\n",
      "train loss:0.030590142585128564\n",
      "train loss:0.008716078593152474\n",
      "train loss:0.01417380087236974\n",
      "train loss:0.01043337102674009\n",
      "train loss:0.011687090451621873\n",
      "train loss:0.02944865786486534\n",
      "train loss:0.004942335069821331\n",
      "train loss:0.016753046009940188\n",
      "train loss:0.06003130311145754\n",
      "train loss:0.05070736142214218\n",
      "train loss:0.0245127018265955\n",
      "train loss:0.014404554066524156\n",
      "train loss:0.015519977667186515\n",
      "train loss:0.02368321646078014\n",
      "train loss:0.008142209335107883\n",
      "train loss:0.02127006487754858\n",
      "train loss:0.019073192313481334\n",
      "train loss:0.04087670305291996\n",
      "train loss:0.01282084521340202\n",
      "train loss:0.013345314207075147\n",
      "train loss:0.02880466977468509\n",
      "train loss:0.0888723868071749\n",
      "train loss:0.06370194151954994\n",
      "train loss:0.013919394309162916\n",
      "train loss:0.055049551577363284\n",
      "train loss:0.007747737559157886\n",
      "train loss:0.013654678129860322\n",
      "train loss:0.009911726434690923\n",
      "train loss:0.03130822959506987\n",
      "train loss:0.04340632939815788\n",
      "train loss:0.016579509401635918\n",
      "train loss:0.041447258206528816\n",
      "train loss:0.036764607352815996\n",
      "train loss:0.006134471377637031\n",
      "train loss:0.009785000923850776\n",
      "train loss:0.012435506770756834\n",
      "train loss:0.014302108228288635\n",
      "train loss:0.02552682839028155\n",
      "train loss:0.00806911020270067\n",
      "train loss:0.057665803774728595\n",
      "train loss:0.018653558017564652\n",
      "train loss:0.07257933033672227\n",
      "train loss:0.06749636111524236\n",
      "train loss:0.012128202230884216\n",
      "train loss:0.019017788316998026\n",
      "train loss:0.011657071197110744\n",
      "train loss:0.0861837398336227\n",
      "train loss:0.017957459002589736\n",
      "train loss:0.010887929935023473\n",
      "train loss:0.01879027227753577\n",
      "train loss:0.020411287199377428\n",
      "train loss:0.007455254266786967\n",
      "train loss:0.014634296414995005\n",
      "train loss:0.07712630147119984\n",
      "train loss:0.012996592743072783\n",
      "train loss:0.01735753745406262\n",
      "train loss:0.018806766890900672\n",
      "train loss:0.04307796136533598\n",
      "train loss:0.010785788603715414\n",
      "train loss:0.029458394276644238\n",
      "train loss:0.055366919722265956\n",
      "train loss:0.009543251288338654\n",
      "train loss:0.01208956187153603\n",
      "train loss:0.02711445451620617\n",
      "train loss:0.010019165721460022\n",
      "train loss:0.02258848241810796\n",
      "train loss:0.01566590739708328\n",
      "train loss:0.0226805475301667\n",
      "train loss:0.033152379164665166\n",
      "train loss:0.00849166464824297\n",
      "train loss:0.024058849349370984\n",
      "train loss:0.01195680898006841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.012498360199740866\n",
      "train loss:0.01933091219895413\n",
      "train loss:0.023199248110678018\n",
      "train loss:0.028317393434375705\n",
      "train loss:0.03496419228920954\n",
      "train loss:0.0071071774545377435\n",
      "train loss:0.011938728403734533\n",
      "train loss:0.0254001182670845\n",
      "train loss:0.022032277585287367\n",
      "train loss:0.02343728348691361\n",
      "train loss:0.01572789459208312\n",
      "train loss:0.006381699967574099\n",
      "train loss:0.021970476857552698\n",
      "train loss:0.016214602852941712\n",
      "train loss:0.0826942473716902\n",
      "train loss:0.02691425399546178\n",
      "train loss:0.04422958514728086\n",
      "train loss:0.014034935088642293\n",
      "train loss:0.006949152731431313\n",
      "train loss:0.10002315562403188\n",
      "train loss:0.035010675130840406\n",
      "train loss:0.01451688787815562\n",
      "train loss:0.020211736922994893\n",
      "train loss:0.022862293742969594\n",
      "train loss:0.00909346449816704\n",
      "train loss:0.013497508685355215\n",
      "train loss:0.03341626250980928\n",
      "train loss:0.03947863458317133\n",
      "train loss:0.010522903526498022\n",
      "train loss:0.011526310457523446\n",
      "train loss:0.007843257711148514\n",
      "train loss:0.016154102906572456\n",
      "train loss:0.022673772447752035\n",
      "train loss:0.007160494387661083\n",
      "train loss:0.016272276674218773\n",
      "train loss:0.02833292106740642\n",
      "train loss:0.008538072583441726\n",
      "train loss:0.016253636262109385\n",
      "train loss:0.015609685377198866\n",
      "train loss:0.008706420292331278\n",
      "train loss:0.027861047653457136\n",
      "train loss:0.018143712757448895\n",
      "train loss:0.018545203449686687\n",
      "train loss:0.027405628975953316\n",
      "train loss:0.0335476032696528\n",
      "train loss:0.05216435041947986\n",
      "train loss:0.010633900645799246\n",
      "train loss:0.006962105834544488\n",
      "train loss:0.018396973783362067\n",
      "train loss:0.02682571900260974\n",
      "train loss:0.012352733279728039\n",
      "train loss:0.01796522549575447\n",
      "train loss:0.014978934135842514\n",
      "train loss:0.012933211553190042\n",
      "train loss:0.012835425546393622\n",
      "train loss:0.042433814214365845\n",
      "train loss:0.026149872661825235\n",
      "train loss:0.016045929242638493\n",
      "train loss:0.012050116039805287\n",
      "train loss:0.04904472473863478\n",
      "train loss:0.014662592709273024\n",
      "train loss:0.012178845470834341\n",
      "train loss:0.014412839467019824\n",
      "train loss:0.015004960673825556\n",
      "train loss:0.014300297778785196\n",
      "train loss:0.02281436908117121\n",
      "train loss:0.012260957392332955\n",
      "train loss:0.05677776695764882\n",
      "train loss:0.017458779360478495\n",
      "train loss:0.013797506058722092\n",
      "train loss:0.013057290853232616\n",
      "train loss:0.022933948802116744\n",
      "train loss:0.0451040514830253\n",
      "train loss:0.016168017223144815\n",
      "train loss:0.00793619036109157\n",
      "train loss:0.010533273072928584\n",
      "train loss:0.016450133423479417\n",
      "train loss:0.0235694747560902\n",
      "train loss:0.007852489770270732\n",
      "train loss:0.017602329110177922\n",
      "train loss:0.008484710302726222\n",
      "train loss:0.021891709108143784\n",
      "train loss:0.015191104027933049\n",
      "train loss:0.025588957009937862\n",
      "train loss:0.013548544372052804\n",
      "train loss:0.014985505631084963\n",
      "train loss:0.007787430780699575\n",
      "train loss:0.012810003912980407\n",
      "train loss:0.03085601911628435\n",
      "train loss:0.010363689673910648\n",
      "train loss:0.011877506595441378\n",
      "train loss:0.010470313948550643\n",
      "train loss:0.06864212010862931\n",
      "train loss:0.012115501303345644\n",
      "train loss:0.023904768409971675\n",
      "train loss:0.020627506621665827\n",
      "train loss:0.02521282518082075\n",
      "train loss:0.032063760376793704\n",
      "train loss:0.021506089411834167\n",
      "train loss:0.01523652245160086\n",
      "train loss:0.022611068585606277\n",
      "train loss:0.0326730752283248\n",
      "train loss:0.015287623489017181\n",
      "train loss:0.011297382932336982\n",
      "train loss:0.023595258790698716\n",
      "train loss:0.0037841374796197\n",
      "train loss:0.022551756191294733\n",
      "train loss:0.04177562963339433\n",
      "train loss:0.016988331823097377\n",
      "train loss:0.019323869922518975\n",
      "train loss:0.006753773059887187\n",
      "train loss:0.016863788548384807\n",
      "train loss:0.0033761659178243447\n",
      "train loss:0.011292239932768034\n",
      "train loss:0.006531687210768858\n",
      "train loss:0.013170006368541397\n",
      "train loss:0.0056917113181494715\n",
      "train loss:0.026554286854713473\n",
      "train loss:0.0094637990467939\n",
      "train loss:0.015298720331861542\n",
      "train loss:0.010773905578211136\n",
      "train loss:0.02090851659958064\n",
      "train loss:0.01292347665157246\n",
      "train loss:0.0072041952096474704\n",
      "train loss:0.021230634804436224\n",
      "train loss:0.00441873729799677\n",
      "train loss:0.043711851205828794\n",
      "train loss:0.013567118257483846\n",
      "train loss:0.01581611173232095\n",
      "train loss:0.01784338104166712\n",
      "train loss:0.03247023872883898\n",
      "train loss:0.015492551708732418\n",
      "train loss:0.028873997573413846\n",
      "train loss:0.04032507549309412\n",
      "train loss:0.011601216840981492\n",
      "train loss:0.07773109072999236\n",
      "train loss:0.09569125102105469\n",
      "train loss:0.024344613106954058\n",
      "train loss:0.01124487198004992\n",
      "train loss:0.010855703023042269\n",
      "train loss:0.008385511486342265\n",
      "train loss:0.01810825065274687\n",
      "train loss:0.02338618225195713\n",
      "train loss:0.0066399927336686804\n",
      "train loss:0.007366360356299241\n",
      "train loss:0.043250746101098796\n",
      "train loss:0.016443752953995135\n",
      "train loss:0.02052372550905806\n",
      "train loss:0.005727359013706471\n",
      "train loss:0.018723150582069845\n",
      "train loss:0.012329306620726881\n",
      "train loss:0.031993797138902796\n",
      "train loss:0.007439476052196475\n",
      "train loss:0.008841762210019342\n",
      "train loss:0.02847894746133198\n",
      "train loss:0.03154590856464068\n",
      "train loss:0.038737429404055945\n",
      "train loss:0.06195173645863166\n",
      "train loss:0.017980186605296485\n",
      "train loss:0.03680198908620705\n",
      "train loss:0.015246133821140883\n",
      "train loss:0.006548000753568011\n",
      "train loss:0.06201010276338548\n",
      "train loss:0.04211471000776847\n",
      "train loss:0.006159329787780612\n",
      "train loss:0.02026079299998556\n",
      "train loss:0.020907934770742816\n",
      "train loss:0.03335763198942246\n",
      "train loss:0.005259355972357074\n",
      "train loss:0.024164659989725012\n",
      "train loss:0.0236716452411641\n",
      "train loss:0.022290818497745512\n",
      "train loss:0.05316690618557018\n",
      "train loss:0.020580980004270465\n",
      "train loss:0.015213257077135762\n",
      "train loss:0.010027190207432615\n",
      "train loss:0.03297872176990729\n",
      "train loss:0.0201523240051912\n",
      "train loss:0.011284344774618567\n",
      "train loss:0.012280262977687079\n",
      "train loss:0.014071879775061923\n",
      "train loss:0.02412328913572126\n",
      "train loss:0.015646495730729616\n",
      "train loss:0.008201882043475635\n",
      "train loss:0.060496274108873455\n",
      "train loss:0.04099187627323073\n",
      "train loss:0.05415901430260578\n",
      "train loss:0.007695073747632031\n",
      "train loss:0.05768302914757891\n",
      "train loss:0.018806079884888475\n",
      "train loss:0.01766692675386397\n",
      "train loss:0.01358650417695709\n",
      "train loss:0.021268941902018747\n",
      "train loss:0.007638910321897352\n",
      "train loss:0.033136279184119724\n",
      "train loss:0.008925883312192035\n",
      "train loss:0.04310902772849385\n",
      "train loss:0.012041882879955104\n",
      "train loss:0.009629238453228932\n",
      "train loss:0.025987303041719464\n",
      "train loss:0.025398488635416334\n",
      "train loss:0.046494981048974086\n",
      "train loss:0.04913049010064996\n",
      "train loss:0.014507507152158235\n",
      "train loss:0.007986820794797688\n",
      "train loss:0.015017753949168593\n",
      "train loss:0.0186058749812693\n",
      "train loss:0.009760079588937293\n",
      "train loss:0.041438978317655036\n",
      "train loss:0.011084202315850063\n",
      "train loss:0.03309597563610129\n",
      "train loss:0.012772890952120863\n",
      "train loss:0.03585882128347069\n",
      "train loss:0.027058807978494453\n",
      "train loss:0.03017715854999727\n",
      "train loss:0.009935101241386506\n",
      "train loss:0.016730589221493043\n",
      "train loss:0.007039737636798842\n",
      "train loss:0.01643333768593685\n",
      "train loss:0.011733119435405575\n",
      "train loss:0.021334517540656864\n",
      "train loss:0.02841822388037528\n",
      "train loss:0.008618580675123852\n",
      "train loss:0.015273873197262004\n",
      "train loss:0.021127466601239816\n",
      "train loss:0.024862529385197413\n",
      "train loss:0.01078290527144501\n",
      "train loss:0.01867606420807018\n",
      "train loss:0.018576407304931524\n",
      "train loss:0.011333890231528952\n",
      "train loss:0.026640739013035028\n",
      "train loss:0.005734440727079344\n",
      "train loss:0.08771269229298442\n",
      "train loss:0.009348296434099346\n",
      "train loss:0.02200392400343443\n",
      "train loss:0.010660057835258741\n",
      "train loss:0.024440577469142025\n",
      "train loss:0.003571750427873579\n",
      "train loss:0.044915341109504056\n",
      "train loss:0.02529136056965091\n",
      "train loss:0.01072232774316254\n",
      "train loss:0.0062734361705051945\n",
      "train loss:0.01186456886826187\n",
      "train loss:0.008799147891362798\n",
      "train loss:0.01992657604923859\n",
      "train loss:0.051775326065810034\n",
      "train loss:0.007643545996947018\n",
      "train loss:0.0058403838225561545\n",
      "train loss:0.010511911416256833\n",
      "train loss:0.006000295228963107\n",
      "train loss:0.026616318595345984\n",
      "train loss:0.02611815101604425\n",
      "train loss:0.01311294671685057\n",
      "train loss:0.03245734932750286\n",
      "train loss:0.04474851480124833\n",
      "train loss:0.024686329753270918\n",
      "train loss:0.024261957173746383\n",
      "train loss:0.015165495321409504\n",
      "train loss:0.006206101741918777\n",
      "train loss:0.007810852232701237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.018774852640826675\n",
      "train loss:0.009058631218846888\n",
      "train loss:0.03213084465318167\n",
      "train loss:0.027446641666631365\n",
      "train loss:0.01175115852297003\n",
      "train loss:0.006228842732606936\n",
      "train loss:0.020570488402201027\n",
      "train loss:0.007216535965850281\n",
      "train loss:0.010157788840258674\n",
      "train loss:0.011667273847585324\n",
      "train loss:0.006367540363297052\n",
      "train loss:0.05824267232152297\n",
      "train loss:0.007506998303808929\n",
      "train loss:0.013958588487375015\n",
      "train loss:0.013042130957612417\n",
      "train loss:0.050366003158473636\n",
      "train loss:0.006129843975991474\n",
      "train loss:0.028229118308815117\n",
      "train loss:0.007355154532000193\n",
      "train loss:0.011505315630342308\n",
      "train loss:0.015594169650989824\n",
      "train loss:0.024351931816901276\n",
      "train loss:0.02230229198157344\n",
      "train loss:0.05065482663155129\n",
      "train loss:0.016186167643649985\n",
      "train loss:0.031013924569963235\n",
      "train loss:0.009055998861917894\n",
      "train loss:0.012124417903923008\n",
      "train loss:0.029372751795469262\n",
      "train loss:0.06204411406095961\n",
      "train loss:0.01478764671946135\n",
      "train loss:0.013449424060717767\n",
      "train loss:0.010305584654079039\n",
      "train loss:0.016538656188494188\n",
      "train loss:0.007632168077494459\n",
      "train loss:0.010131341676942476\n",
      "train loss:0.06689608109358178\n",
      "train loss:0.008221038422289885\n",
      "train loss:0.05733853856122816\n",
      "train loss:0.0334603953008197\n",
      "train loss:0.013281752467187193\n",
      "train loss:0.05504475575178494\n",
      "train loss:0.011370819212420053\n",
      "train loss:0.05482102654789088\n",
      "train loss:0.02968480838616698\n",
      "train loss:0.011320146476907378\n",
      "train loss:0.02885321374303359\n",
      "train loss:0.013319866518704806\n",
      "train loss:0.007783483962732025\n",
      "train loss:0.0073947415397261\n",
      "train loss:0.01755905880039923\n",
      "train loss:0.03145790893038359\n",
      "train loss:0.032216907429481474\n",
      "train loss:0.006123078904781809\n",
      "train loss:0.030493104486468466\n",
      "train loss:0.006787635834191689\n",
      "train loss:0.02378205480874727\n",
      "train loss:0.01956972632897375\n",
      "train loss:0.025072191618220573\n",
      "train loss:0.006631453566697882\n",
      "train loss:0.039755555305303066\n",
      "train loss:0.014788298610212447\n",
      "train loss:0.005697140617888999\n",
      "train loss:0.02089799896686613\n",
      "train loss:0.016530301662255137\n",
      "train loss:0.013144437727143781\n",
      "train loss:0.0817069262743972\n",
      "train loss:0.016368654112125982\n",
      "train loss:0.021937479408058666\n",
      "train loss:0.02410041911745158\n",
      "train loss:0.01396360492196323\n",
      "train loss:0.03348785986523437\n",
      "train loss:0.02820387906835612\n",
      "train loss:0.019789638989648802\n",
      "train loss:0.02100590810732399\n",
      "train loss:0.029333885629870746\n",
      "train loss:0.025108808762128246\n",
      "train loss:0.04482785460309943\n",
      "train loss:0.011278102324251423\n",
      "train loss:0.021852886238190542\n",
      "train loss:0.0191347678890248\n",
      "train loss:0.02628095779936292\n",
      "train loss:0.064836025410109\n",
      "train loss:0.03740833394580953\n",
      "train loss:0.036172278059404\n",
      "train loss:0.021471641401503646\n",
      "train loss:0.0159285712081729\n",
      "train loss:0.02801175765545247\n",
      "train loss:0.02686563457356329\n",
      "train loss:0.03599852144746138\n",
      "train loss:0.03657617492704418\n",
      "train loss:0.00922894272783119\n",
      "train loss:0.005083827587629411\n",
      "train loss:0.010965861910698873\n",
      "train loss:0.02714408881399004\n",
      "train loss:0.023318723918769543\n",
      "train loss:0.014130419422900997\n",
      "train loss:0.009602449818636555\n",
      "train loss:0.009128866620547993\n",
      "train loss:0.019051619633859113\n",
      "train loss:0.020453400180406756\n",
      "train loss:0.02632041943465306\n",
      "train loss:0.014447322369676292\n",
      "train loss:0.025712649847468825\n",
      "train loss:0.01732174273102427\n",
      "train loss:0.02826643435949922\n",
      "train loss:0.010827620308764043\n",
      "train loss:0.020187931746483992\n",
      "train loss:0.018391749838290852\n",
      "train loss:0.023042732123021093\n",
      "train loss:0.013010911840602017\n",
      "train loss:0.04751085236224279\n",
      "train loss:0.019165981728965085\n",
      "train loss:0.00857106828466075\n",
      "train loss:0.019653118800357264\n",
      "train loss:0.009889884093195241\n",
      "=== epoch:14, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.012291238099831925\n",
      "train loss:0.02077447511122432\n",
      "train loss:0.009923421811352617\n",
      "train loss:0.04666225041332024\n",
      "train loss:0.02399091846605623\n",
      "train loss:0.017686776622308623\n",
      "train loss:0.016173669773787622\n",
      "train loss:0.008183791412220113\n",
      "train loss:0.01532516864032006\n",
      "train loss:0.00786502695232919\n",
      "train loss:0.029367276676091524\n",
      "train loss:0.013485823947135432\n",
      "train loss:0.018692070840848982\n",
      "train loss:0.012995836253206267\n",
      "train loss:0.03237972627276731\n",
      "train loss:0.017065873132007806\n",
      "train loss:0.008867026917701687\n",
      "train loss:0.022847836109195715\n",
      "train loss:0.012734327316699823\n",
      "train loss:0.009668392631958617\n",
      "train loss:0.016741223222967398\n",
      "train loss:0.046011210623999384\n",
      "train loss:0.03784118609414633\n",
      "train loss:0.03772639968511407\n",
      "train loss:0.019721244363659827\n",
      "train loss:0.017397991217475265\n",
      "train loss:0.006534617121723331\n",
      "train loss:0.009500316841940573\n",
      "train loss:0.045902780923186566\n",
      "train loss:0.02273568801521643\n",
      "train loss:0.03310084346642995\n",
      "train loss:0.024648704603480575\n",
      "train loss:0.02390809480833545\n",
      "train loss:0.024326116783243017\n",
      "train loss:0.014243916404458093\n",
      "train loss:0.018142161453120535\n",
      "train loss:0.010415717130225615\n",
      "train loss:0.01004332926309656\n",
      "train loss:0.019239965148099683\n",
      "train loss:0.015389519669361895\n",
      "train loss:0.004116568299332892\n",
      "train loss:0.016740401234482817\n",
      "train loss:0.027061117338410847\n",
      "train loss:0.02206933124893671\n",
      "train loss:0.02266477201481733\n",
      "train loss:0.01606083867316858\n",
      "train loss:0.021862076332995924\n",
      "train loss:0.007584354730940065\n",
      "train loss:0.02527390398560685\n",
      "train loss:0.009175585503126622\n",
      "train loss:0.011556449200088373\n",
      "train loss:0.009396853161881252\n",
      "train loss:0.047829265592188495\n",
      "train loss:0.008876495231014067\n",
      "train loss:0.01635261894168093\n",
      "train loss:0.01985130110717606\n",
      "train loss:0.006984406997281311\n",
      "train loss:0.009628069411476151\n",
      "train loss:0.00978460220257602\n",
      "train loss:0.017205919704744527\n",
      "train loss:0.011386015777726537\n",
      "train loss:0.033376445343351185\n",
      "train loss:0.010142870830506434\n",
      "train loss:0.006757989059315968\n",
      "train loss:0.011821541363374459\n",
      "train loss:0.021043941606701342\n",
      "train loss:0.007836563157487562\n",
      "train loss:0.03489653004839659\n",
      "train loss:0.01453941186912023\n",
      "train loss:0.04682157803255671\n",
      "train loss:0.011348545279733701\n",
      "train loss:0.004908416887981408\n",
      "train loss:0.007534869487989089\n",
      "train loss:0.013985327153442756\n",
      "train loss:0.022598051950478874\n",
      "train loss:0.012322418290715873\n",
      "train loss:0.008337620324540414\n",
      "train loss:0.04377572279116371\n",
      "train loss:0.015206865432606142\n",
      "train loss:0.006577672243147678\n",
      "train loss:0.009999214735400415\n",
      "train loss:0.013014586339240448\n",
      "train loss:0.023454274277462838\n",
      "train loss:0.03015233188829345\n",
      "train loss:0.010155157486397885\n",
      "train loss:0.008242041158793545\n",
      "train loss:0.032595986290382045\n",
      "train loss:0.016626033389667113\n",
      "train loss:0.014536825750365261\n",
      "train loss:0.01008520229535088\n",
      "train loss:0.007360753496305593\n",
      "train loss:0.023328940956913916\n",
      "train loss:0.018408060689302244\n",
      "train loss:0.0032603905481606264\n",
      "train loss:0.02115692164623064\n",
      "train loss:0.041096980399768286\n",
      "train loss:0.004915256130221451\n",
      "train loss:0.017335257637423936\n",
      "train loss:0.01653779006149628\n",
      "train loss:0.03169482425119919\n",
      "train loss:0.018432881077566434\n",
      "train loss:0.00800434480625254\n",
      "train loss:0.025375661149018236\n",
      "train loss:0.010265243797775513\n",
      "train loss:0.013782018425058905\n",
      "train loss:0.00788331899652891\n",
      "train loss:0.015497331982865643\n",
      "train loss:0.026878054055618675\n",
      "train loss:0.019596030126381007\n",
      "train loss:0.01166204635330641\n",
      "train loss:0.016564895618401475\n",
      "train loss:0.010474710781350713\n",
      "train loss:0.026937584400735685\n",
      "train loss:0.02201724772329653\n",
      "train loss:0.008838116790675777\n",
      "train loss:0.009565230985484811\n",
      "train loss:0.01441950455233266\n",
      "train loss:0.014565278949910302\n",
      "train loss:0.010718251028255682\n",
      "train loss:0.011366290219945195\n",
      "train loss:0.06920164808277138\n",
      "train loss:0.03699731677882639\n",
      "train loss:0.0036789120055009078\n",
      "train loss:0.006419042409170604\n",
      "train loss:0.018530847022346465\n",
      "train loss:0.012271623080233902\n",
      "train loss:0.026660572127243976\n",
      "train loss:0.006032310861743194\n",
      "train loss:0.028373467230470673\n",
      "train loss:0.016198292755490486\n",
      "train loss:0.03514591998763868\n",
      "train loss:0.015337028019445878\n",
      "train loss:0.01930319463515566\n",
      "train loss:0.01724259869018281\n",
      "train loss:0.022965335170082892\n",
      "train loss:0.00927576565269158\n",
      "train loss:0.018926518144321974\n",
      "train loss:0.0066735311856685655\n",
      "train loss:0.007535817657032451\n",
      "train loss:0.004653214372064968\n",
      "train loss:0.014956403280660707\n",
      "train loss:0.028594611014869504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.04271318045853152\n",
      "train loss:0.029611675059871164\n",
      "train loss:0.019668916913574624\n",
      "train loss:0.03741184991848532\n",
      "train loss:0.01144132717110015\n",
      "train loss:0.0321500319777221\n",
      "train loss:0.04168657305600141\n",
      "train loss:0.010266574399534593\n",
      "train loss:0.02393284331260487\n",
      "train loss:0.01047735459373081\n",
      "train loss:0.018572441642236814\n",
      "train loss:0.012977277482414102\n",
      "train loss:0.010922763994099085\n",
      "train loss:0.030465338773623923\n",
      "train loss:0.013637458578140972\n",
      "train loss:0.01067750221545785\n",
      "train loss:0.012191348820241097\n",
      "train loss:0.003786122892299771\n",
      "train loss:0.013794108989070655\n",
      "train loss:0.017608432708665175\n",
      "train loss:0.018578589864129216\n",
      "train loss:0.05809726386302393\n",
      "train loss:0.012336257820852323\n",
      "train loss:0.009457280282845999\n",
      "train loss:0.01732135492160518\n",
      "train loss:0.010301210742485124\n",
      "train loss:0.009958815785252937\n",
      "train loss:0.02716291885376564\n",
      "train loss:0.019307624598191166\n",
      "train loss:0.02958474909357035\n",
      "train loss:0.0352090260016968\n",
      "train loss:0.005693290493595123\n",
      "train loss:0.04626694751238374\n",
      "train loss:0.011529727898126688\n",
      "train loss:0.004453304822624584\n",
      "train loss:0.017440532317897713\n",
      "train loss:0.007647640796001876\n",
      "train loss:0.025568981649992484\n",
      "train loss:0.02412777054884254\n",
      "train loss:0.029265671540087138\n",
      "train loss:0.016523043087567447\n",
      "train loss:0.0027623155648540828\n",
      "train loss:0.011653934962450496\n",
      "train loss:0.012208649749211192\n",
      "train loss:0.01383610821236132\n",
      "train loss:0.01186309776046337\n",
      "train loss:0.005602227706988072\n",
      "train loss:0.01288043776943747\n",
      "train loss:0.019660950708962975\n",
      "train loss:0.015877255028241125\n",
      "train loss:0.017417466359569766\n",
      "train loss:0.0059383176611493725\n",
      "train loss:0.011792287653803532\n",
      "train loss:0.01970688516682999\n",
      "train loss:0.08714307395502954\n",
      "train loss:0.02622297664236535\n",
      "train loss:0.016960029670993127\n",
      "train loss:0.009108230673307792\n",
      "train loss:0.05570882705618256\n",
      "train loss:0.010188536902938779\n",
      "train loss:0.007274368155010147\n",
      "train loss:0.009690142293921756\n",
      "train loss:0.03621342011109838\n",
      "train loss:0.02288729902412337\n",
      "train loss:0.04978093997409155\n",
      "train loss:0.008521937339741006\n",
      "train loss:0.042694885157708945\n",
      "train loss:0.007506006182128141\n",
      "train loss:0.023670260029014942\n",
      "train loss:0.008548520613687997\n",
      "train loss:0.010630158242451402\n",
      "train loss:0.016295829469876618\n",
      "train loss:0.010452441627844504\n",
      "train loss:0.034431709394171194\n",
      "train loss:0.019567896350719375\n",
      "train loss:0.01410377163257562\n",
      "train loss:0.01789128319386177\n",
      "train loss:0.0105275369156824\n",
      "train loss:0.01638129091478567\n",
      "train loss:0.008460400783154805\n",
      "train loss:0.02152976056950037\n",
      "train loss:0.03583283470705438\n",
      "train loss:0.027321742327491735\n",
      "train loss:0.020427547528349202\n",
      "train loss:0.019221177912233937\n",
      "train loss:0.03755399830042995\n",
      "train loss:0.019753972694176617\n",
      "train loss:0.018195991012921343\n",
      "train loss:0.024236376889629107\n",
      "train loss:0.017365316050819157\n",
      "train loss:0.016814944425235578\n",
      "train loss:0.015798587713010077\n",
      "train loss:0.014187171475856766\n",
      "train loss:0.04518466152300319\n",
      "train loss:0.030294660266268786\n",
      "train loss:0.025497131949455656\n",
      "train loss:0.021702582362642726\n",
      "train loss:0.023616008080968892\n",
      "train loss:0.005576154437084425\n",
      "train loss:0.0707495435007073\n",
      "train loss:0.011905892097347407\n",
      "train loss:0.015635197557826736\n",
      "train loss:0.007753841913472751\n",
      "train loss:0.01935233360343872\n",
      "train loss:0.054669936037035696\n",
      "train loss:0.010570828790434942\n",
      "train loss:0.048188566583751466\n",
      "train loss:0.020702074205015256\n",
      "train loss:0.019667965004700458\n",
      "train loss:0.008505513442918526\n",
      "train loss:0.020357614026813572\n",
      "train loss:0.005658828724690088\n",
      "train loss:0.007829919530989257\n",
      "train loss:0.008301886760798094\n",
      "train loss:0.00950497017629938\n",
      "train loss:0.01745174142238273\n",
      "train loss:0.02789185239625677\n",
      "train loss:0.008821007155051968\n",
      "train loss:0.016243701740035275\n",
      "train loss:0.016543249168767194\n",
      "train loss:0.01886615453056159\n",
      "train loss:0.015802615485702172\n",
      "train loss:0.011624246431843288\n",
      "train loss:0.01804961988511365\n",
      "train loss:0.013650639834798917\n",
      "train loss:0.057985812103335095\n",
      "train loss:0.016472353176000203\n",
      "train loss:0.02068247029858428\n",
      "train loss:0.01401842611963387\n",
      "train loss:0.022482891795083653\n",
      "train loss:0.018799876125618186\n",
      "train loss:0.013961399346808779\n",
      "train loss:0.019362424767182477\n",
      "train loss:0.03104830732117149\n",
      "train loss:0.005580895866814981\n",
      "train loss:0.023422174016280838\n",
      "train loss:0.009329158111809371\n",
      "train loss:0.016112845178019548\n",
      "train loss:0.021379517448081913\n",
      "train loss:0.019955740296130533\n",
      "train loss:0.01805258926064208\n",
      "train loss:0.01643563526591686\n",
      "train loss:0.01064045360786711\n",
      "train loss:0.03811984559226827\n",
      "train loss:0.007528425961615765\n",
      "train loss:0.031612470292510624\n",
      "train loss:0.02607107793261376\n",
      "train loss:0.014457359883033349\n",
      "train loss:0.004766016114223976\n",
      "train loss:0.018607269860541623\n",
      "train loss:0.009188912027118802\n",
      "train loss:0.013204091364017402\n",
      "train loss:0.009657549064083813\n",
      "train loss:0.04426924802669037\n",
      "train loss:0.011684739582485166\n",
      "train loss:0.006720385735453737\n",
      "train loss:0.010350825244743478\n",
      "train loss:0.007885782337546372\n",
      "train loss:0.018433842700678237\n",
      "train loss:0.024322798548769363\n",
      "train loss:0.021818654650284933\n",
      "train loss:0.020249927602102855\n",
      "train loss:0.008948249726571076\n",
      "train loss:0.013639382480746327\n",
      "train loss:0.015542868644814141\n",
      "train loss:0.013217368472751559\n",
      "train loss:0.006158261699242709\n",
      "train loss:0.012805929986474724\n",
      "train loss:0.035450394133226544\n",
      "train loss:0.0054636377690151575\n",
      "train loss:0.05168827898811653\n",
      "train loss:0.02345943817205904\n",
      "train loss:0.01960598901639925\n",
      "train loss:0.01288126418185299\n",
      "train loss:0.03154857553718369\n",
      "train loss:0.021197980424219613\n",
      "train loss:0.023514517514103668\n",
      "train loss:0.02318802049504098\n",
      "train loss:0.005143722142353595\n",
      "train loss:0.01544544592444656\n",
      "train loss:0.022969666601256766\n",
      "train loss:0.0202337445922514\n",
      "train loss:0.033154093336802457\n",
      "train loss:0.010594308926030922\n",
      "train loss:0.010397019994569595\n",
      "train loss:0.03240639829404166\n",
      "train loss:0.034813712314803215\n",
      "train loss:0.008951132798614033\n",
      "train loss:0.006150735059038997\n",
      "train loss:0.030357896251675124\n",
      "train loss:0.011382749850486939\n",
      "train loss:0.010006512292616614\n",
      "train loss:0.005587595297214525\n",
      "train loss:0.013884310956504695\n",
      "train loss:0.009656479439995008\n",
      "train loss:0.015826334240727134\n",
      "train loss:0.00722059911731336\n",
      "train loss:0.014514355101765635\n",
      "train loss:0.0070867913267616345\n",
      "train loss:0.019740206942039616\n",
      "train loss:0.01585102510701418\n",
      "train loss:0.006552773453543402\n",
      "train loss:0.025020480208482628\n",
      "train loss:0.01418562227891567\n",
      "train loss:0.014307433918049532\n",
      "train loss:0.03446949220099203\n",
      "train loss:0.007804287859447106\n",
      "train loss:0.01751709509018005\n",
      "train loss:0.040901947778585954\n",
      "train loss:0.03420626380478029\n",
      "train loss:0.017801753661196785\n",
      "train loss:0.010103505173522466\n",
      "train loss:0.019656401864490674\n",
      "train loss:0.022507218234249324\n",
      "train loss:0.012395456479429965\n",
      "train loss:0.03371689329913829\n",
      "train loss:0.008339645486895498\n",
      "train loss:0.010068868801079293\n",
      "train loss:0.003133904652981639\n",
      "train loss:0.014693400043994307\n",
      "train loss:0.013709872887208591\n",
      "train loss:0.02423758755850705\n",
      "train loss:0.018396720556594227\n",
      "train loss:0.017129712417539137\n",
      "train loss:0.002316777455317252\n",
      "train loss:0.00910152236568328\n",
      "train loss:0.024734664046056558\n",
      "train loss:0.020741425443737853\n",
      "train loss:0.007953269485613156\n",
      "train loss:0.012376503375733237\n",
      "train loss:0.025177992571154696\n",
      "train loss:0.02750935306395382\n",
      "train loss:0.025615381577997132\n",
      "train loss:0.01016297096408571\n",
      "train loss:0.027327135250904595\n",
      "train loss:0.030980417653315797\n",
      "train loss:0.013305604602549741\n",
      "train loss:0.02135205756511909\n",
      "train loss:0.005057186585514488\n",
      "train loss:0.020119234936108833\n",
      "train loss:0.049278253141956496\n",
      "train loss:0.01478161902212826\n",
      "train loss:0.017467516606246732\n",
      "train loss:0.010159147680067917\n",
      "train loss:0.009201301412410627\n",
      "train loss:0.006520366656912624\n",
      "train loss:0.0031230119600656014\n",
      "train loss:0.01796757540323137\n",
      "train loss:0.061072808118160966\n",
      "train loss:0.018730204072151882\n",
      "train loss:0.015909481333574015\n",
      "train loss:0.012861428036548606\n",
      "train loss:0.026469848951639202\n",
      "train loss:0.03076906749798254\n",
      "train loss:0.011530861253030258\n",
      "train loss:0.01744344799217124\n",
      "train loss:0.02724310239882227\n",
      "train loss:0.028160250720097472\n",
      "train loss:0.028987696862330915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03048475346469029\n",
      "train loss:0.024062232879706543\n",
      "train loss:0.005861030365871239\n",
      "train loss:0.010970836294136184\n",
      "train loss:0.04260905807145788\n",
      "train loss:0.0059566928829942675\n",
      "train loss:0.014410745670745281\n",
      "train loss:0.014186432354519387\n",
      "train loss:0.00524650295482772\n",
      "train loss:0.018655501335863613\n",
      "train loss:0.019416973302194447\n",
      "train loss:0.025176173828573005\n",
      "train loss:0.026083758269458933\n",
      "train loss:0.02241754178083671\n",
      "train loss:0.006655191462900564\n",
      "train loss:0.006818385952961775\n",
      "train loss:0.008644000405865939\n",
      "train loss:0.020436558770156744\n",
      "train loss:0.02393449081997306\n",
      "train loss:0.013620087523589564\n",
      "train loss:0.01282672016355902\n",
      "train loss:0.0063042996528298745\n",
      "train loss:0.013671345163686526\n",
      "train loss:0.0030666716849394515\n",
      "train loss:0.006177412075068578\n",
      "train loss:0.02435016746878063\n",
      "train loss:0.013221054788370023\n",
      "train loss:0.006894697449410504\n",
      "train loss:0.09487869602200844\n",
      "train loss:0.04671230339252091\n",
      "train loss:0.015319344828871689\n",
      "train loss:0.0030808191857612504\n",
      "train loss:0.013089628391364799\n",
      "train loss:0.028351425627974387\n",
      "train loss:0.012917149444016132\n",
      "train loss:0.009842091278914566\n",
      "train loss:0.010885053359742415\n",
      "train loss:0.018010313124131344\n",
      "train loss:0.012608209084512465\n",
      "train loss:0.01105215334163723\n",
      "train loss:0.007062160738220018\n",
      "train loss:0.027274690086599728\n",
      "train loss:0.013311275185052616\n",
      "train loss:0.02536116700966566\n",
      "train loss:0.005460580109073324\n",
      "train loss:0.01741420918971278\n",
      "train loss:0.02033593436069199\n",
      "train loss:0.018708157106095892\n",
      "train loss:0.034594344289038355\n",
      "=== epoch:15, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.02537426267273216\n",
      "train loss:0.00402377420570946\n",
      "train loss:0.016341801074269864\n",
      "train loss:0.015215350559278605\n",
      "train loss:0.01614937740686609\n",
      "train loss:0.014052494463261322\n",
      "train loss:0.008696432285200345\n",
      "train loss:0.00907629158875186\n",
      "train loss:0.03092612275820169\n",
      "train loss:0.03754198035686561\n",
      "train loss:0.006822434666428594\n",
      "train loss:0.015552560328043767\n",
      "train loss:0.021478088768008528\n",
      "train loss:0.008572871261892195\n",
      "train loss:0.028352745945746615\n",
      "train loss:0.047786834710134445\n",
      "train loss:0.019234774962087916\n",
      "train loss:0.0035298534289603784\n",
      "train loss:0.008268237657059785\n",
      "train loss:0.011989893198875314\n",
      "train loss:0.021604051759922083\n",
      "train loss:0.015199311632983697\n",
      "train loss:0.013696479245833126\n",
      "train loss:0.017905602372701487\n",
      "train loss:0.006461675013502508\n",
      "train loss:0.01427497116088548\n",
      "train loss:0.032739948153491655\n",
      "train loss:0.04190139095213776\n",
      "train loss:0.008500388942721508\n",
      "train loss:0.020410368191725477\n",
      "train loss:0.014758606733598712\n",
      "train loss:0.009822191374675417\n",
      "train loss:0.01566286895121265\n",
      "train loss:0.011653037423228468\n",
      "train loss:0.014672436254006055\n",
      "train loss:0.020977963525285265\n",
      "train loss:0.028051426848127302\n",
      "train loss:0.006390810701946022\n",
      "train loss:0.009869838773118462\n",
      "train loss:0.01062507740279099\n",
      "train loss:0.00783451818468179\n",
      "train loss:0.014234033304056657\n",
      "train loss:0.009920943966049828\n",
      "train loss:0.011221699141875253\n",
      "train loss:0.04535016518705958\n",
      "train loss:0.011593487926563775\n",
      "train loss:0.016473266489994687\n",
      "train loss:0.007359832874627511\n",
      "train loss:0.019800987272619322\n",
      "train loss:0.05435727456347683\n",
      "train loss:0.005093155807842581\n",
      "train loss:0.0030071189895068385\n",
      "train loss:0.007227750797095183\n",
      "train loss:0.01910654493965579\n",
      "train loss:0.013452218151665078\n",
      "train loss:0.04197128346679003\n",
      "train loss:0.005433689702551852\n",
      "train loss:0.00394616512179034\n",
      "train loss:0.017739654974349314\n",
      "train loss:0.010675097565938785\n",
      "train loss:0.01333296114916762\n",
      "train loss:0.008117246609969075\n",
      "train loss:0.012664894986109025\n",
      "train loss:0.02098680321209217\n",
      "train loss:0.025407687577321463\n",
      "train loss:0.014366173477143902\n",
      "train loss:0.010242719529439406\n",
      "train loss:0.005501254229175955\n",
      "train loss:0.004292086042275491\n",
      "train loss:0.01598232142291457\n",
      "train loss:0.05143872243439396\n",
      "train loss:0.008294331963770909\n",
      "train loss:0.02210328661239815\n",
      "train loss:0.015379987373937356\n",
      "train loss:0.030241595864011334\n",
      "train loss:0.010009694061380768\n",
      "train loss:0.027505962189229693\n",
      "train loss:0.013806721325802358\n",
      "train loss:0.013886379375367823\n",
      "train loss:0.010505697387676411\n",
      "train loss:0.021577089694032322\n",
      "train loss:0.01937994904920801\n",
      "train loss:0.03235021519666045\n",
      "train loss:0.018760610016866127\n",
      "train loss:0.004042437451793232\n",
      "train loss:0.007041772530798974\n",
      "train loss:0.011493878737848549\n",
      "train loss:0.028481740651597844\n",
      "train loss:0.022839390980940028\n",
      "train loss:0.0084743268494006\n",
      "train loss:0.03800273708577625\n",
      "train loss:0.025059020787359866\n",
      "train loss:0.029840885102268132\n",
      "train loss:0.018747238071596412\n",
      "train loss:0.037626856810947705\n",
      "train loss:0.019853093929437363\n",
      "train loss:0.01165905715428858\n",
      "train loss:0.008286710282844792\n",
      "train loss:0.021212206476039278\n",
      "train loss:0.012690098155742869\n",
      "train loss:0.05657296933298734\n",
      "train loss:0.07085706482601595\n",
      "train loss:0.03992822979164041\n",
      "train loss:0.06717904117545997\n",
      "train loss:0.015445640397795966\n",
      "train loss:0.007661741535671343\n",
      "train loss:0.013963300840454718\n",
      "train loss:0.015074189360128172\n",
      "train loss:0.06034452302826013\n",
      "train loss:0.03897483506573652\n",
      "train loss:0.013407612728967892\n",
      "train loss:0.025278557854992226\n",
      "train loss:0.007387549710114432\n",
      "train loss:0.014144530767311507\n",
      "train loss:0.05259867654076064\n",
      "train loss:0.004328374820417979\n",
      "train loss:0.009640506532588305\n",
      "train loss:0.021948944302736858\n",
      "train loss:0.007158984300731789\n",
      "train loss:0.006341349391412785\n",
      "train loss:0.011163215051136577\n",
      "train loss:0.026295660481556203\n",
      "train loss:0.00810446785912297\n",
      "train loss:0.018348514987920325\n",
      "train loss:0.0066471464608383905\n",
      "train loss:0.006125070912702135\n",
      "train loss:0.004615811756377649\n",
      "train loss:0.008436245315875956\n",
      "train loss:0.00774736517162026\n",
      "train loss:0.0065327392242300565\n",
      "train loss:0.032895109926638244\n",
      "train loss:0.01466031520381629\n",
      "train loss:0.032403833623826594\n",
      "train loss:0.018638631519933092\n",
      "train loss:0.005825317973046877\n",
      "train loss:0.01254016703267181\n",
      "train loss:0.008931237824235745\n",
      "train loss:0.007185784262385294\n",
      "train loss:0.01907592550838852\n",
      "train loss:0.009015457264578893\n",
      "train loss:0.022633231530999852\n",
      "train loss:0.012569049589365612\n",
      "train loss:0.005179118322431238\n",
      "train loss:0.010012439873252368\n",
      "train loss:0.00755184342430683\n",
      "train loss:0.018026564521363177\n",
      "train loss:0.06367471999343945\n",
      "train loss:0.036305600661525605\n",
      "train loss:0.015604073504939208\n",
      "train loss:0.022990119254586102\n",
      "train loss:0.0244905222550388\n",
      "train loss:0.007522010714306721\n",
      "train loss:0.0054778148306164265\n",
      "train loss:0.003516936106264362\n",
      "train loss:0.0042410415125715886\n",
      "train loss:0.020560038612049615\n",
      "train loss:0.02098737099392077\n",
      "train loss:0.00855552466684816\n",
      "train loss:0.006261412593437725\n",
      "train loss:0.024494835217044583\n",
      "train loss:0.028482436103981105\n",
      "train loss:0.025973638628080685\n",
      "train loss:0.014981679891724553\n",
      "train loss:0.024787613385290518\n",
      "train loss:0.018158468546132896\n",
      "train loss:0.010189691402043153\n",
      "train loss:0.0068690258947507535\n",
      "train loss:0.014962659766482346\n",
      "train loss:0.01670943999730999\n",
      "train loss:0.007581618972973448\n",
      "train loss:0.019304170102182924\n",
      "train loss:0.010601070616644517\n",
      "train loss:0.031158407612365716\n",
      "train loss:0.010056379207269633\n",
      "train loss:0.01563055767066511\n",
      "train loss:0.010775272685692703\n",
      "train loss:0.01819037378663667\n",
      "train loss:0.01093394369170791\n",
      "train loss:0.02909838953278327\n",
      "train loss:0.020800629978313043\n",
      "train loss:0.047439687153226245\n",
      "train loss:0.06959483389829942\n",
      "train loss:0.015525423043561337\n",
      "train loss:0.012271101837213633\n",
      "train loss:0.008730752259445014\n",
      "train loss:0.019213949801757415\n",
      "train loss:0.015670656988646267\n",
      "train loss:0.03399352330240679\n",
      "train loss:0.008830968867008116\n",
      "train loss:0.008753314260639614\n",
      "train loss:0.014809348097929593\n",
      "train loss:0.02044100004272802\n",
      "train loss:0.027127136756286165\n",
      "train loss:0.014507178391279952\n",
      "train loss:0.01228686920146981\n",
      "train loss:0.006089226459944773\n",
      "train loss:0.028460538120137796\n",
      "train loss:0.014451875842633046\n",
      "train loss:0.028977166073291382\n",
      "train loss:0.004929523810132682\n",
      "train loss:0.03272653835249174\n",
      "train loss:0.033184460090276195\n",
      "train loss:0.023218425204043523\n",
      "train loss:0.01602210464727969\n",
      "train loss:0.02008834368522315\n",
      "train loss:0.013948739784687208\n",
      "train loss:0.007604891682477534\n",
      "train loss:0.016275605671681178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0158791514584384\n",
      "train loss:0.009906010411597554\n",
      "train loss:0.032218935410081403\n",
      "train loss:0.005161254279009201\n",
      "train loss:0.014415217384131967\n",
      "train loss:0.005315194491081377\n",
      "train loss:0.030423852115502886\n",
      "train loss:0.026085505679570797\n",
      "train loss:0.013067532502320402\n",
      "train loss:0.005870122198316616\n",
      "train loss:0.01350655601653084\n",
      "train loss:0.0080370740001745\n",
      "train loss:0.041928027972820185\n",
      "train loss:0.014742305788442487\n",
      "train loss:0.012417440547065095\n",
      "train loss:0.018093256238639274\n",
      "train loss:0.0044379210726707054\n",
      "train loss:0.005678584651763046\n",
      "train loss:0.02401229230330883\n",
      "train loss:0.007914483540491847\n",
      "train loss:0.020517351947514653\n",
      "train loss:0.007794324283795878\n",
      "train loss:0.01645947690248687\n",
      "train loss:0.01882480754757321\n",
      "train loss:0.1123062421265492\n",
      "train loss:0.00869577694190419\n",
      "train loss:0.018674847046517656\n",
      "train loss:0.01700282378237236\n",
      "train loss:0.007370728507387879\n",
      "train loss:0.008387917065483908\n",
      "train loss:0.019119991157819247\n",
      "train loss:0.010899645769060533\n",
      "train loss:0.029756810170343536\n",
      "train loss:0.013240245956933915\n",
      "train loss:0.010277810671061913\n",
      "train loss:0.03339770630154448\n",
      "train loss:0.06920028194715322\n",
      "train loss:0.011879505076298081\n",
      "train loss:0.012435792837448863\n",
      "train loss:0.052715923132358455\n",
      "train loss:0.007456755355139172\n",
      "train loss:0.0073388599568629635\n",
      "train loss:0.04036605114255042\n",
      "train loss:0.011929916422738935\n",
      "train loss:0.00784942838613117\n",
      "train loss:0.023974660648033806\n",
      "train loss:0.0043683453112431615\n",
      "train loss:0.005556498771008663\n",
      "train loss:0.02816243317725874\n",
      "train loss:0.017486304561475537\n",
      "train loss:0.013463427170751562\n",
      "train loss:0.007894884075370453\n",
      "train loss:0.01641146127798344\n",
      "train loss:0.008894600815522518\n",
      "train loss:0.012021417021284906\n",
      "train loss:0.011736125341344812\n",
      "train loss:0.019621661071349855\n",
      "train loss:0.005389318981499584\n",
      "train loss:0.025982109991193613\n",
      "train loss:0.009326624791509152\n",
      "train loss:0.023161093850412054\n",
      "train loss:0.009007688688189734\n",
      "train loss:0.008353032591688007\n",
      "train loss:0.016332403198434715\n",
      "train loss:0.010957756153797446\n",
      "train loss:0.00890171441780588\n",
      "train loss:0.021783788009108566\n",
      "train loss:0.028638365683198067\n",
      "train loss:0.018406697373081694\n",
      "train loss:0.03069068670655934\n",
      "train loss:0.058275191849537854\n",
      "train loss:0.01395129117354309\n",
      "train loss:0.033010795832366574\n",
      "train loss:0.008942651784325545\n",
      "train loss:0.010851988155251055\n",
      "train loss:0.022944458009655815\n",
      "train loss:0.03817855368483217\n",
      "train loss:0.029783618072559834\n",
      "train loss:0.01522519214100188\n",
      "train loss:0.022503958046960787\n",
      "train loss:0.014781285026105588\n",
      "train loss:0.015680371246620528\n",
      "train loss:0.007648933440370934\n",
      "train loss:0.016685881231134275\n",
      "train loss:0.03578719520611458\n",
      "train loss:0.00983981160532631\n",
      "train loss:0.035361758818151186\n",
      "train loss:0.004825045235598216\n",
      "train loss:0.011797597337575657\n",
      "train loss:0.011009982722098789\n",
      "train loss:0.017310086560515935\n",
      "train loss:0.013530708984067418\n",
      "train loss:0.042773602848034296\n",
      "train loss:0.024106141824115378\n",
      "train loss:0.013998068049237653\n",
      "train loss:0.00439719297855438\n",
      "train loss:0.012222070451995044\n",
      "train loss:0.005781442492146302\n",
      "train loss:0.009335171775616024\n",
      "train loss:0.018367187187604748\n",
      "train loss:0.00848198428984707\n",
      "train loss:0.013144090823448917\n",
      "train loss:0.01435609747842494\n",
      "train loss:0.004114607714172861\n",
      "train loss:0.040675799026174415\n",
      "train loss:0.0177964386770514\n",
      "train loss:0.054143031626340425\n",
      "train loss:0.012107440585767949\n",
      "train loss:0.017584704310776886\n",
      "train loss:0.03450111504471854\n",
      "train loss:0.04834859562482222\n",
      "train loss:0.012778977869968795\n",
      "train loss:0.008814140058484096\n",
      "train loss:0.010280596214608997\n",
      "train loss:0.045103935607283645\n",
      "train loss:0.013941540570988183\n",
      "train loss:0.009733194400090121\n",
      "train loss:0.010166215346109507\n",
      "train loss:0.011799659216506254\n",
      "train loss:0.01374659164256071\n",
      "train loss:0.011430610423308365\n",
      "train loss:0.0538772819496461\n",
      "train loss:0.03293666212938791\n",
      "train loss:0.006653119027944072\n",
      "train loss:0.017936628094531327\n",
      "train loss:0.004947445235880995\n",
      "train loss:0.024838639749610748\n",
      "train loss:0.014609864597132339\n",
      "train loss:0.007172209161895269\n",
      "train loss:0.007711340075171361\n",
      "train loss:0.008629473086721014\n",
      "train loss:0.014274248360913568\n",
      "train loss:0.01342565656797565\n",
      "train loss:0.03223386516983037\n",
      "train loss:0.02659820079260133\n",
      "train loss:0.007307447380189738\n",
      "train loss:0.07328668261098514\n",
      "train loss:0.04574225090891511\n",
      "train loss:0.006852012517063895\n",
      "train loss:0.027506100612188788\n",
      "train loss:0.013825397535581978\n",
      "train loss:0.012710204292108656\n",
      "train loss:0.03339394577795314\n",
      "train loss:0.062045250670097064\n",
      "train loss:0.014738114845184358\n",
      "train loss:0.008844795266356863\n",
      "train loss:0.018919593431377967\n",
      "train loss:0.023879635749783353\n",
      "train loss:0.013530114053592083\n",
      "train loss:0.014853464086590938\n",
      "train loss:0.02289343959556365\n",
      "train loss:0.012060347938833633\n",
      "train loss:0.023883430158076913\n",
      "train loss:0.021814513382820085\n",
      "train loss:0.010705979180995716\n",
      "train loss:0.0334750069160227\n",
      "train loss:0.01062687315044889\n",
      "train loss:0.037954354929376\n",
      "train loss:0.011025299838058101\n",
      "train loss:0.018573497549429746\n",
      "train loss:0.01629366414893364\n",
      "train loss:0.012074453125335926\n",
      "train loss:0.004418640747368123\n",
      "train loss:0.008679561477738134\n",
      "train loss:0.010545972428402595\n",
      "train loss:0.017506076755876194\n",
      "train loss:0.00497061287048232\n",
      "train loss:0.007258941180173458\n",
      "train loss:0.01911974443421133\n",
      "train loss:0.010076352821367408\n",
      "train loss:0.013561352837906524\n",
      "train loss:0.006690301454435189\n",
      "train loss:0.011537256836562388\n",
      "train loss:0.0414131056640944\n",
      "train loss:0.010729192670273911\n",
      "train loss:0.04213700695365703\n",
      "train loss:0.024128932036672834\n",
      "train loss:0.008197900259632955\n",
      "train loss:0.011168234582750276\n",
      "train loss:0.006102638194532196\n",
      "train loss:0.01992892861747574\n",
      "train loss:0.018020022528865188\n",
      "train loss:0.015655599676589776\n",
      "train loss:0.008816131537925466\n",
      "train loss:0.009700772772565874\n",
      "train loss:0.007637738557282178\n",
      "train loss:0.030747490091468113\n",
      "train loss:0.07394713919325212\n",
      "train loss:0.010930756737710334\n",
      "train loss:0.00272441221084828\n",
      "train loss:0.012516695851877318\n",
      "train loss:0.025058728427635426\n",
      "train loss:0.024272342912937943\n",
      "train loss:0.005525138249492535\n",
      "train loss:0.01621810539684056\n",
      "train loss:0.030139462785477796\n",
      "train loss:0.009415298753285268\n",
      "train loss:0.007371812224471763\n",
      "train loss:0.016576026488241753\n",
      "train loss:0.024001175729282145\n",
      "train loss:0.010624842610564612\n",
      "train loss:0.004974185856044833\n",
      "train loss:0.05606429283602348\n",
      "train loss:0.04290364283666333\n",
      "train loss:0.012969470620516075\n",
      "train loss:0.05241243955282755\n",
      "train loss:0.016062721002662466\n",
      "train loss:0.03276021249516569\n",
      "train loss:0.012708875420152685\n",
      "train loss:0.024745894914944496\n",
      "train loss:0.009174461225963743\n",
      "train loss:0.022302570774073473\n",
      "train loss:0.01450730656170881\n",
      "train loss:0.011500322543158175\n",
      "train loss:0.013124547341269667\n",
      "train loss:0.01868936077179745\n",
      "train loss:0.004836101732847533\n",
      "train loss:0.007872106566937674\n",
      "train loss:0.022260374208086905\n",
      "train loss:0.011728084398554172\n",
      "train loss:0.01288401588582409\n",
      "train loss:0.008781108297569298\n",
      "train loss:0.026626483539235673\n",
      "train loss:0.027591309263767964\n",
      "train loss:0.02257759389423855\n",
      "train loss:0.023418802433592498\n",
      "train loss:0.01567428722166764\n",
      "train loss:0.052366305138253695\n",
      "train loss:0.01707750181829818\n",
      "train loss:0.008878018751071377\n",
      "train loss:0.01240032123387132\n",
      "train loss:0.010124010088763626\n",
      "train loss:0.054703194823579215\n",
      "train loss:0.010549669295690949\n",
      "train loss:0.01721236869368326\n",
      "train loss:0.01134524256751479\n",
      "train loss:0.03601735972445185\n",
      "train loss:0.014573659958481753\n",
      "train loss:0.008524417451109932\n",
      "train loss:0.0070742568308124745\n",
      "train loss:0.030213002616855188\n",
      "train loss:0.010885876110376464\n",
      "=== epoch:16, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.013332726682253056\n",
      "train loss:0.024416068550262\n",
      "train loss:0.011416075333145233\n",
      "train loss:0.0224155944306459\n",
      "train loss:0.020068942035175875\n",
      "train loss:0.04977029952242709\n",
      "train loss:0.014762812507582686\n",
      "train loss:0.012701176254042722\n",
      "train loss:0.007178817749213664\n",
      "train loss:0.007338096531951927\n",
      "train loss:0.01566867927561726\n",
      "train loss:0.0055817825591835965\n",
      "train loss:0.015147065777610222\n",
      "train loss:0.012077206401949956\n",
      "train loss:0.04428685916993086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02340851439138799\n",
      "train loss:0.002578108878102485\n",
      "train loss:0.004652626165489198\n",
      "train loss:0.006085497575574072\n",
      "train loss:0.02354088097378905\n",
      "train loss:0.00704094633420081\n",
      "train loss:0.019658732179671082\n",
      "train loss:0.022658336404788218\n",
      "train loss:0.020801294816003204\n",
      "train loss:0.008001532803162247\n",
      "train loss:0.017650484110617995\n",
      "train loss:0.02173402297732071\n",
      "train loss:0.03890892014903728\n",
      "train loss:0.03293287676784622\n",
      "train loss:0.008972955661065332\n",
      "train loss:0.013542075770948563\n",
      "train loss:0.02251746542004128\n",
      "train loss:0.06412232014570671\n",
      "train loss:0.021211115807132114\n",
      "train loss:0.011375732164966401\n",
      "train loss:0.010589490523559295\n",
      "train loss:0.03209522543537986\n",
      "train loss:0.005081267532188763\n",
      "train loss:0.022632122250967418\n",
      "train loss:0.017894546120683086\n",
      "train loss:0.007867025381791178\n",
      "train loss:0.030329911542071205\n",
      "train loss:0.010613791640407724\n",
      "train loss:0.009166221534986451\n",
      "train loss:0.011223430328413341\n",
      "train loss:0.036720374976302046\n",
      "train loss:0.009186961907966272\n",
      "train loss:0.017420512602225668\n",
      "train loss:0.015807285824698706\n",
      "train loss:0.019038487592021494\n",
      "train loss:0.010282158046079157\n",
      "train loss:0.007437682621214483\n",
      "train loss:0.006055535847380695\n",
      "train loss:0.0075179044936703465\n",
      "train loss:0.02058397214731335\n",
      "train loss:0.02893414173072502\n",
      "train loss:0.005405104023823133\n",
      "train loss:0.029271180605064652\n",
      "train loss:0.009277607264521698\n",
      "train loss:0.01652600167205929\n",
      "train loss:0.004474463986659406\n",
      "train loss:0.011516605430239303\n",
      "train loss:0.029763569219659423\n",
      "train loss:0.007136601258408695\n",
      "train loss:0.012867498480018941\n",
      "train loss:0.023455661055988695\n",
      "train loss:0.010142706852187424\n",
      "train loss:0.010723924743720161\n",
      "train loss:0.011121720216551891\n",
      "train loss:0.014028491323838885\n",
      "train loss:0.011981120273546593\n",
      "train loss:0.026827674668532517\n",
      "train loss:0.017751029030563394\n",
      "train loss:0.023085279989914373\n",
      "train loss:0.00982594288464759\n",
      "train loss:0.013178659429579447\n",
      "train loss:0.037088814443452155\n",
      "train loss:0.025903768485640678\n",
      "train loss:0.023541129655843776\n",
      "train loss:0.0069572738766620415\n",
      "train loss:0.010177710681408925\n",
      "train loss:0.027310813404645747\n",
      "train loss:0.008429390584855083\n",
      "train loss:0.02028325761304439\n",
      "train loss:0.0073965936030971425\n",
      "train loss:0.03621972443978998\n",
      "train loss:0.017087105951253952\n",
      "train loss:0.017616044415286733\n",
      "train loss:0.05795202750981639\n",
      "train loss:0.003881559330931965\n",
      "train loss:0.004442675640751857\n",
      "train loss:0.026456105546033493\n",
      "train loss:0.011970418657741623\n",
      "train loss:0.012384539222216988\n",
      "train loss:0.010118083881771127\n",
      "train loss:0.009627745033455528\n",
      "train loss:0.05772000720202717\n",
      "train loss:0.0196274255060475\n",
      "train loss:0.014445686728765308\n",
      "train loss:0.008121800554197312\n",
      "train loss:0.013162872379583787\n",
      "train loss:0.010790807383975554\n",
      "train loss:0.02572742480371791\n",
      "train loss:0.006548245329846782\n",
      "train loss:0.01844787784325991\n",
      "train loss:0.016040699172390736\n",
      "train loss:0.0036879379074685344\n",
      "train loss:0.012125304095878572\n",
      "train loss:0.023369978941553095\n",
      "train loss:0.01830730719858912\n",
      "train loss:0.011500169015649573\n",
      "train loss:0.016075321358982042\n",
      "train loss:0.014120988088679887\n",
      "train loss:0.01014042418425048\n",
      "train loss:0.009816484868233905\n",
      "train loss:0.005702009572974246\n",
      "train loss:0.018621523753643593\n",
      "train loss:0.009160272831032863\n",
      "train loss:0.012889174169638256\n",
      "train loss:0.006029687834078038\n",
      "train loss:0.010507069827075189\n",
      "train loss:0.012262491283649604\n",
      "train loss:0.009285794093081978\n",
      "train loss:0.016988196387733367\n",
      "train loss:0.04828649611656587\n",
      "train loss:0.01571506180323666\n",
      "train loss:0.04900813172773727\n",
      "train loss:0.006766964584093924\n",
      "train loss:0.010411566642468088\n",
      "train loss:0.016871322166366116\n",
      "train loss:0.007574057831461178\n",
      "train loss:0.012780282581055165\n",
      "train loss:0.015651376220615347\n",
      "train loss:0.00796610257212411\n",
      "train loss:0.007475508080578824\n",
      "train loss:0.011810887068272024\n",
      "train loss:0.07508347800412717\n",
      "train loss:0.09227839179148598\n",
      "train loss:0.009036076859417515\n",
      "train loss:0.005758472858588256\n",
      "train loss:0.01763922679303829\n",
      "train loss:0.013625013526630491\n",
      "train loss:0.012617442609231462\n",
      "train loss:0.0159233244222894\n",
      "train loss:0.00926198718025959\n",
      "train loss:0.0052414625336889365\n",
      "train loss:0.0537971445869629\n",
      "train loss:0.02391174373062295\n",
      "train loss:0.025271847410179592\n",
      "train loss:0.012893883364124782\n",
      "train loss:0.011366382936285469\n",
      "train loss:0.014639436523366136\n",
      "train loss:0.01024924804199357\n",
      "train loss:0.012642986851992728\n",
      "train loss:0.013220791857329321\n",
      "train loss:0.015583697294600914\n",
      "train loss:0.03980985429447498\n",
      "train loss:0.12021695404651994\n",
      "train loss:0.008779632501030867\n",
      "train loss:0.008418077822285801\n",
      "train loss:0.011441085291640335\n",
      "train loss:0.010223280850675311\n",
      "train loss:0.006226720730324803\n",
      "train loss:0.04800796054372088\n",
      "train loss:0.015290530134103164\n",
      "train loss:0.03984738743235\n",
      "train loss:0.023776519584301905\n",
      "train loss:0.003008592030550328\n",
      "train loss:0.012590301851594826\n",
      "train loss:0.017499938820909253\n",
      "train loss:0.015103018186800994\n",
      "train loss:0.013019215642950065\n",
      "train loss:0.007855564845288094\n",
      "train loss:0.01715138219352089\n",
      "train loss:0.010356603383963438\n",
      "train loss:0.012501128787077037\n",
      "train loss:0.007696387292864316\n",
      "train loss:0.024874593427172294\n",
      "train loss:0.008646529696583076\n",
      "train loss:0.007146258118539327\n",
      "train loss:0.0333575975107869\n",
      "train loss:0.013512655860052845\n",
      "train loss:0.04366089563086026\n",
      "train loss:0.008792675637473652\n",
      "train loss:0.015193480692319326\n",
      "train loss:0.005148696017404975\n",
      "train loss:0.013303398333534644\n",
      "train loss:0.009460888093061877\n",
      "train loss:0.023259827240860207\n",
      "train loss:0.01126593845196161\n",
      "train loss:0.008530539526840873\n",
      "train loss:0.0716000927470331\n",
      "train loss:0.04084741327559059\n",
      "train loss:0.007820084796612172\n",
      "train loss:0.005579023359364605\n",
      "train loss:0.006008600072733513\n",
      "train loss:0.009625607202914692\n",
      "train loss:0.02860433916524073\n",
      "train loss:0.013029635703913476\n",
      "train loss:0.01067022030709095\n",
      "train loss:0.011697113720355177\n",
      "train loss:0.02665240166646182\n",
      "train loss:0.013764393852695173\n",
      "train loss:0.009710123513614007\n",
      "train loss:0.049571396542853224\n",
      "train loss:0.006984887402671432\n",
      "train loss:0.018298952504940055\n",
      "train loss:0.006084313327654295\n",
      "train loss:0.012505319979894871\n",
      "train loss:0.03846869364606628\n",
      "train loss:0.022899501751108914\n",
      "train loss:0.020835658930109403\n",
      "train loss:0.006034996399600582\n",
      "train loss:0.007210638513700144\n",
      "train loss:0.01578422404181056\n",
      "train loss:0.011649394710188452\n",
      "train loss:0.016813326551332036\n",
      "train loss:0.03343974592421022\n",
      "train loss:0.003800579869846253\n",
      "train loss:0.015397597849643539\n",
      "train loss:0.01773556523680164\n",
      "train loss:0.013368329986761336\n",
      "train loss:0.04551501362402741\n",
      "train loss:0.0146669908362718\n",
      "train loss:0.01409770916616524\n",
      "train loss:0.008400723821994508\n",
      "train loss:0.011687804042641563\n",
      "train loss:0.01762441629939154\n",
      "train loss:0.029957564463240875\n",
      "train loss:0.018726614238792218\n",
      "train loss:0.014589382792198458\n",
      "train loss:0.015043365113569834\n",
      "train loss:0.009830522231872186\n",
      "train loss:0.005228954516643411\n",
      "train loss:0.026122468031033522\n",
      "train loss:0.008299994193478157\n",
      "train loss:0.010878496664065153\n",
      "train loss:0.011096105302073233\n",
      "train loss:0.01069675583627848\n",
      "train loss:0.013816074248067125\n",
      "train loss:0.02294241095936686\n",
      "train loss:0.006705807444099798\n",
      "train loss:0.015547902875360507\n",
      "train loss:0.005690514459503677\n",
      "train loss:0.03750270211407058\n",
      "train loss:0.029096798491880488\n",
      "train loss:0.008902670235004895\n",
      "train loss:0.016375840008758344\n",
      "train loss:0.015245813205456054\n",
      "train loss:0.006241771162817033\n",
      "train loss:0.005811318901238154\n",
      "train loss:0.007132426959690455\n",
      "train loss:0.009765643338997904\n",
      "train loss:0.05638318886781371\n",
      "train loss:0.01773886570571216\n",
      "train loss:0.008645830579610596\n",
      "train loss:0.015654734208505324\n",
      "train loss:0.007571531446049026\n",
      "train loss:0.011125517355652623\n",
      "train loss:0.011365512439342284\n",
      "train loss:0.006576309366979686\n",
      "train loss:0.012025794233660443\n",
      "train loss:0.0056429725573740545\n",
      "train loss:0.015924152181835852\n",
      "train loss:0.005986222358161724\n",
      "train loss:0.010376314711547832\n",
      "train loss:0.024162280076063792\n",
      "train loss:0.009553056248826625\n",
      "train loss:0.055662424201385126\n",
      "train loss:0.042058168660229434\n",
      "train loss:0.01645542245081694\n",
      "train loss:0.030527323493483173\n",
      "train loss:0.01758595120828977\n",
      "train loss:0.009401590200023026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0033847051172057035\n",
      "train loss:0.015779419708780818\n",
      "train loss:0.014628205516381403\n",
      "train loss:0.020106549459366123\n",
      "train loss:0.0308233836009366\n",
      "train loss:0.011838467968860362\n",
      "train loss:0.008392343044261749\n",
      "train loss:0.019720148485621914\n",
      "train loss:0.010679016090995428\n",
      "train loss:0.007176161507001015\n",
      "train loss:0.008376230623831358\n",
      "train loss:0.01285363498135774\n",
      "train loss:0.014916697685674732\n",
      "train loss:0.00884624756924348\n",
      "train loss:0.023926042435877894\n",
      "train loss:0.009316165872135058\n",
      "train loss:0.021734964150954553\n",
      "train loss:0.006468100671785079\n",
      "train loss:0.036134964063583644\n",
      "train loss:0.009843674727135283\n",
      "train loss:0.10269128740376253\n",
      "train loss:0.03374479194727599\n",
      "train loss:0.02791294282273769\n",
      "train loss:0.0205903657963585\n",
      "train loss:0.014993505989642495\n",
      "train loss:0.01574166690146833\n",
      "train loss:0.010953449874139076\n",
      "train loss:0.008758929693052418\n",
      "train loss:0.01816172786170893\n",
      "train loss:0.011805122705520659\n",
      "train loss:0.015303087630712283\n",
      "train loss:0.012464773741567065\n",
      "train loss:0.009131599305115723\n",
      "train loss:0.009028105799764345\n",
      "train loss:0.018028544145780933\n",
      "train loss:0.005926176975690049\n",
      "train loss:0.02487731845284632\n",
      "train loss:0.011502490190268513\n",
      "train loss:0.01142216699270445\n",
      "train loss:0.006593096769896952\n",
      "train loss:0.008046680326731647\n",
      "train loss:0.00737473403478227\n",
      "train loss:0.014635309146510607\n",
      "train loss:0.014416481581423947\n",
      "train loss:0.014867376024062786\n",
      "train loss:0.013816855352255013\n",
      "train loss:0.006808146047071708\n",
      "train loss:0.01757710565559356\n",
      "train loss:0.021524255911744342\n",
      "train loss:0.02146586991892315\n",
      "train loss:0.01070319773395678\n",
      "train loss:0.008394063988600112\n",
      "train loss:0.057906095027815185\n",
      "train loss:0.015177891628586042\n",
      "train loss:0.03332084738871459\n",
      "train loss:0.0069728856927955985\n",
      "train loss:0.015431120906541784\n",
      "train loss:0.008250961474603316\n",
      "train loss:0.014054240853396684\n",
      "train loss:0.031569352288937515\n",
      "train loss:0.015649453848724758\n",
      "train loss:0.013112604854421108\n",
      "train loss:0.006264926725344197\n",
      "train loss:0.009726965069107953\n",
      "train loss:0.02984485244860072\n",
      "train loss:0.019525152152295923\n",
      "train loss:0.011744874818469666\n",
      "train loss:0.009603664026222808\n",
      "train loss:0.01750768055369615\n",
      "train loss:0.00796116740596966\n",
      "train loss:0.022774417107426642\n",
      "train loss:0.02965913234683295\n",
      "train loss:0.013973886429609686\n",
      "train loss:0.008634071936862282\n",
      "train loss:0.017576724598709955\n",
      "train loss:0.008726982678149732\n",
      "train loss:0.025038986159237376\n",
      "train loss:0.013168430090300559\n",
      "train loss:0.010206161232554946\n",
      "train loss:0.018954005734940008\n",
      "train loss:0.007970637214583857\n",
      "train loss:0.012050260820809866\n",
      "train loss:0.007456799525303077\n",
      "train loss:0.028376370709709716\n",
      "train loss:0.0094750747698995\n",
      "train loss:0.008796346736300168\n",
      "train loss:0.026934937890084082\n",
      "train loss:0.007908166999899132\n",
      "train loss:0.023407190470167972\n",
      "train loss:0.006153852768238853\n",
      "train loss:0.007462487082976302\n",
      "train loss:0.013700833767634014\n",
      "train loss:0.02634748990361502\n",
      "train loss:0.021432443749361912\n",
      "train loss:0.027824556140587885\n",
      "train loss:0.007606925479473855\n",
      "train loss:0.039120549384698916\n",
      "train loss:0.013876392987081964\n",
      "train loss:0.01380038185778305\n",
      "train loss:0.006095745216699965\n",
      "train loss:0.008125380090404687\n",
      "train loss:0.02095931798662092\n",
      "train loss:0.005960430440527675\n",
      "train loss:0.017976085874194248\n",
      "train loss:0.005374362077103305\n",
      "train loss:0.018688618677516124\n",
      "train loss:0.01836235356748598\n",
      "train loss:0.1679909154114909\n",
      "train loss:0.02855156524121947\n",
      "train loss:0.007510801161142946\n",
      "train loss:0.03922724139361605\n",
      "train loss:0.008551993839281966\n",
      "train loss:0.011945282529724228\n",
      "train loss:0.010298712970935908\n",
      "train loss:0.005519784214004774\n",
      "train loss:0.008286914881553675\n",
      "train loss:0.010741078631538267\n",
      "train loss:0.010230157398472313\n",
      "train loss:0.013797471159057327\n",
      "train loss:0.05246212579475365\n",
      "train loss:0.007307092116593692\n",
      "train loss:0.041692209563467114\n",
      "train loss:0.00939848959363601\n",
      "train loss:0.004734476164257172\n",
      "train loss:0.022511775250804632\n",
      "train loss:0.04118403334442796\n",
      "train loss:0.01088642746275097\n",
      "train loss:0.006821308662630896\n",
      "train loss:0.007192818748851985\n",
      "train loss:0.02266592853093247\n",
      "train loss:0.008701905030487855\n",
      "train loss:0.01943731562575327\n",
      "train loss:0.008752036942168015\n",
      "train loss:0.011681501120703063\n",
      "train loss:0.013295410769832476\n",
      "train loss:0.02399760934940029\n",
      "train loss:0.01055268578527856\n",
      "train loss:0.013553765696824658\n",
      "train loss:0.017908884256545066\n",
      "train loss:0.008066856440627065\n",
      "train loss:0.007246221603418799\n",
      "train loss:0.021839296578549963\n",
      "train loss:0.013237250163312468\n",
      "train loss:0.003935702372529611\n",
      "train loss:0.01630921988544741\n",
      "train loss:0.015066296607593557\n",
      "train loss:0.02788309611772708\n",
      "train loss:0.003017427784796731\n",
      "train loss:0.008709824341835322\n",
      "train loss:0.013602499342999499\n",
      "train loss:0.009108125275592313\n",
      "train loss:0.016089774527336544\n",
      "train loss:0.0665753559934548\n",
      "train loss:0.013059913455886587\n",
      "train loss:0.00587049091640237\n",
      "train loss:0.011656450862779269\n",
      "train loss:0.013401559998671325\n",
      "train loss:0.010212520256108103\n",
      "train loss:0.008557028453495636\n",
      "train loss:0.01951385787384068\n",
      "train loss:0.006895109590045197\n",
      "train loss:0.011890808031091262\n",
      "train loss:0.011152202694140755\n",
      "train loss:0.08521336418348752\n",
      "train loss:0.003645071972668963\n",
      "train loss:0.009583639913155393\n",
      "train loss:0.025480297947546946\n",
      "train loss:0.03036533973862889\n",
      "train loss:0.008917239523654882\n",
      "train loss:0.024525334351332386\n",
      "train loss:0.018392063865292935\n",
      "train loss:0.012257910832344378\n",
      "train loss:0.09163140021373772\n",
      "train loss:0.021882632182763036\n",
      "train loss:0.009803020994028433\n",
      "train loss:0.004189007738898857\n",
      "=== epoch:17, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.053031669354534176\n",
      "train loss:0.012565488027712134\n",
      "train loss:0.010528247548794658\n",
      "train loss:0.013266217640390416\n",
      "train loss:0.04541393695323988\n",
      "train loss:0.018494451830673764\n",
      "train loss:0.04064611446586446\n",
      "train loss:0.007025449739317845\n",
      "train loss:0.00868734413540378\n",
      "train loss:0.059879545994315136\n",
      "train loss:0.0027646969703422892\n",
      "train loss:0.011482116105295705\n",
      "train loss:0.010778347241284696\n",
      "train loss:0.006656518673531902\n",
      "train loss:0.019083735164662687\n",
      "train loss:0.08155667877351477\n",
      "train loss:0.020769944310248298\n",
      "train loss:0.010338784847312916\n",
      "train loss:0.005206946741240666\n",
      "train loss:0.0406002970531562\n",
      "train loss:0.011244158679304086\n",
      "train loss:0.007799210645040168\n",
      "train loss:0.012863287973394087\n",
      "train loss:0.010716718577492863\n",
      "train loss:0.022646330635091916\n",
      "train loss:0.012044882288930702\n",
      "train loss:0.005724348752831759\n",
      "train loss:0.08082250123620176\n",
      "train loss:0.019105075059727267\n",
      "train loss:0.008458753299470613\n",
      "train loss:0.010947699904879317\n",
      "train loss:0.003954062607410209\n",
      "train loss:0.012089415014035865\n",
      "train loss:0.011748926391306027\n",
      "train loss:0.003491047861206538\n",
      "train loss:0.007221264200464888\n",
      "train loss:0.009834097249313811\n",
      "train loss:0.009919539454669234\n",
      "train loss:0.013263895810720332\n",
      "train loss:0.01533964136378517\n",
      "train loss:0.011719142822727244\n",
      "train loss:0.017772592310449465\n",
      "train loss:0.012289327387666344\n",
      "train loss:0.012641341887268877\n",
      "train loss:0.027422707157763897\n",
      "train loss:0.007797675059128364\n",
      "train loss:0.007225090319119699\n",
      "train loss:0.011010194560604392\n",
      "train loss:0.014046902874609775\n",
      "train loss:0.007699546341288732\n",
      "train loss:0.01696969563560872\n",
      "train loss:0.00756511330868975\n",
      "train loss:0.0077523708185419735\n",
      "train loss:0.00431676910780483\n",
      "train loss:0.00960179115168579\n",
      "train loss:0.04593729987088383\n",
      "train loss:0.006050268142104441\n",
      "train loss:0.008394587395364021\n",
      "train loss:0.050946094520901\n",
      "train loss:0.04614653336911743\n",
      "train loss:0.04571364191420889\n",
      "train loss:0.017005806617479174\n",
      "train loss:0.012629899797565219\n",
      "train loss:0.015604305099336097\n",
      "train loss:0.02236522496522543\n",
      "train loss:0.05463954452426786\n",
      "train loss:0.010266466770174766\n",
      "train loss:0.006038367765463767\n",
      "train loss:0.018259937266693293\n",
      "train loss:0.015720561857516593\n",
      "train loss:0.010491358140813326\n",
      "train loss:0.022851925499400832\n",
      "train loss:0.00754952348768307\n",
      "train loss:0.010446296241834773\n",
      "train loss:0.04269259750853864\n",
      "train loss:0.024532222597494914\n",
      "train loss:0.020855375669629632\n",
      "train loss:0.02721141244794147\n",
      "train loss:0.02265821108131667\n",
      "train loss:0.0132961728435759\n",
      "train loss:0.010220371090156936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.028995823103184895\n",
      "train loss:0.04045194744692985\n",
      "train loss:0.01367800122710195\n",
      "train loss:0.020532140886100118\n",
      "train loss:0.006175308812597481\n",
      "train loss:0.009708309920881132\n",
      "train loss:0.03293191063639913\n",
      "train loss:0.023115950697583317\n",
      "train loss:0.03807347503596781\n",
      "train loss:0.048311293626095854\n",
      "train loss:0.00940333149428084\n",
      "train loss:0.008473770824777642\n",
      "train loss:0.07864610788731652\n",
      "train loss:0.01340249386685681\n",
      "train loss:0.013179750964009031\n",
      "train loss:0.026063474068373463\n",
      "train loss:0.02404663427687146\n",
      "train loss:0.005933239255110104\n",
      "train loss:0.012278161831585331\n",
      "train loss:0.014750490743759536\n",
      "train loss:0.058843680155122674\n",
      "train loss:0.017239088989296207\n",
      "train loss:0.016830980138624586\n",
      "train loss:0.028870418532364973\n",
      "train loss:0.012499415294359521\n",
      "train loss:0.0074756526521517494\n",
      "train loss:0.004197376128550036\n",
      "train loss:0.00428007081544322\n",
      "train loss:0.014896195054044102\n",
      "train loss:0.006860843403287866\n",
      "train loss:0.01361624977260825\n",
      "train loss:0.05435498838598943\n",
      "train loss:0.02558555109800402\n",
      "train loss:0.013567163360369146\n",
      "train loss:0.020558772157686676\n",
      "train loss:0.029485535737082697\n",
      "train loss:0.05869418689465926\n",
      "train loss:0.034335328550660876\n",
      "train loss:0.012142043552278001\n",
      "train loss:0.011059197663474322\n",
      "train loss:0.030497293682751798\n",
      "train loss:0.014727104001202761\n",
      "train loss:0.09485654035604915\n",
      "train loss:0.019205982252112803\n",
      "train loss:0.007933615706915946\n",
      "train loss:0.012133576126245948\n",
      "train loss:0.007395025350551032\n",
      "train loss:0.015609217510435256\n",
      "train loss:0.017891831260500087\n",
      "train loss:0.0047471369613605405\n",
      "train loss:0.007056241858466841\n",
      "train loss:0.016863708109487566\n",
      "train loss:0.02977854518427757\n",
      "train loss:0.016035757992926593\n",
      "train loss:0.06867862702725998\n",
      "train loss:0.05202369626951873\n",
      "train loss:0.004797050401263808\n",
      "train loss:0.0059710736719668376\n",
      "train loss:0.010255304809273849\n",
      "train loss:0.006536403396919009\n",
      "train loss:0.05442853292547265\n",
      "train loss:0.016660762080768648\n",
      "train loss:0.003209023981683094\n",
      "train loss:0.029752320111669305\n",
      "train loss:0.010353620532106686\n",
      "train loss:0.02417994695661041\n",
      "train loss:0.007574156110938619\n",
      "train loss:0.009804480400205956\n",
      "train loss:0.01583036395159804\n",
      "train loss:0.010947959641527076\n",
      "train loss:0.003494154282712523\n",
      "train loss:0.008444586373821275\n",
      "train loss:0.006269646787898969\n",
      "train loss:0.03012021555580956\n",
      "train loss:0.01439810469655387\n",
      "train loss:0.01973598669448387\n",
      "train loss:0.02108089289869557\n",
      "train loss:0.012837162242539592\n",
      "train loss:0.01022133691387714\n",
      "train loss:0.03575642223222519\n",
      "train loss:0.01293021660222593\n",
      "train loss:0.007760016775251859\n",
      "train loss:0.02257059194454157\n",
      "train loss:0.010264156999989577\n",
      "train loss:0.011212164506598716\n",
      "train loss:0.0462428376240329\n",
      "train loss:0.014430055259687047\n",
      "train loss:0.010538436480996196\n",
      "train loss:0.015899742759140446\n",
      "train loss:0.002463450445622331\n",
      "train loss:0.020784938205812384\n",
      "train loss:0.0030615959570875384\n",
      "train loss:0.008327680515046369\n",
      "train loss:0.004484525339950991\n",
      "train loss:0.0317131453286347\n",
      "train loss:0.01444947964130085\n",
      "train loss:0.024185431685998054\n",
      "train loss:0.020720768849034166\n",
      "train loss:0.013490811964808145\n",
      "train loss:0.013606627684363347\n",
      "train loss:0.014114994782725921\n",
      "train loss:0.02823193925708913\n",
      "train loss:0.014263509018257944\n",
      "train loss:0.00840067311683993\n",
      "train loss:0.00582512123249134\n",
      "train loss:0.013396886250687223\n",
      "train loss:0.020200178965154178\n",
      "train loss:0.09148605016023766\n",
      "train loss:0.042745075252139414\n",
      "train loss:0.012550449645632915\n",
      "train loss:0.010721710821789902\n",
      "train loss:0.01588185085794753\n",
      "train loss:0.015376851961062279\n",
      "train loss:0.0213709862992999\n",
      "train loss:0.011533959751136458\n",
      "train loss:0.018630170460914888\n",
      "train loss:0.1565265434828555\n",
      "train loss:0.019787256935828345\n",
      "train loss:0.013272900877845117\n",
      "train loss:0.016401862867387956\n",
      "train loss:0.04215249636934788\n",
      "train loss:0.011462153206418193\n",
      "train loss:0.004973771601541729\n",
      "train loss:0.015367443288710618\n",
      "train loss:0.02283944540652581\n",
      "train loss:0.006791383587331156\n",
      "train loss:0.014436885076676065\n",
      "train loss:0.02026076077537407\n",
      "train loss:0.006622929846041244\n",
      "train loss:0.016742211175685168\n",
      "train loss:0.014161211181244492\n",
      "train loss:0.0070249049125622074\n",
      "train loss:0.16705528333460357\n",
      "train loss:0.014699525586028838\n",
      "train loss:0.014887430960304194\n",
      "train loss:0.008364350870502107\n",
      "train loss:0.01350370094218771\n",
      "train loss:0.03279529124162621\n",
      "train loss:0.004948993732665208\n",
      "train loss:0.008804786990196002\n",
      "train loss:0.039279975931416085\n",
      "train loss:0.00581564875705039\n",
      "train loss:0.010570670814514481\n",
      "train loss:0.013605380878584624\n",
      "train loss:0.025982872658333836\n",
      "train loss:0.012620221351283321\n",
      "train loss:0.016903447953127287\n",
      "train loss:0.009693060992903905\n",
      "train loss:0.020754823729268887\n",
      "train loss:0.014186089764894504\n",
      "train loss:0.006491871763959286\n",
      "train loss:0.007836142558260468\n",
      "train loss:0.012068057053171185\n",
      "train loss:0.02462117123476042\n",
      "train loss:0.008548873890075453\n",
      "train loss:0.0021634795652097664\n",
      "train loss:0.014540003578875034\n",
      "train loss:0.037946652070249304\n",
      "train loss:0.02524400043396736\n",
      "train loss:0.010184636235166975\n",
      "train loss:0.02278238848277209\n",
      "train loss:0.014632338935680752\n",
      "train loss:0.012128932776901101\n",
      "train loss:0.002791515057171206\n",
      "train loss:0.04596589072436981\n",
      "train loss:0.026558601363353182\n",
      "train loss:0.007312920208696552\n",
      "train loss:0.011966605234312115\n",
      "train loss:0.014202494278688645\n",
      "train loss:0.017137647783795517\n",
      "train loss:0.007952094707648709\n",
      "train loss:0.012045417081528252\n",
      "train loss:0.008808527708935643\n",
      "train loss:0.008499476729706797\n",
      "train loss:0.012897247412090656\n",
      "train loss:0.1644513845358506\n",
      "train loss:0.016809503056477684\n",
      "train loss:0.03206272350086159\n",
      "train loss:0.023855329123601065\n",
      "train loss:0.018790566446859367\n",
      "train loss:0.032766796852392276\n",
      "train loss:0.010550993804177925\n",
      "train loss:0.023967660270408805\n",
      "train loss:0.0035850498191238992\n",
      "train loss:0.008973726826274146\n",
      "train loss:0.018704826990326023\n",
      "train loss:0.014104034597566513\n",
      "train loss:0.007863919992414576\n",
      "train loss:0.0320352639971204\n",
      "train loss:0.015708087563343517\n",
      "train loss:0.008058018239757181\n",
      "train loss:0.0071473315314410915\n",
      "train loss:0.008633177296121633\n",
      "train loss:0.01359864018484683\n",
      "train loss:0.010235470269275813\n",
      "train loss:0.009896080957549354\n",
      "train loss:0.017320878199875787\n",
      "train loss:0.1480408881557805\n",
      "train loss:0.017605010962562853\n",
      "train loss:0.006927651573920598\n",
      "train loss:0.016027613624270177\n",
      "train loss:0.005697685312247675\n",
      "train loss:0.01930577655587056\n",
      "train loss:0.008087935108539425\n",
      "train loss:0.03173599530308372\n",
      "train loss:0.01036089945026985\n",
      "train loss:0.017118096726887876\n",
      "train loss:0.005685298014851993\n",
      "train loss:0.013236152758607964\n",
      "train loss:0.020074324180373426\n",
      "train loss:0.005183271632081381\n",
      "train loss:0.010442367093972851\n",
      "train loss:0.014835480234068775\n",
      "train loss:0.02088593296936159\n",
      "train loss:0.007326697451614077\n",
      "train loss:0.03126260487014633\n",
      "train loss:0.009376778258349315\n",
      "train loss:0.021672235569883135\n",
      "train loss:0.009032863613241626\n",
      "train loss:0.0056270078658321905\n",
      "train loss:0.015815719806712435\n",
      "train loss:0.007855368950075515\n",
      "train loss:0.03221080024990781\n",
      "train loss:0.00838080867232535\n",
      "train loss:0.009735343646644799\n",
      "train loss:0.01938450489025183\n",
      "train loss:0.01856725229542317\n",
      "train loss:0.010263100432269374\n",
      "train loss:0.005811126566165629\n",
      "train loss:0.02214352174703519\n",
      "train loss:0.01431783339197731\n",
      "train loss:0.0037749226049789415\n",
      "train loss:0.009528228992065079\n",
      "train loss:0.010176296632372158\n",
      "train loss:0.03743420927524896\n",
      "train loss:0.010945021411381586\n",
      "train loss:0.011898321534733157\n",
      "train loss:0.04236838143912996\n",
      "train loss:0.02196274695104159\n",
      "train loss:0.007401187487109762\n",
      "train loss:0.01850988548934786\n",
      "train loss:0.016185374536983297\n",
      "train loss:0.013289673366092568\n",
      "train loss:0.022273422411392425\n",
      "train loss:0.01206803748048511\n",
      "train loss:0.04925684448276142\n",
      "train loss:0.007060891423127983\n",
      "train loss:0.01158009319331847\n",
      "train loss:0.027012976784417116\n",
      "train loss:0.00758160449476896\n",
      "train loss:0.005287987478645243\n",
      "train loss:0.009146566878392402\n",
      "train loss:0.009388577943557349\n",
      "train loss:0.01427535413220241\n",
      "train loss:0.005602085245689619\n",
      "train loss:0.003292764323854932\n",
      "train loss:0.009110813500613102\n",
      "train loss:0.013042652852200963\n",
      "train loss:0.0075176733916462925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.009374162006178709\n",
      "train loss:0.00867166969248788\n",
      "train loss:0.024448782255834977\n",
      "train loss:0.04777025997000358\n",
      "train loss:0.0057834528615326375\n",
      "train loss:0.015111857377715041\n",
      "train loss:0.013864256814581278\n",
      "train loss:0.02644651673152048\n",
      "train loss:0.011342446640661356\n",
      "train loss:0.01722403928353042\n",
      "train loss:0.01871293776058511\n",
      "train loss:0.04034473834190841\n",
      "train loss:0.018926803870561254\n",
      "train loss:0.012988593467990271\n",
      "train loss:0.004522849512583196\n",
      "train loss:0.010251402339519686\n",
      "train loss:0.023853872498956007\n",
      "train loss:0.006352372217256267\n",
      "train loss:0.007018318268190556\n",
      "train loss:0.012152820227042475\n",
      "train loss:0.022632783816946275\n",
      "train loss:0.009441722880783874\n",
      "train loss:0.013915580396226765\n",
      "train loss:0.009784952789040924\n",
      "train loss:0.057092185608643466\n",
      "train loss:0.012065264598665573\n",
      "train loss:0.014838335800361858\n",
      "train loss:0.026958933494038648\n",
      "train loss:0.037563364758877664\n",
      "train loss:0.008845164334759303\n",
      "train loss:0.03621313925350134\n",
      "train loss:0.00945075931693778\n",
      "train loss:0.027218619931260023\n",
      "train loss:0.03914745052974525\n",
      "train loss:0.009404043827821871\n",
      "train loss:0.0360123187169222\n",
      "train loss:0.0071742500399515205\n",
      "train loss:0.020615980537156915\n",
      "train loss:0.011576327026375184\n",
      "train loss:0.0896041896986162\n",
      "train loss:0.022989973612372726\n",
      "train loss:0.005561691121232572\n",
      "train loss:0.009947048867884733\n",
      "train loss:0.009416515859807755\n",
      "train loss:0.025387872789314366\n",
      "train loss:0.024939059940094332\n",
      "train loss:0.010110878305395098\n",
      "train loss:0.015296787094283677\n",
      "train loss:0.014436617011485083\n",
      "train loss:0.009308639452118517\n",
      "train loss:0.023287957670135282\n",
      "train loss:0.006531416721800375\n",
      "train loss:0.015179259969078287\n",
      "train loss:0.01676003651418268\n",
      "train loss:0.021016252780806512\n",
      "train loss:0.014905134993023754\n",
      "train loss:0.01437235697207627\n",
      "train loss:0.03399819042751015\n",
      "train loss:0.00956306106868477\n",
      "train loss:0.009301627325214565\n",
      "train loss:0.003527835321840331\n",
      "train loss:0.018539205354724576\n",
      "train loss:0.005100018719186921\n",
      "train loss:0.016218394187564215\n",
      "train loss:0.013242673812927888\n",
      "train loss:0.01095003072849934\n",
      "train loss:0.030205073711625303\n",
      "train loss:0.015450505348934813\n",
      "train loss:0.0440427845690578\n",
      "train loss:0.014432943337606783\n",
      "train loss:0.05984912607296625\n",
      "train loss:0.01703339863552107\n",
      "train loss:0.006463773206973713\n",
      "train loss:0.030160574703067287\n",
      "train loss:0.007871808401592408\n",
      "train loss:0.026817106010751823\n",
      "train loss:0.006081987826695222\n",
      "train loss:0.01586023466709233\n",
      "train loss:0.01614073242180335\n",
      "train loss:0.009868042903477337\n",
      "train loss:0.01429804443055774\n",
      "train loss:0.017314939814111915\n",
      "train loss:0.02591123398728523\n",
      "train loss:0.04846986333582335\n",
      "train loss:0.006294880403550347\n",
      "train loss:0.004888303599887769\n",
      "train loss:0.006194120799826478\n",
      "train loss:0.007513625913479073\n",
      "train loss:0.016141329611961942\n",
      "train loss:0.019718121373839163\n",
      "train loss:0.02184112545025322\n",
      "train loss:0.029724395919822975\n",
      "train loss:0.006753331280889682\n",
      "train loss:0.008498235434175848\n",
      "train loss:0.007332741699927181\n",
      "train loss:0.021107034295123096\n",
      "train loss:0.011107839182491537\n",
      "train loss:0.008100375573550813\n",
      "train loss:0.01716490417980106\n",
      "train loss:0.01521025757147133\n",
      "train loss:0.01522736929297695\n",
      "train loss:0.03840833703779041\n",
      "train loss:0.011653254077325472\n",
      "train loss:0.014430748639076873\n",
      "train loss:0.012672580565717701\n",
      "train loss:0.013683525618410455\n",
      "train loss:0.015618481175547254\n",
      "train loss:0.0219422255591613\n",
      "train loss:0.02457272906465622\n",
      "train loss:0.005521728828696023\n",
      "=== epoch:18, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.015538992045593695\n",
      "train loss:0.01981599996068828\n",
      "train loss:0.008247326872435162\n",
      "train loss:0.012808623556319914\n",
      "train loss:0.0038801020794481255\n",
      "train loss:0.03508494197011947\n",
      "train loss:0.015474772657175036\n",
      "train loss:0.00866692879588083\n",
      "train loss:0.007566736893636432\n",
      "train loss:0.004362229685531047\n",
      "train loss:0.020865149842015276\n",
      "train loss:0.02692939656235446\n",
      "train loss:0.01073189874679637\n",
      "train loss:0.02336572945894092\n",
      "train loss:0.006374243248966193\n",
      "train loss:0.020529759465711422\n",
      "train loss:0.0032887419518722676\n",
      "train loss:0.019479076651052068\n",
      "train loss:0.01640642256182055\n",
      "train loss:0.0045460077514697375\n",
      "train loss:0.029547220645922457\n",
      "train loss:0.0358496002779914\n",
      "train loss:0.0037074979057335564\n",
      "train loss:0.013505378086492895\n",
      "train loss:0.011365773266930335\n",
      "train loss:0.010315664472778239\n",
      "train loss:0.010511314216171987\n",
      "train loss:0.003901254038246908\n",
      "train loss:0.005405870905610535\n",
      "train loss:0.006222742372379773\n",
      "train loss:0.0136637491100956\n",
      "train loss:0.012226322005632737\n",
      "train loss:0.017897912363141182\n",
      "train loss:0.0028245001332786357\n",
      "train loss:0.03389921285632722\n",
      "train loss:0.019654077137127107\n",
      "train loss:0.04436967690694225\n",
      "train loss:0.00948986715262318\n",
      "train loss:0.010749812311563913\n",
      "train loss:0.019967770606153387\n",
      "train loss:0.010587190444116703\n",
      "train loss:0.021838254214246074\n",
      "train loss:0.005207769423660258\n",
      "train loss:0.011580623620825894\n",
      "train loss:0.008255584327926236\n",
      "train loss:0.011887605042467098\n",
      "train loss:0.008469896057099698\n",
      "train loss:0.010637759447817995\n",
      "train loss:0.01290842929725161\n",
      "train loss:0.08384832853996996\n",
      "train loss:0.023407006748220674\n",
      "train loss:0.01440690943236271\n",
      "train loss:0.015564444401823195\n",
      "train loss:0.011777525504078396\n",
      "train loss:0.0067718708177356346\n",
      "train loss:0.017114374080330407\n",
      "train loss:0.013602535844132154\n",
      "train loss:0.018398864919553053\n",
      "train loss:0.01295874316180017\n",
      "train loss:0.028492813472394043\n",
      "train loss:0.016806204865310793\n",
      "train loss:0.005157160558504669\n",
      "train loss:0.019598355014953002\n",
      "train loss:0.008618603437304856\n",
      "train loss:0.015008478318406033\n",
      "train loss:0.022196608383083728\n",
      "train loss:0.008345337192427454\n",
      "train loss:0.006414598084545491\n",
      "train loss:0.006085353366029114\n",
      "train loss:0.010173663264829167\n",
      "train loss:0.01980873250628111\n",
      "train loss:0.015909880582848698\n",
      "train loss:0.010989019329233114\n",
      "train loss:0.021516281628458613\n",
      "train loss:0.0067559741824877095\n",
      "train loss:0.009677362294579345\n",
      "train loss:0.013183148506858332\n",
      "train loss:0.011056973712116749\n",
      "train loss:0.014896417099867722\n",
      "train loss:0.01120188361309907\n",
      "train loss:0.004374574760171282\n",
      "train loss:0.02935485893329093\n",
      "train loss:0.018147413883508122\n",
      "train loss:0.00947161684057134\n",
      "train loss:0.02450120201848844\n",
      "train loss:0.010029458105765809\n",
      "train loss:0.013861428195488705\n",
      "train loss:0.04261980792955894\n",
      "train loss:0.025239361827830212\n",
      "train loss:0.016499417161817114\n",
      "train loss:0.009715405972809536\n",
      "train loss:0.0073660194398024025\n",
      "train loss:0.010285916889307896\n",
      "train loss:0.005279990857995758\n",
      "train loss:0.007844108696629343\n",
      "train loss:0.03996800753733917\n",
      "train loss:0.011330317125316937\n",
      "train loss:0.00480874372166994\n",
      "train loss:0.011676753219682787\n",
      "train loss:0.009802883488339943\n",
      "train loss:0.010419846148783884\n",
      "train loss:0.009443731378206824\n",
      "train loss:0.048102192494856914\n",
      "train loss:0.009533453882943625\n",
      "train loss:0.011975807608964939\n",
      "train loss:0.006055131727073446\n",
      "train loss:0.07883798594705543\n",
      "train loss:0.016911697278212342\n",
      "train loss:0.004446668305416009\n",
      "train loss:0.00394051552013082\n",
      "train loss:0.0362839268459086\n",
      "train loss:0.028597449038979983\n",
      "train loss:0.011670787297603673\n",
      "train loss:0.008251279659765519\n",
      "train loss:0.007611319621795665\n",
      "train loss:0.026750203025339276\n",
      "train loss:0.011346300469747186\n",
      "train loss:0.02843060031906216\n",
      "train loss:0.011764817191398635\n",
      "train loss:0.026026323744547455\n",
      "train loss:0.07052349272824636\n",
      "train loss:0.01817695478462698\n",
      "train loss:0.005640880235668136\n",
      "train loss:0.015350631557451372\n",
      "train loss:0.009140911078556932\n",
      "train loss:0.009495137860132412\n",
      "train loss:0.023675256330728804\n",
      "train loss:0.009527035575718186\n",
      "train loss:0.02215332178101239\n",
      "train loss:0.018226415264465248\n",
      "train loss:0.01470957515674731\n",
      "train loss:0.017651968193126758\n",
      "train loss:0.023289732144846215\n",
      "train loss:0.006797652391639311\n",
      "train loss:0.01094713261595072\n",
      "train loss:0.010638122346485748\n",
      "train loss:0.01908498161994544\n",
      "train loss:0.008412081124684816\n",
      "train loss:0.009370658701883914\n",
      "train loss:0.008883967225308181\n",
      "train loss:0.011550109491851542\n",
      "train loss:0.021068651597235283\n",
      "train loss:0.018429960374852627\n",
      "train loss:0.015426560002355833\n",
      "train loss:0.03074218982920128\n",
      "train loss:0.03920854027827259\n",
      "train loss:0.008253556389854014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.015120275980039077\n",
      "train loss:0.005575554080189666\n",
      "train loss:0.012750225557598202\n",
      "train loss:0.05440057022268118\n",
      "train loss:0.007340147160474736\n",
      "train loss:0.011340076044600793\n",
      "train loss:0.03645841127316309\n",
      "train loss:0.007538151240317924\n",
      "train loss:0.005238710560278972\n",
      "train loss:0.012808468728600393\n",
      "train loss:0.006822448530718711\n",
      "train loss:0.022997333122591345\n",
      "train loss:0.009019114377991142\n",
      "train loss:0.004834545919662136\n",
      "train loss:0.007581830890781578\n",
      "train loss:0.01945562306331813\n",
      "train loss:0.00859821212543793\n",
      "train loss:0.013496910178406613\n",
      "train loss:0.013608659000650315\n",
      "train loss:0.01769988420843619\n",
      "train loss:0.00741154744467511\n",
      "train loss:0.004202990332860954\n",
      "train loss:0.01201593963787567\n",
      "train loss:0.01620899389417238\n",
      "train loss:0.013456307312218481\n",
      "train loss:0.018076570360146524\n",
      "train loss:0.004597038523820387\n",
      "train loss:0.035928355180152256\n",
      "train loss:0.010466595773058007\n",
      "train loss:0.041429441666269724\n",
      "train loss:0.025508371541039493\n",
      "train loss:0.05928053881222952\n",
      "train loss:0.059073912201297755\n",
      "train loss:0.004122617468959933\n",
      "train loss:0.03265152212511366\n",
      "train loss:0.14714983010799887\n",
      "train loss:0.010894828553193696\n",
      "train loss:0.007369051663146369\n",
      "train loss:0.008468523129001983\n",
      "train loss:0.004306376131571901\n",
      "train loss:0.011376209088856788\n",
      "train loss:0.010476082151449143\n",
      "train loss:0.024889184020563993\n",
      "train loss:0.020710175293177625\n",
      "train loss:0.007869379680246414\n",
      "train loss:0.01957561849516871\n",
      "train loss:0.013037166377614384\n",
      "train loss:0.014478624428835742\n",
      "train loss:0.02550193195990949\n",
      "train loss:0.02160697134214246\n",
      "train loss:0.008631940912038845\n",
      "train loss:0.007623195259847935\n",
      "train loss:0.00873625693789238\n",
      "train loss:0.07610220294085679\n",
      "train loss:0.03288266520865164\n",
      "train loss:0.010993640385298415\n",
      "train loss:0.01302589564644969\n",
      "train loss:0.006403172945232537\n",
      "train loss:0.017950780773304184\n",
      "train loss:0.007433590468345881\n",
      "train loss:0.009451541739933997\n",
      "train loss:0.019641522234775093\n",
      "train loss:0.010219926923990793\n",
      "train loss:0.011058910123841265\n",
      "train loss:0.01509582238282733\n",
      "train loss:0.005143250890276766\n",
      "train loss:0.00711754640823284\n",
      "train loss:0.0757241377483387\n",
      "train loss:0.008753979875175219\n",
      "train loss:0.008530815740778484\n",
      "train loss:0.031242784933338118\n",
      "train loss:0.004870211850863882\n",
      "train loss:0.01324848830815492\n",
      "train loss:0.0052246980501664585\n",
      "train loss:0.012952866876138266\n",
      "train loss:0.005250538979535963\n",
      "train loss:0.015603793984578274\n",
      "train loss:0.011704688898977054\n",
      "train loss:0.006022016904460661\n",
      "train loss:0.029999968807367817\n",
      "train loss:0.024508942820502155\n",
      "train loss:0.044714962718490535\n",
      "train loss:0.014648129995786468\n",
      "train loss:0.016212627712695094\n",
      "train loss:0.003621178073567191\n",
      "train loss:0.010990053979060803\n",
      "train loss:0.013668611436141289\n",
      "train loss:0.018243302168172076\n",
      "train loss:0.009089655431797301\n",
      "train loss:0.00861933451252138\n",
      "train loss:0.014605757025768898\n",
      "train loss:0.034089123748627846\n",
      "train loss:0.020751306939677562\n",
      "train loss:0.012913765333918224\n",
      "train loss:0.015933687776283078\n",
      "train loss:0.01203159160196418\n",
      "train loss:0.03805225407571681\n",
      "train loss:0.006130245885115631\n",
      "train loss:0.011087371091793904\n",
      "train loss:0.020728423351344186\n",
      "train loss:0.018356046483975243\n",
      "train loss:0.012127917456950112\n",
      "train loss:0.07978010923445772\n",
      "train loss:0.00822155369613877\n",
      "train loss:0.006174284279093373\n",
      "train loss:0.003311615470014193\n",
      "train loss:0.021237045231248877\n",
      "train loss:0.004884943227877574\n",
      "train loss:0.03170531689485538\n",
      "train loss:0.00979711222223773\n",
      "train loss:0.009683731346253062\n",
      "train loss:0.01183541669723896\n",
      "train loss:0.012474335723722483\n",
      "train loss:0.005609846812700995\n",
      "train loss:0.008494876683258204\n",
      "train loss:0.007629177196988718\n",
      "train loss:0.011991371140317269\n",
      "train loss:0.005466245395590074\n",
      "train loss:0.00422937102395893\n",
      "train loss:0.007263557143432864\n",
      "train loss:0.005476897658320303\n",
      "train loss:0.012471458064137455\n",
      "train loss:0.03556618895965223\n",
      "train loss:0.025691990838366822\n",
      "train loss:0.027457293397928066\n",
      "train loss:0.021510612216497957\n",
      "train loss:0.009531190964200582\n",
      "train loss:0.012067845972268919\n",
      "train loss:0.008217457446957687\n",
      "train loss:0.007259994844864841\n",
      "train loss:0.08083092203099516\n",
      "train loss:0.015835449957520314\n",
      "train loss:0.007673606625296127\n",
      "train loss:0.02612312268859955\n",
      "train loss:0.007861561014066147\n",
      "train loss:0.020071331479805523\n",
      "train loss:0.014670577814945098\n",
      "train loss:0.011313336935393284\n",
      "train loss:0.037461684036838555\n",
      "train loss:0.011428165977550924\n",
      "train loss:0.006768554139496346\n",
      "train loss:0.005346778765782203\n",
      "train loss:0.02251969433178347\n",
      "train loss:0.006703883689618\n",
      "train loss:0.012460397985708532\n",
      "train loss:0.006098583520311335\n",
      "train loss:0.009204003822671924\n",
      "train loss:0.015327888503860035\n",
      "train loss:0.004979581649843037\n",
      "train loss:0.0024294774071815617\n",
      "train loss:0.03214426627836823\n",
      "train loss:0.008442557245150105\n",
      "train loss:0.01897256636905432\n",
      "train loss:0.020647188364516116\n",
      "train loss:0.02054251378480688\n",
      "train loss:0.00932680510015344\n",
      "train loss:0.04302755294529269\n",
      "train loss:0.0041210433911884675\n",
      "train loss:0.049443300453013504\n",
      "train loss:0.00852272524097189\n",
      "train loss:0.018313023350954437\n",
      "train loss:0.03350356878950253\n",
      "train loss:0.006518030553010686\n",
      "train loss:0.014303497318215891\n",
      "train loss:0.011548129029585434\n",
      "train loss:0.006310995728254162\n",
      "train loss:0.07820679135618132\n",
      "train loss:0.005236838400868365\n",
      "train loss:0.005713465025758151\n",
      "train loss:0.019390406960617536\n",
      "train loss:0.004941952635978721\n",
      "train loss:0.011834101591112174\n",
      "train loss:0.02102555168805261\n",
      "train loss:0.034657888073003046\n",
      "train loss:0.0059437828166451715\n",
      "train loss:0.030879754636935354\n",
      "train loss:0.017710516351489528\n",
      "train loss:0.0064550295591698816\n",
      "train loss:0.004665938859870566\n",
      "train loss:0.015345900948761588\n",
      "train loss:0.011711790162675704\n",
      "train loss:0.02557125903140675\n",
      "train loss:0.013113877146999124\n",
      "train loss:0.009427754374543401\n",
      "train loss:0.010880013875720242\n",
      "train loss:0.013857608084295927\n",
      "train loss:0.007118027896153729\n",
      "train loss:0.008370996019599727\n",
      "train loss:0.01375455836645187\n",
      "train loss:0.013629269306955993\n",
      "train loss:0.06958044544783007\n",
      "train loss:0.011139984307895947\n",
      "train loss:0.009637843341064366\n",
      "train loss:0.008151793753839245\n",
      "train loss:0.060730772823205735\n",
      "train loss:0.008454276996607361\n",
      "train loss:0.039735895481191205\n",
      "train loss:0.016344577669364743\n",
      "train loss:0.008373938650470238\n",
      "train loss:0.014622541387988804\n",
      "train loss:0.007565632341839802\n",
      "train loss:0.006963195862925029\n",
      "train loss:0.01361812385712896\n",
      "train loss:0.003894319463159429\n",
      "train loss:0.01668811361873067\n",
      "train loss:0.004264790263022254\n",
      "train loss:0.018598432359403252\n",
      "train loss:0.04460882639724086\n",
      "train loss:0.0094740291774578\n",
      "train loss:0.016092656071063798\n",
      "train loss:0.011520677020297677\n",
      "train loss:0.003747340780027966\n",
      "train loss:0.01712939776379673\n",
      "train loss:0.01667414319134757\n",
      "train loss:0.002927478398917725\n",
      "train loss:0.009204555212750256\n",
      "train loss:0.0637902733492908\n",
      "train loss:0.014804670499561112\n",
      "train loss:0.025385976751513422\n",
      "train loss:0.0268038070533589\n",
      "train loss:0.00392216668831463\n",
      "train loss:0.010698770226845242\n",
      "train loss:0.028523919790997734\n",
      "train loss:0.014063931693623436\n",
      "train loss:0.009108303361647368\n",
      "train loss:0.031036709799165077\n",
      "train loss:0.009185620445247913\n",
      "train loss:0.012329865768090127\n",
      "train loss:0.010216237531127666\n",
      "train loss:0.023880003972245226\n",
      "train loss:0.010506792751952649\n",
      "train loss:0.024041163076505792\n",
      "train loss:0.0075405153446422265\n",
      "train loss:0.02006855207058222\n",
      "train loss:0.006339114860788423\n",
      "train loss:0.037794243451345404\n",
      "train loss:0.019745249305714835\n",
      "train loss:0.005586295777590678\n",
      "train loss:0.004177701015223973\n",
      "train loss:0.04319130386741941\n",
      "train loss:0.025800050455337505\n",
      "train loss:0.01168832405623149\n",
      "train loss:0.01703181901847747\n",
      "train loss:0.014597642114538284\n",
      "train loss:0.005783421490599101\n",
      "train loss:0.028516396929744173\n",
      "train loss:0.007567465541431094\n",
      "train loss:0.01982972940919191\n",
      "train loss:0.0156878981310668\n",
      "train loss:0.007904143986515984\n",
      "train loss:0.06254026675615802\n",
      "train loss:0.02774887039787061\n",
      "train loss:0.006682156155903721\n",
      "train loss:0.025675721378212737\n",
      "train loss:0.005207801094771385\n",
      "train loss:0.010591080258284688\n",
      "train loss:0.014242207955173669\n",
      "train loss:0.009177313869012174\n",
      "train loss:0.016122057803676752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00964060843975931\n",
      "train loss:0.01160356755565716\n",
      "train loss:0.0035015986885506634\n",
      "train loss:0.01778131912661698\n",
      "train loss:0.018105274431545516\n",
      "train loss:0.009310548229780852\n",
      "train loss:0.03771381897492077\n",
      "train loss:0.009130567514137395\n",
      "train loss:0.008364198585622918\n",
      "train loss:0.011097425441820567\n",
      "train loss:0.02822849050131139\n",
      "train loss:0.015557726520588315\n",
      "train loss:0.007709127220736975\n",
      "train loss:0.020496456892396306\n",
      "train loss:0.010252200462152803\n",
      "train loss:0.07599782454100659\n",
      "train loss:0.015188124092504685\n",
      "train loss:0.02397891368122541\n",
      "train loss:0.02816307616348753\n",
      "train loss:0.010472304400853068\n",
      "train loss:0.022024884723751356\n",
      "train loss:0.029387770755374926\n",
      "train loss:0.019534934280605903\n",
      "train loss:0.005482261843871092\n",
      "train loss:0.01637595211365692\n",
      "train loss:0.011342917419403447\n",
      "train loss:0.012764219329162519\n",
      "train loss:0.05113270063999287\n",
      "train loss:0.01980770366460899\n",
      "train loss:0.019649012619665635\n",
      "train loss:0.0074687964159851706\n",
      "train loss:0.003955365278839369\n",
      "train loss:0.013856178077854369\n",
      "train loss:0.034658814549505174\n",
      "train loss:0.003940465472472617\n",
      "train loss:0.0760667849461187\n",
      "train loss:0.04228526212108144\n",
      "train loss:0.008929651812547689\n",
      "train loss:0.012805024822580778\n",
      "train loss:0.03594221371336713\n",
      "train loss:0.010840923293637995\n",
      "train loss:0.008577350877841717\n",
      "train loss:0.027732458914162122\n",
      "train loss:0.0092515811510416\n",
      "=== epoch:19, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.013355897063496895\n",
      "train loss:0.016073166259427306\n",
      "train loss:0.0062510947825073085\n",
      "train loss:0.0285663048650585\n",
      "train loss:0.013660776575052205\n",
      "train loss:0.011995859784311096\n",
      "train loss:0.014517724282526538\n",
      "train loss:0.03629947996198303\n",
      "train loss:0.014208520930606438\n",
      "train loss:0.011394436457732072\n",
      "train loss:0.02158872330409562\n",
      "train loss:0.026113409048675915\n",
      "train loss:0.016573892981323855\n",
      "train loss:0.007998362099399463\n",
      "train loss:0.030067122841522242\n",
      "train loss:0.018750942560055467\n",
      "train loss:0.017815923472979913\n",
      "train loss:0.012769064063568343\n",
      "train loss:0.010998701004224674\n",
      "train loss:0.01116564223050915\n",
      "train loss:0.013041650610107895\n",
      "train loss:0.008625937982197041\n",
      "train loss:0.011015676945356347\n",
      "train loss:0.03272409592174958\n",
      "train loss:0.009087073853068867\n",
      "train loss:0.005681296946264437\n",
      "train loss:0.03232187184641053\n",
      "train loss:0.00796377834176041\n",
      "train loss:0.0061011051939755655\n",
      "train loss:0.018194350553745554\n",
      "train loss:0.014456897970018345\n",
      "train loss:0.026380651149193066\n",
      "train loss:0.014560180724015957\n",
      "train loss:0.0278099039371965\n",
      "train loss:0.03537597119523531\n",
      "train loss:0.0072456219869360084\n",
      "train loss:0.013647535568225472\n",
      "train loss:0.015703758863831893\n",
      "train loss:0.008560993275026001\n",
      "train loss:0.006466387784350943\n",
      "train loss:0.004807527325402792\n",
      "train loss:0.011815959848811307\n",
      "train loss:0.047118625004544966\n",
      "train loss:0.008119285891357223\n",
      "train loss:0.0069598399614966625\n",
      "train loss:0.014461559768260237\n",
      "train loss:0.002734151084373178\n",
      "train loss:0.01309718052373136\n",
      "train loss:0.012307691080405143\n",
      "train loss:0.005677064661833131\n",
      "train loss:0.006481091223619379\n",
      "train loss:0.009008003281929924\n",
      "train loss:0.01631070574102269\n",
      "train loss:0.025411297411391053\n",
      "train loss:0.006143796516258576\n",
      "train loss:0.035989300730784365\n",
      "train loss:0.009781840414168054\n",
      "train loss:0.004229343339804855\n",
      "train loss:0.02129249730097794\n",
      "train loss:0.010458665721901801\n",
      "train loss:0.013544447187355433\n",
      "train loss:0.01052159318960433\n",
      "train loss:0.013701696289710973\n",
      "train loss:0.017902128523320376\n",
      "train loss:0.005175917031158619\n",
      "train loss:0.009907651889041837\n",
      "train loss:0.010485477804149346\n",
      "train loss:0.006965006006433583\n",
      "train loss:0.003034280345490624\n",
      "train loss:0.04459656161357841\n",
      "train loss:0.009950936249986924\n",
      "train loss:0.008296613369823385\n",
      "train loss:0.017787671895347076\n",
      "train loss:0.008584535753965594\n",
      "train loss:0.011516725867555946\n",
      "train loss:0.011414090206591565\n",
      "train loss:0.013607132814308136\n",
      "train loss:0.009785012167763875\n",
      "train loss:0.01243214555795215\n",
      "train loss:0.010119781039231557\n",
      "train loss:0.026367290333872945\n",
      "train loss:0.01180612492869713\n",
      "train loss:0.017979735821072814\n",
      "train loss:0.003166120587700974\n",
      "train loss:0.018663980038504762\n",
      "train loss:0.04429263596193418\n",
      "train loss:0.013400167499893758\n",
      "train loss:0.014329462458571253\n",
      "train loss:0.027076610665967754\n",
      "train loss:0.007820273706173135\n",
      "train loss:0.010939317647007611\n",
      "train loss:0.035661259812952296\n",
      "train loss:0.008954817732959235\n",
      "train loss:0.014263716407262919\n",
      "train loss:0.010559207859607591\n",
      "train loss:0.01918310665887842\n",
      "train loss:0.005777003352397444\n",
      "train loss:0.007186775585266759\n",
      "train loss:0.012513311250802812\n",
      "train loss:0.009022947992244366\n",
      "train loss:0.004337123875060784\n",
      "train loss:0.006959712604915858\n",
      "train loss:0.004080124968284819\n",
      "train loss:0.006549337507846893\n",
      "train loss:0.0708797866246166\n",
      "train loss:0.020335868584342567\n",
      "train loss:0.013989783070347485\n",
      "train loss:0.006324029346421397\n",
      "train loss:0.00425106846856074\n",
      "train loss:0.03327760643045259\n",
      "train loss:0.019931546737726288\n",
      "train loss:0.03361447409775345\n",
      "train loss:0.01153370201973237\n",
      "train loss:0.037418349338828316\n",
      "train loss:0.009204638877222341\n",
      "train loss:0.0295224642185995\n",
      "train loss:0.03454143590834685\n",
      "train loss:0.027732903213971562\n",
      "train loss:0.005863905789844731\n",
      "train loss:0.052788269398961474\n",
      "train loss:0.038833674952075674\n",
      "train loss:0.0071068212415184824\n",
      "train loss:0.01001814938541845\n",
      "train loss:0.009825378175433741\n",
      "train loss:0.011261749592101087\n",
      "train loss:0.01139941825656121\n",
      "train loss:0.02395120833294622\n",
      "train loss:0.03821214897970371\n",
      "train loss:0.016341785919849354\n",
      "train loss:0.005255104733337279\n",
      "train loss:0.012765662281011258\n",
      "train loss:0.00811601129988007\n",
      "train loss:0.0076554982229526435\n",
      "train loss:0.008964299159785648\n",
      "train loss:0.016378428699949953\n",
      "train loss:0.038496345917404994\n",
      "train loss:0.01046687237041462\n",
      "train loss:0.023806650013865574\n",
      "train loss:0.0101077024496207\n",
      "train loss:0.030645909722031836\n",
      "train loss:0.05412140168414572\n",
      "train loss:0.02181852696122558\n",
      "train loss:0.037102165665082704\n",
      "train loss:0.0117128747870924\n",
      "train loss:0.019744043427489432\n",
      "train loss:0.011458895310334895\n",
      "train loss:0.01852700739648304\n",
      "train loss:0.0040043745347931895\n",
      "train loss:0.017456269151480523\n",
      "train loss:0.030529035025307273\n",
      "train loss:0.013948906611197789\n",
      "train loss:0.01723456107824714\n",
      "train loss:0.015607293394599129\n",
      "train loss:0.012567215541193479\n",
      "train loss:0.0028013743606855367\n",
      "train loss:0.007178891170764953\n",
      "train loss:0.01860118333476731\n",
      "train loss:0.007794403588454002\n",
      "train loss:0.007406921118717071\n",
      "train loss:0.0268750312049188\n",
      "train loss:0.01045045480408034\n",
      "train loss:0.00194090868956351\n",
      "train loss:0.007175909671049011\n",
      "train loss:0.02341139644112602\n",
      "train loss:0.006115919043217241\n",
      "train loss:0.0049180066718733\n",
      "train loss:0.012463472210744848\n",
      "train loss:0.04084759867117036\n",
      "train loss:0.02013506953435402\n",
      "train loss:0.018384020411404446\n",
      "train loss:0.006368060333468806\n",
      "train loss:0.014274328439547441\n",
      "train loss:0.02093536314794677\n",
      "train loss:0.015252554806994625\n",
      "train loss:0.02504235870835141\n",
      "train loss:0.0044531149608241175\n",
      "train loss:0.007175340670756709\n",
      "train loss:0.017216726344536862\n",
      "train loss:0.01870471375326095\n",
      "train loss:0.006535832155726779\n",
      "train loss:0.019656692957636723\n",
      "train loss:0.010437307689761628\n",
      "train loss:0.003212890775316625\n",
      "train loss:0.012883228849797146\n",
      "train loss:0.01445360187058763\n",
      "train loss:0.013835698185979493\n",
      "train loss:0.021069358139120843\n",
      "train loss:0.014023247022011374\n",
      "train loss:0.01284344117700946\n",
      "train loss:0.007705630847724076\n",
      "train loss:0.005886910261004705\n",
      "train loss:0.013865156057674277\n",
      "train loss:0.013715575427002264\n",
      "train loss:0.008226260337303965\n",
      "train loss:0.011643891362462945\n",
      "train loss:0.007145857336128572\n",
      "train loss:0.02432826479995914\n",
      "train loss:0.014273588376833483\n",
      "train loss:0.02146032860887537\n",
      "train loss:0.009817015472755571\n",
      "train loss:0.005666007038566552\n",
      "train loss:0.012200755534967973\n",
      "train loss:0.00500461798667716\n",
      "train loss:0.003945496109761162\n",
      "train loss:0.011153683245152092\n",
      "train loss:0.01719952787787228\n",
      "train loss:0.043999579308553506\n",
      "train loss:0.08419361050501886\n",
      "train loss:0.002741663483213245\n",
      "train loss:0.011493198153341284\n",
      "train loss:0.010313390423016091\n",
      "train loss:0.005836108084982341\n",
      "train loss:0.008137186456327074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.009069191784924295\n",
      "train loss:0.010969541348890917\n",
      "train loss:0.07615629629052771\n",
      "train loss:0.03533546110528655\n",
      "train loss:0.051433814719643006\n",
      "train loss:0.009188865653501825\n",
      "train loss:0.011099831887613945\n",
      "train loss:0.00996775257792942\n",
      "train loss:0.04235934792196655\n",
      "train loss:0.007101486515657566\n",
      "train loss:0.01571246912008952\n",
      "train loss:0.014827467202455591\n",
      "train loss:0.01069322980486199\n",
      "train loss:0.009015856249084435\n",
      "train loss:0.02528778291509493\n",
      "train loss:0.0055719015780662726\n",
      "train loss:0.014923310083006338\n",
      "train loss:0.004138418671695407\n",
      "train loss:0.03909261471691158\n",
      "train loss:0.010219958791846723\n",
      "train loss:0.00205584708272705\n",
      "train loss:0.004956222365006485\n",
      "train loss:0.005985972227969229\n",
      "train loss:0.01867361828707588\n",
      "train loss:0.11735113640232003\n",
      "train loss:0.011872100793799552\n",
      "train loss:0.007505161619872457\n",
      "train loss:0.01205165029522851\n",
      "train loss:0.0073780931795294725\n",
      "train loss:0.020946444305909438\n",
      "train loss:0.006703465773259475\n",
      "train loss:0.00750489829581265\n",
      "train loss:0.031065264679393537\n",
      "train loss:0.014536632860984838\n",
      "train loss:0.017258898669585864\n",
      "train loss:0.032836047203473544\n",
      "train loss:0.013513951640585243\n",
      "train loss:0.008939348322177739\n",
      "train loss:0.012356988565694161\n",
      "train loss:0.010585328470015736\n",
      "train loss:0.024942766805967916\n",
      "train loss:0.023085826374619676\n",
      "train loss:0.010387480992362228\n",
      "train loss:0.01055635318314651\n",
      "train loss:0.006426820962905958\n",
      "train loss:0.016278957609445283\n",
      "train loss:0.03895759999750441\n",
      "train loss:0.005908738609312675\n",
      "train loss:0.01082895543925792\n",
      "train loss:0.004117329960338371\n",
      "train loss:0.005844689602255303\n",
      "train loss:0.009033068587861298\n",
      "train loss:0.006103218156096501\n",
      "train loss:0.02589576393995883\n",
      "train loss:0.008887884479200909\n",
      "train loss:0.01496169429610503\n",
      "train loss:0.00844420155379727\n",
      "train loss:0.04873360601266919\n",
      "train loss:0.0074100048152083\n",
      "train loss:0.02221226083500591\n",
      "train loss:0.008994150127421443\n",
      "train loss:0.004419649725388242\n",
      "train loss:0.021125131848566913\n",
      "train loss:0.03236547726690335\n",
      "train loss:0.006087796073541155\n",
      "train loss:0.016495362551581738\n",
      "train loss:0.011687176736689602\n",
      "train loss:0.010208324241283593\n",
      "train loss:0.005203666161174201\n",
      "train loss:0.014771766580964947\n",
      "train loss:0.007420121477894698\n",
      "train loss:0.00996894309125422\n",
      "train loss:0.005544242707789911\n",
      "train loss:0.006790252804712972\n",
      "train loss:0.003906778994249341\n",
      "train loss:0.006848306594579833\n",
      "train loss:0.01018638425942073\n",
      "train loss:0.020669855612684016\n",
      "train loss:0.01081808968102989\n",
      "train loss:0.01904345086700315\n",
      "train loss:0.011824646165357899\n",
      "train loss:0.0057482137490896205\n",
      "train loss:0.007776217532473259\n",
      "train loss:0.005630267059817378\n",
      "train loss:0.010708734440705865\n",
      "train loss:0.013642740303905358\n",
      "train loss:0.009504032122520007\n",
      "train loss:0.00730783925947743\n",
      "train loss:0.006747202030220526\n",
      "train loss:0.005784037630845249\n",
      "train loss:0.004638672758611104\n",
      "train loss:0.008042842610255792\n",
      "train loss:0.018050325649820575\n",
      "train loss:0.0074047435388443225\n",
      "train loss:0.0032434069396286846\n",
      "train loss:0.02635901336500887\n",
      "train loss:0.009299990094985871\n",
      "train loss:0.018537634505328665\n",
      "train loss:0.0743101730714121\n",
      "train loss:0.008595698535979907\n",
      "train loss:0.0046614990891056494\n",
      "train loss:0.006374545071348247\n",
      "train loss:0.009366984421891317\n",
      "train loss:0.029030688520319283\n",
      "train loss:0.008518184326084066\n",
      "train loss:0.0148108655519566\n",
      "train loss:0.042540185597863\n",
      "train loss:0.013873406953889354\n",
      "train loss:0.00466063775026426\n",
      "train loss:0.011541504569774333\n",
      "train loss:0.008333615365436624\n",
      "train loss:0.02289562147272997\n",
      "train loss:0.00262094444180524\n",
      "train loss:0.009923411426378825\n",
      "train loss:0.0059329517016223935\n",
      "train loss:0.0035052409318708233\n",
      "train loss:0.0036662221666169557\n",
      "train loss:0.03916859055899045\n",
      "train loss:0.011150798071352497\n",
      "train loss:0.004150289018789263\n",
      "train loss:0.010101409468958426\n",
      "train loss:0.008657040583871253\n",
      "train loss:0.0046788378051973644\n",
      "train loss:0.009806467425247111\n",
      "train loss:0.015629855744387834\n",
      "train loss:0.02193974007844925\n",
      "train loss:0.0033439026193726475\n",
      "train loss:0.015462449594361563\n",
      "train loss:0.015438389350283986\n",
      "train loss:0.011205949681990752\n",
      "train loss:0.026520987047985677\n",
      "train loss:0.01420867723034657\n",
      "train loss:0.004116285411564606\n",
      "train loss:0.010547891991326408\n",
      "train loss:0.015668919231363786\n",
      "train loss:0.007975070558645055\n",
      "train loss:0.006262594513852039\n",
      "train loss:0.021202157043561164\n",
      "train loss:0.004195484166620685\n",
      "train loss:0.012268976452851844\n",
      "train loss:0.0018812299503703043\n",
      "train loss:0.006900239236709821\n",
      "train loss:0.006482409893235139\n",
      "train loss:0.00423471763624487\n",
      "train loss:0.0312899678125236\n",
      "train loss:0.0029512756637582473\n",
      "train loss:0.017860983717327313\n",
      "train loss:0.01718316573586934\n",
      "train loss:0.015728311841072217\n",
      "train loss:0.0174193667151054\n",
      "train loss:0.013827434519764011\n",
      "train loss:0.00984712338617441\n",
      "train loss:0.007023620552645604\n",
      "train loss:0.00681496618884322\n",
      "train loss:0.017324300724004794\n",
      "train loss:0.009238433165988545\n",
      "train loss:0.006375708572223837\n",
      "train loss:0.01141001773717697\n",
      "train loss:0.03127445430558378\n",
      "train loss:0.013915912067812525\n",
      "train loss:0.004572886467280313\n",
      "train loss:0.00434569345224797\n",
      "train loss:0.006985175858476852\n",
      "train loss:0.005158540068295556\n",
      "train loss:0.015972371707091915\n",
      "train loss:0.0077348926799759195\n",
      "train loss:0.015567657665192563\n",
      "train loss:0.008095871300742274\n",
      "train loss:0.015683400027070128\n",
      "train loss:0.007016516594074956\n",
      "train loss:0.008064797959133174\n",
      "train loss:0.025655407108924472\n",
      "train loss:0.0731768857381789\n",
      "train loss:0.022146863672902622\n",
      "train loss:0.017751655834870175\n",
      "train loss:0.009056474917004224\n",
      "train loss:0.004193343126270537\n",
      "train loss:0.011693715551384152\n",
      "train loss:0.017231179433423126\n",
      "train loss:0.010047231416210882\n",
      "train loss:0.03590484383783463\n",
      "train loss:0.0076227161462073655\n",
      "train loss:0.035162730077746326\n",
      "train loss:0.00751762730136515\n",
      "train loss:0.010583534195986089\n",
      "train loss:0.07448738997964499\n",
      "train loss:0.014850017420095413\n",
      "train loss:0.014851581610741531\n",
      "train loss:0.010215763045786398\n",
      "train loss:0.016000330489181213\n",
      "train loss:0.005223973983231238\n",
      "train loss:0.005002417433655783\n",
      "train loss:0.005790884839250037\n",
      "train loss:0.009000757836573085\n",
      "train loss:0.014452963660246279\n",
      "train loss:0.011565174223156876\n",
      "train loss:0.022036762564644676\n",
      "train loss:0.007733252284939971\n",
      "train loss:0.01852639794461085\n",
      "train loss:0.023468543098499682\n",
      "train loss:0.012255916113388398\n",
      "train loss:0.02347772361497088\n",
      "train loss:0.025625382427851506\n",
      "train loss:0.009484397740872428\n",
      "train loss:0.06168001984978467\n",
      "train loss:0.012346684083674606\n",
      "train loss:0.016616266015913803\n",
      "train loss:0.01097379823065622\n",
      "train loss:0.0051730556263326795\n",
      "train loss:0.027698074986415824\n",
      "train loss:0.010418894648470296\n",
      "train loss:0.020755203771935214\n",
      "train loss:0.005752641458323942\n",
      "train loss:0.004029982739606724\n",
      "train loss:0.011773590613533495\n",
      "train loss:0.04045152340745276\n",
      "train loss:0.0019747002457339386\n",
      "train loss:0.007850167116242921\n",
      "train loss:0.008624104596195604\n",
      "train loss:0.009583985319511611\n",
      "train loss:0.004007396926553799\n",
      "train loss:0.00985261891828583\n",
      "train loss:0.011848412095651416\n",
      "train loss:0.03313789762451266\n",
      "train loss:0.011019981315714236\n",
      "train loss:0.01812126541449564\n",
      "train loss:0.02858621901096299\n",
      "train loss:0.010572908168990263\n",
      "train loss:0.023079642694257975\n",
      "train loss:0.010873204318227725\n",
      "train loss:0.009258160860056179\n",
      "train loss:0.006272308560953008\n",
      "train loss:0.0051992984779365824\n",
      "train loss:0.026153733384282993\n",
      "train loss:0.014892806884516174\n",
      "train loss:0.024473096405149563\n",
      "train loss:0.0024385160855353\n",
      "=== epoch:20, train acc:1.0, test acc:1.0 ===\n",
      "train loss:0.004473867874462491\n",
      "train loss:0.007312597303974536\n",
      "train loss:0.011596823729844266\n",
      "train loss:0.0070642859723101074\n",
      "train loss:0.004713018959712228\n",
      "train loss:0.010859472996723733\n",
      "train loss:0.003976677933628712\n",
      "train loss:0.06028882011844625\n",
      "train loss:0.008896370394037033\n",
      "train loss:0.014679291830695149\n",
      "train loss:0.008873743186824299\n",
      "train loss:0.009648226597374106\n",
      "train loss:0.005186598882714588\n",
      "train loss:0.007553714771967861\n",
      "train loss:0.013158962649386596\n",
      "train loss:0.014079107339491017\n",
      "train loss:0.01489826165955418\n",
      "train loss:0.013959305651401958\n",
      "train loss:0.007894745120555663\n",
      "train loss:0.014876107688495987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.012857163364323193\n",
      "train loss:0.011146037497175281\n",
      "train loss:0.0149926888543946\n",
      "train loss:0.012426968356754515\n",
      "train loss:0.01065895325575764\n",
      "train loss:0.15766787387830317\n",
      "train loss:0.0105444594718379\n",
      "train loss:0.0046402492855736385\n",
      "train loss:0.056879894350409105\n",
      "train loss:0.014819525024751735\n",
      "train loss:0.020242622891193527\n",
      "train loss:0.015066775413958913\n",
      "train loss:0.0046374608100453385\n",
      "train loss:0.011152535646546455\n",
      "train loss:0.007198321012171668\n",
      "train loss:0.004551535526592255\n",
      "train loss:0.013931041973126551\n",
      "train loss:0.012181651173435448\n",
      "train loss:0.0038104850183073536\n",
      "train loss:0.011332283760353258\n",
      "train loss:0.011058977544855804\n",
      "train loss:0.014322980170276675\n",
      "train loss:0.006587269922512122\n",
      "train loss:0.011629835107501934\n",
      "train loss:0.04111786402432147\n",
      "train loss:0.01134506015323893\n",
      "train loss:0.02246349554625185\n",
      "train loss:0.01027212035024322\n",
      "train loss:0.011843498389007133\n",
      "train loss:0.041790171759496654\n",
      "train loss:0.01960350813781283\n",
      "train loss:0.020418468560213038\n",
      "train loss:0.0063535831800253986\n",
      "train loss:0.010869960169832852\n",
      "train loss:0.017360468100779708\n",
      "train loss:0.006360182215663139\n",
      "train loss:0.007387361839265312\n",
      "train loss:0.014067472031278578\n",
      "train loss:0.004930169891571762\n",
      "train loss:0.019908011414939718\n",
      "train loss:0.018070182807305532\n",
      "train loss:0.006450443748738271\n",
      "train loss:0.006898335823786434\n",
      "train loss:0.011097643992037658\n",
      "train loss:0.02841374357040316\n",
      "train loss:0.008200028900320077\n",
      "train loss:0.016866476436289773\n",
      "train loss:0.004363175213787089\n",
      "train loss:0.013504192392133574\n",
      "train loss:0.009867351243162263\n",
      "train loss:0.030843529725505747\n",
      "train loss:0.0070296996425903945\n",
      "train loss:0.022850278793513323\n",
      "train loss:0.0242362972327469\n",
      "train loss:0.01196526781066254\n",
      "train loss:0.006578541547032435\n",
      "train loss:0.006160471269266945\n",
      "train loss:0.011728900535879685\n",
      "train loss:0.010211524758180516\n",
      "train loss:0.003799117184314842\n",
      "train loss:0.0034680215354063572\n",
      "train loss:0.019040701879557725\n",
      "train loss:0.00522402175604616\n",
      "train loss:0.007529117451242633\n",
      "train loss:0.00815168834685321\n",
      "train loss:0.0055786565599155544\n",
      "train loss:0.009230021394426396\n",
      "train loss:0.010718545705137965\n",
      "train loss:0.008041151733918341\n",
      "train loss:0.023425548622604084\n",
      "train loss:0.007170604510012306\n",
      "train loss:0.01860976098945566\n",
      "train loss:0.008973304902200054\n",
      "train loss:0.015576005412286853\n",
      "train loss:0.011018700285181586\n",
      "train loss:0.013460991380195644\n",
      "train loss:0.010100336803859421\n",
      "train loss:0.011662648319381857\n",
      "train loss:0.003011870402893033\n",
      "train loss:0.008576603434557727\n",
      "train loss:0.011290581255666055\n",
      "train loss:0.013527093297918727\n",
      "train loss:0.02024354354018332\n",
      "train loss:0.07382909637676283\n",
      "train loss:0.00700473054825761\n",
      "train loss:0.008185975331717547\n",
      "train loss:0.01502072918668648\n",
      "train loss:0.00935071608127915\n",
      "train loss:0.009969881913621635\n",
      "train loss:0.04747694274371985\n",
      "train loss:0.01116893491995135\n",
      "train loss:0.004552318338321295\n",
      "train loss:0.03935991913135689\n",
      "train loss:0.03595689775156268\n",
      "train loss:0.01110057014403575\n",
      "train loss:0.0074525175233780515\n",
      "train loss:0.006177642266516133\n",
      "train loss:0.01920996405227924\n",
      "train loss:0.012942059187437403\n",
      "train loss:0.004512985093928719\n",
      "train loss:0.07799866142466719\n",
      "train loss:0.04342420552916385\n",
      "train loss:0.012072699611328801\n",
      "train loss:0.020649447085736412\n",
      "train loss:0.005649401798674303\n",
      "train loss:0.05477441910652391\n",
      "train loss:0.008476450317121817\n",
      "train loss:0.008126368630033057\n",
      "train loss:0.01035398255109489\n",
      "train loss:0.039038744331885054\n",
      "train loss:0.03999182453756268\n",
      "train loss:0.018479883806392088\n",
      "train loss:0.009351849486937453\n",
      "train loss:0.01582918027995266\n",
      "train loss:0.008188878499858105\n",
      "train loss:0.012720875986129133\n",
      "train loss:0.004143033780534434\n",
      "train loss:0.0014384939044907855\n",
      "train loss:0.040245255208045935\n",
      "train loss:0.028145123161912727\n",
      "train loss:0.05421445157066512\n",
      "train loss:0.004946424610680427\n",
      "train loss:0.013612573044075474\n",
      "train loss:0.009885786772137323\n",
      "train loss:0.005388837867424314\n",
      "train loss:0.033102155973371857\n",
      "train loss:0.011193646495633213\n",
      "train loss:0.00852584559442083\n",
      "train loss:0.005599678860700331\n",
      "train loss:0.00915888956605346\n",
      "train loss:0.01892979860239148\n",
      "train loss:0.014441347221680773\n",
      "train loss:0.021569318398022265\n",
      "train loss:0.004490858062350269\n",
      "train loss:0.005955198555491067\n",
      "train loss:0.026855311575411517\n",
      "train loss:0.018407797299663062\n",
      "train loss:0.003244168044943804\n",
      "train loss:0.01844971633663943\n",
      "train loss:0.014483999459802806\n",
      "train loss:0.023079117180751188\n",
      "train loss:0.00811604635820208\n",
      "train loss:0.01621435293672924\n",
      "train loss:0.006508926882394106\n",
      "train loss:0.010973871537881118\n",
      "train loss:0.019830409704514558\n",
      "train loss:0.006767187647552981\n",
      "train loss:0.015836989877532903\n",
      "train loss:0.01340292582747441\n",
      "train loss:0.019338891769813925\n",
      "train loss:0.008375630192479767\n",
      "train loss:0.011608777250282074\n",
      "train loss:0.023477800518100472\n",
      "train loss:0.00887919710047393\n",
      "train loss:0.021992139670892082\n",
      "train loss:0.011476550070762918\n",
      "train loss:0.019164666933370482\n",
      "train loss:0.012649041828959534\n",
      "train loss:0.014457438035941506\n",
      "train loss:0.016153039689572685\n",
      "train loss:0.012916565382100809\n",
      "train loss:0.012402072047416987\n",
      "train loss:0.01887709720965261\n",
      "train loss:0.018327703284787104\n",
      "train loss:0.015064659072509532\n",
      "train loss:0.011705630958668986\n",
      "train loss:0.013521031062899614\n",
      "train loss:0.015837518871660596\n",
      "train loss:0.00938615145463118\n",
      "train loss:0.009369610617447384\n",
      "train loss:0.07203204174403348\n",
      "train loss:0.0018334767230344837\n",
      "train loss:0.058534333207133186\n",
      "train loss:0.007234773245179178\n",
      "train loss:0.022575347751110288\n",
      "train loss:0.029421555820014586\n",
      "train loss:0.013580612834168535\n",
      "train loss:0.0065327050129123125\n",
      "train loss:0.0073468940711876925\n",
      "train loss:0.0199042634493005\n",
      "train loss:0.0073303768892213975\n",
      "train loss:0.02172499375290406\n",
      "train loss:0.013063871388172778\n",
      "train loss:0.004464033741085974\n",
      "train loss:0.01207884543220516\n",
      "train loss:0.006982639098221871\n",
      "train loss:0.005378039224635485\n",
      "train loss:0.005622457643487952\n",
      "train loss:0.006923436895988816\n",
      "train loss:0.013126508507330688\n",
      "train loss:0.012348113272887207\n",
      "train loss:0.00878093541767653\n",
      "train loss:0.003959967620082954\n",
      "train loss:0.005849372281109912\n",
      "train loss:0.0211466082894276\n",
      "train loss:0.007627831118432477\n",
      "train loss:0.008606361685248696\n",
      "train loss:0.08470337482742826\n",
      "train loss:0.006235698547092645\n",
      "train loss:0.013931814454518424\n",
      "train loss:0.049901759463478745\n",
      "train loss:0.012554392116095428\n",
      "train loss:0.011035257883334688\n",
      "train loss:0.012929926841035668\n",
      "train loss:0.009877617763198829\n",
      "train loss:0.018049968742606552\n",
      "train loss:0.008990289018410574\n",
      "train loss:0.05573139967955057\n",
      "train loss:0.009555765101063214\n",
      "train loss:0.012740410551041708\n",
      "train loss:0.012023587648876803\n",
      "train loss:0.049833306208929964\n",
      "train loss:0.02658429912251495\n",
      "train loss:0.007316520582007602\n",
      "train loss:0.006485697907031823\n",
      "train loss:0.010279111054196668\n",
      "train loss:0.007157331537422495\n",
      "train loss:0.006094630786415724\n",
      "train loss:0.008170135303857448\n",
      "train loss:0.005320230026736717\n",
      "train loss:0.017244473102963755\n",
      "train loss:0.010026032198441191\n",
      "train loss:0.0148537148567385\n",
      "train loss:0.013335081828501261\n",
      "train loss:0.006225678279651658\n",
      "train loss:0.009641240915149183\n",
      "train loss:0.01656876818225854\n",
      "train loss:0.008805440139312568\n",
      "train loss:0.010878778704951486\n",
      "train loss:0.007244143913317994\n",
      "train loss:0.015400038576506656\n",
      "train loss:0.01749829427989195\n",
      "train loss:0.01007894064169319\n",
      "train loss:0.014800043683898843\n",
      "train loss:0.018952638490448415\n",
      "train loss:0.020508535503339344\n",
      "train loss:0.015786075766086406\n",
      "train loss:0.009521097110743023\n",
      "train loss:0.01023951165409397\n",
      "train loss:0.01038729410725506\n",
      "train loss:0.024918945409935115\n",
      "train loss:0.008369526639943985\n",
      "train loss:0.004821226199214152\n",
      "train loss:0.0072373220293493355\n",
      "train loss:0.017891460179120352\n",
      "train loss:0.005875349675036286\n",
      "train loss:0.008257678531596034\n",
      "train loss:0.010179603411064492\n",
      "train loss:0.022441510910683984\n",
      "train loss:0.0037344097773037567\n",
      "train loss:0.012766577205330388\n",
      "train loss:0.013336551655245913\n",
      "train loss:0.04186884580682548\n",
      "train loss:0.05898203071391759\n",
      "train loss:0.006332343469167513\n",
      "train loss:0.01844647958757772\n",
      "train loss:0.006077158687703385\n",
      "train loss:0.0088789176736792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.01100235117701177\n",
      "train loss:0.011289275912583085\n",
      "train loss:0.013358191195039542\n",
      "train loss:0.009514916587916145\n",
      "train loss:0.011579562812426952\n",
      "train loss:0.02803809481917736\n",
      "train loss:0.03394887505000449\n",
      "train loss:0.020386035723857585\n",
      "train loss:0.017526134235536794\n",
      "train loss:0.005936169114744413\n",
      "train loss:0.015691157175969066\n",
      "train loss:0.013781173054535729\n",
      "train loss:0.01850364762215243\n",
      "train loss:0.012187584187714857\n",
      "train loss:0.02474073193250135\n",
      "train loss:0.010350864251257546\n",
      "train loss:0.012372963853859326\n",
      "train loss:0.017028144434429265\n",
      "train loss:0.01987256684970857\n",
      "train loss:0.01093916660527942\n",
      "train loss:0.014844242506651215\n",
      "train loss:0.012523991658719522\n",
      "train loss:0.007881478350885974\n",
      "train loss:0.004642705899184705\n",
      "train loss:0.003969334556714946\n",
      "train loss:0.011606082058070986\n",
      "train loss:0.02446113907128316\n",
      "train loss:0.013834864524501958\n",
      "train loss:0.021231387617682274\n",
      "train loss:0.005691505396244665\n",
      "train loss:0.005689167859310982\n",
      "train loss:0.01016871264759438\n",
      "train loss:0.03166378633951455\n",
      "train loss:0.007789895325092876\n",
      "train loss:0.01272207018331351\n",
      "train loss:0.016127475707371046\n",
      "train loss:0.011858534109591472\n",
      "train loss:0.013688600964973652\n",
      "train loss:0.007390622224852379\n",
      "train loss:0.007263729353322204\n",
      "train loss:0.0022615960104338264\n",
      "train loss:0.008709270463559675\n",
      "train loss:0.00677191814251985\n",
      "train loss:0.009299270615149924\n",
      "train loss:0.03884887798459959\n",
      "train loss:0.008060650820083556\n",
      "train loss:0.01868904704647645\n",
      "train loss:0.01989660854660134\n",
      "train loss:0.01225335985441071\n",
      "train loss:0.004804715590586015\n",
      "train loss:0.00847948687020751\n",
      "train loss:0.009896633823080207\n",
      "train loss:0.01768293617176581\n",
      "train loss:0.022687624536566205\n",
      "train loss:0.016977892791996314\n",
      "train loss:0.008525013953064327\n",
      "train loss:0.023910389884250812\n",
      "train loss:0.0072534098314882885\n",
      "train loss:0.006461783795871859\n",
      "train loss:0.020550029919254354\n",
      "train loss:0.015453466789372849\n",
      "train loss:0.006944397922879043\n",
      "train loss:0.012955528683109735\n",
      "train loss:0.00553065234723289\n",
      "train loss:0.011426594957388524\n",
      "train loss:0.04291930731964164\n",
      "train loss:0.06686773426723429\n",
      "train loss:0.009648573505329655\n",
      "train loss:0.019350489268156423\n",
      "train loss:0.012891576715864524\n",
      "train loss:0.011983309494189466\n",
      "train loss:0.004740786012660959\n",
      "train loss:0.011936651277756238\n",
      "train loss:0.02151271222516972\n",
      "train loss:0.007575813291575247\n",
      "train loss:0.01768672069310713\n",
      "train loss:0.008456276137142222\n",
      "train loss:0.009161518599838573\n",
      "train loss:0.0074661213747553275\n",
      "train loss:0.029573307787460847\n",
      "train loss:0.022244121307271117\n",
      "train loss:0.021884004158801033\n",
      "train loss:0.04039356444222411\n",
      "train loss:0.004437986127378089\n",
      "train loss:0.006597753746708204\n",
      "train loss:0.00988976933653707\n",
      "train loss:0.010837753960042308\n",
      "train loss:0.007009911562951169\n",
      "train loss:0.009074455941350576\n",
      "train loss:0.015050780273932904\n",
      "train loss:0.011258121845007489\n",
      "train loss:0.012509431355367487\n",
      "train loss:0.0074076323869005445\n",
      "train loss:0.013444372665481928\n",
      "train loss:0.012848829016159618\n",
      "train loss:0.007130698183386468\n",
      "train loss:0.022533728454789986\n",
      "train loss:0.015294690276678307\n",
      "train loss:0.00466207580359181\n",
      "train loss:0.012537706964664736\n",
      "train loss:0.007175871630062949\n",
      "train loss:0.004062165812268098\n",
      "train loss:0.029940791127873033\n",
      "train loss:0.011631972937456663\n",
      "train loss:0.012798877578697885\n",
      "train loss:0.007490203653418138\n",
      "train loss:0.010774431251407176\n",
      "train loss:0.009696099608758654\n",
      "train loss:0.0076179952805648906\n",
      "train loss:0.007912276363347964\n",
      "train loss:0.06565020719004033\n",
      "train loss:0.006987197891353622\n",
      "train loss:0.01871251407469675\n",
      "train loss:0.015919243161671287\n",
      "train loss:0.017967000475911057\n",
      "train loss:0.005138283865063543\n",
      "train loss:0.008205052380154931\n",
      "train loss:0.007813009552254767\n",
      "train loss:0.01277081218412181\n",
      "train loss:0.013679131030065407\n",
      "train loss:0.007503180236215392\n",
      "train loss:0.007239151250671113\n",
      "train loss:0.009678068140532778\n",
      "train loss:0.017044981738102597\n",
      "train loss:0.011337353344090735\n",
      "train loss:0.031777362575557265\n",
      "train loss:0.012885661054890703\n",
      "train loss:0.03794986277792804\n",
      "train loss:0.004167046302721034\n",
      "train loss:0.011478435374093918\n",
      "train loss:0.013283527219663787\n",
      "train loss:0.08819749986842967\n",
      "train loss:0.03077253404433968\n",
      "train loss:0.007061525557704238\n",
      "train loss:0.011019923915251646\n",
      "train loss:0.014590095028364647\n",
      "train loss:0.019471374257919442\n",
      "train loss:0.010336506772563491\n",
      "train loss:0.010833462578410996\n",
      "train loss:0.006215597578790057\n",
      "train loss:0.008488517450788956\n",
      "train loss:0.00694636204248614\n",
      "train loss:0.007520491073222152\n",
      "train loss:0.005155525123642586\n",
      "train loss:0.009176112193789705\n",
      "train loss:0.002014032674803435\n",
      "train loss:0.05214238433445282\n",
      "train loss:0.017996036864092596\n",
      "train loss:0.03178883515313059\n",
      "train loss:0.03328390421644232\n",
      "train loss:0.007214545860050425\n",
      "train loss:0.010257444355320593\n",
      "train loss:0.010809698166416885\n",
      "train loss:0.015053994247763247\n",
      "train loss:0.003111340229293975\n",
      "train loss:0.008868358150886943\n",
      "train loss:0.0026393282655319877\n",
      "train loss:0.01138573820010301\n",
      "train loss:0.008915252884346105\n",
      "train loss:0.007705034012078774\n",
      "train loss:0.003477948541789526\n",
      "train loss:0.00557934723231954\n",
      "train loss:0.0053912637305884616\n",
      "train loss:0.012896951265909135\n",
      "train loss:0.009386902862332295\n",
      "train loss:0.028473973780201726\n",
      "train loss:0.017534769241810567\n",
      "train loss:0.010410410957051282\n",
      "train loss:0.006103615468077259\n",
      "train loss:0.011171660874112204\n",
      "train loss:0.009906087178943536\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:1.0\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34min 16s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "max_epochs = 5\n",
    "\n",
    "network = SimpleConvnet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, X_train, y_train, X_val, y_val,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='AdaGrad', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = network.predict(X_test)\n",
    "pred = np.argmax(tmp,axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.987"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】（アドバンス課題）LeNet\n",
    "CNNで画像認識を行う際は、フィルタサイズや層の数などを１から考えるのではなく、有名な構造を利用することが一般的です。現在では実用的に使われることはありませんが、歴史的に重要なのは1998年の LeNet です。この構造を再現してMNISTに対して動かし、Accuracyを計算してください。\n",
    "\n",
    "[Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※上記論文から引用\n",
    "\n",
    "\n",
    "サブサンプリングとは現在のプーリングに相当するものです。現代風に以下のように作ってみることにします。活性化関数も当時はシグモイド関数ですが、ReLUとします。\n",
    "\n",
    "\n",
    "* 1.畳み込み層　出力チャンネル数6、フィルタサイズ5×5、ストライド1\n",
    "* 2.ReLU\n",
    "* 3.最大プーリング\n",
    "* 4.畳み込み層　出力チャンネル数16、フィルタサイズ5×5、ストライド1\n",
    "* 5.ReLU\n",
    "* 6.最大プーリング\n",
    "* 7.平滑化\n",
    "* 8.全結合層　出力ノード数120\n",
    "* 9.ReLU\n",
    "* 10.全結合層　出力ノード数84\n",
    "* 11.ReLU\n",
    "* 12.全結合層　出力ノード数10\n",
    "* 13.ソフトマックス関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題8】（アドバンス課題）有名な画像認識モデルの調査\n",
    "CNNの代表的な構造としてははAlexNet(2012)、VGG16(2014)などがあります。こういったものはフレームワークで既に用意されていることも多いです。\n",
    "\n",
    "\n",
    "どういったものがあるか簡単に調べてまとめてください。名前だけでも見ておくと良いでしょう。\n",
    "\n",
    "\n",
    "《参考》\n",
    "\n",
    "\n",
    "[Applications - Keras Documentation](https://keras.io/ja/applications/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題9】出力サイズとパラメータ数の計算\n",
    "CNNモデルを構築する際には、全結合層に入力する段階で特徴量がいくつになっているかを事前に計算する必要があります。\n",
    "\n",
    "\n",
    "また、巨大なモデルを扱うようになると、メモリや計算速度の関係でパラメータ数の計算は必須になってきます。フレームワークでは各層のパラメータ数を表示させることが可能ですが、意味を理解していなくては適切な調整が行えません。\n",
    "\n",
    "\n",
    "以下の3つの畳み込み層の出力サイズとパラメータ数を計算してください。パラメータ数についてはバイアス項も考えてください。\n",
    "\n",
    "\n",
    "1.\n",
    "\n",
    "\n",
    "* 入力サイズ : 144×144, 3チャンネル\n",
    "* フィルタサイズ : 3×3, 6チャンネル\n",
    "* ストライド : 1\n",
    "* パディング : なし\n",
    "\n",
    "2.\n",
    "\n",
    "\n",
    "* 入力サイズ : 60×60, 24チャンネル\n",
    "* フィルタサイズ : 3×3, 48チャンネル\n",
    "* ストライド　: 1\n",
    "* パディング : なし\n",
    "\n",
    "3.\n",
    "\n",
    "\n",
    "* 入力サイズ : 20×20, 10チャンネル\n",
    "* フィルタサイズ: 3×3, 20チャンネル\n",
    "* ストライド : 2\n",
    "* パディング : なし\n",
    "\n",
    "＊最後の例は丁度良く畳み込みをすることができない場合です。フレームワークでは余ったピクセルを見ないという処理が行われることがあるので、その場合を考えて計算してください。端が欠けてしまうので、こういった設定は好ましくないという例です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q9_func(X, filter_size, b, S=1, P=0):\n",
    "    FN, C, FH, FW = filter_size.shape  \n",
    "    N, C, H, W =X.shape\n",
    "    out_h = int((H + 2*P - FH) / S +1)\n",
    "    out_w = int((W + 2*P - FW) / S + 1)  \n",
    "\n",
    "    col = im2col(X, FH, FW, S, P)\n",
    "    #print(col.shape)\n",
    "    col_W = filter_size.reshape(FN, -1).T # フィルターの展開\n",
    "    #print(col_W.shape)\n",
    "    weight = col_W.shape[0] * col_W.shape[1]\n",
    "\n",
    "    out = np.dot(col, col_W) + b\n",
    "    out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "    return out, weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問1\n",
    "入力サイズ : 144×144, 3チャンネル\n",
    "\n",
    "フィルタサイズ : 3×3, 6チャンネル\n",
    "\n",
    "ストライド : 1\n",
    "\n",
    "パディング : なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解答 : \n",
      "出力サイズ：(1, 6, 142, 142)\n",
      "重み : 162(6, 3, 3, 3)\n",
      "バイアス : (1, 1, 6)\n"
     ]
    }
   ],
   "source": [
    "b = np.zeros((1,1,6))\n",
    "\n",
    "X = np.random.randn(1, 3, 144, 144)\n",
    "filter_size = np.zeros((6, 3, 3, 3))\n",
    "\n",
    "out, weight = q9_func(X, filter_size, b)\n",
    "\n",
    "print(\"解答 : \\n出力サイズ：{}\\n重み : {}{}\".format(out.shape, weight, filter_size.shape))\n",
    "print(\"バイアス : {}\".format(b.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問2\n",
    "\n",
    "入力サイズ : 60×60, 24チャンネル\n",
    "\n",
    "フィルタサイズ : 3×3, 48チャンネル\n",
    "\n",
    "ストライド　: 1\n",
    "\n",
    "パディング : なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解答 : \n",
      "出力サイズ：(1, 48, 58, 58)\n",
      "重み : 10368(48, 24, 3, 3)\n",
      "バイアス : (1, 1, 48)\n"
     ]
    }
   ],
   "source": [
    "b = np.zeros((1,1,48))\n",
    "\n",
    "X = np.random.randn(1, 24, 60, 60)\n",
    "filter_size = np.zeros((48, 24, 3, 3))\n",
    "\n",
    "out, weight = q9_func(X, filter_size, b)\n",
    "\n",
    "print(\"解答 : \\n出力サイズ：{}\\n重み : {}{}\".format(out.shape, weight, filter_size.shape))\n",
    "print(\"バイアス : {}\".format(b.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問3\n",
    "\n",
    "入力サイズ : 20×20, 10チャンネル\n",
    "\n",
    "フィルタサイズ: 3×3, 20チャンネル\n",
    "\n",
    "ストライド : 2\n",
    "\n",
    "パディング : なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解答 : \n",
      "出力サイズ：(1, 20, 9, 9)\n",
      "重み : 1800(20, 10, 3, 3)\n",
      "バイアス : (1, 1, 20)\n"
     ]
    }
   ],
   "source": [
    "b = np.zeros((1,1,20))\n",
    "\n",
    "X = np.random.randn(1, 10, 20, 20)\n",
    "#print(X[0][0][0])\n",
    "filter_size = np.ones((20, 10, 3, 3))\n",
    "\n",
    "out, weight = q9_func(X, filter_size, b, 2)\n",
    "\n",
    "print(\"解答 : \\n出力サイズ：{}\\n重み : {}{}\".format(out.shape, weight, filter_size.shape))\n",
    "print(\"バイアス : {}\".format(b.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strideが2なので、(20,20)の列の内18までしか畳み込めておらず、19,20行列目は無視されている。\n",
    "strideを1にするか、フィルタをsirideとかけたときに20の公約数になるように調節すればちゃんと全部使われるはず。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題10】（アドバンス課題）フィルタサイズに関する調査\n",
    "畳み込み層にはフィルタサイズというハイパーパラメータがありますが、2次元畳み込み層において現在では3×3と1×1の使用が大半です。以下のそれぞれを調べたり、自分なりに考えて説明してください。\n",
    "\n",
    "\n",
    "* 7×7などの大きめのものではなく、3×3のフィルタが一般的に使われる理由\n",
    "* 高さや幅方向を持たない1×1のフィルタの効果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
